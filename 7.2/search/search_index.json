{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>kineticadb/charts</code>","text":"<p>Accelerate your AI and analytics. Kinetica harnesses real-time data and the power of CPUs &amp;  GPUs for  lightning-fast insights due to it being uniquely designed for fast and flexible analytics on large volumes  of changing data with incredible performance.</p> <p>Kinetica DB can be quickly installed into Kubernetes using Helm.</p> <ul> <li> <p> Set up in 15 minutes </p> <p>Install the Kinetica DB locally on <code>Kind</code> or <code>k3s</code> with <code>helm</code> to get up and running in minutes.</p> <p> Quickstart</p> </li> <li> <p> Prepare to Install</p> <p>What you need to know &amp; do before beginning a production installation.</p> <p> Preparation and Prerequisites</p> </li> <li> <p> Production DB Installation</p> <p>Install the Kinetica DB with helm to get up and running quickly</p> <p> Installation</p> </li> <li> <p> Channel Your Inner Ninja</p> <p>Advanced Installation Topics which go beyond the basic installation.</p> <p> Advanced Topics</p> </li> <li> <p> Running and Managing the Platform</p> <p>Metrics, Monitoring, Logs and Telemetry Distribution.</p> <p> Operations</p> </li> <li> <p> Product Architecture</p> <p>The Modern Analytics Database Architected for Performance at Scale.</p> <p> Architecture</p> </li> <li> <p> Support</p> <p>Additional Help, Tutorials and Troubleshooting resources.</p> <p> Support</p> </li> <li> <p> Configuration in Detail</p> <p>Detailed reference material for the Helm Charts &amp; Kinetica for Kubernetes CRDs.</p> <p> Reference Documentation</p> </li> </ul>"},{"location":"Advanced/","title":"Advanced Topics","text":"<ul> <li> <p> Find alternative chart versions</p> <p>How to use pre-release or development Chart version if requested to by Kinetica Support. </p> <p> Alternative Charts</p> </li> <li> <p> Configuring Ingress Records</p> <p>How to expose Kinetica via Kubernetes Ingress.</p> <p> Ingress Configuration</p> </li> <li> <p> Air-Gapped Environments</p> <p>Specifics for installing Kinetica for Kubernetes in an Air-Gapped Environment</p> <p> Airgapped</p> </li> <li> <p> Using your own OpenTelemetry Collector</p> <p>How to configure Kinetica for Kubernetes to use your open OpenTelemetry collector. </p> <p> External OTEL</p> </li> <li> <p> Minio for Dev/Test S3 Storage </p> <p>Install Minio in order to enable S3 storage for Development.</p> <p> min.io</p> </li> <li> <p> Installing Velero for Backup/Restore</p> <p>Install Velero in order to enable Kinetica for Kubernetes  Backup/Restore functionality.</p> <p> Velero</p> </li> <li> <p> Creating Resources with Kubernetes APIs</p> <p>Create Users, Roles, DB Schema etc. using Kubernertes CRs.</p> <p> Resources</p> </li> <li> <p> Kinetica on Apple OS X (Apple Silicon) </p> <p>Install the Kinetica DB on a new Kubernetes 'production-like' cluster on Apple OS X   (Apple Silicon) using UTM.</p> <p> Apple ARM64</p> </li> <li> <p> Bare Metal/VM Installation from Scratch</p> <p>Install the Kinetica DB on a new Kubernetes 'production-like' bare metal (or VMs)    cluster via <code>kubeadm</code>   using <code>cilium</code> Networking,   <code>kube-vip</code> LoadBalancer.</p> <p> Bare Metal/VM Installation</p> </li> <li> <p> Software LoadBalancer</p> <p>Install a software Kubernetes CCM/LoadBalancer for bare metal or  VM based Kubernetes CLusters.  <code>kube-vip</code> LoadBalancer.</p> <p> Software LoadBalancer</p> </li> </ul> <p>  Home</p>"},{"location":"Advanced/advanced_topics/","title":"Advanced Topics","text":""},{"location":"Advanced/advanced_topics/#install-from-a-developmentpre-release-chart-version","title":"Install from a development/pre-release chart version","text":"<p>Find all alternative chart versions with:</p> Find alternative chart versions<pre><code>helm search repo kinetica-operators --devel --versions\n</code></pre> <p></p> <p>Then append <code>--devel --version [CHART-DEVEL-VERSION]</code> to the end of the Helm install command. See here.</p>"},{"location":"Advanced/advanced_topics/#using-your-own-opentelemetry-collector","title":"Using your own OpenTelemetry Collector","text":""},{"location":"Advanced/advanced_topics/#coming-soon","title":"Coming Soon","text":""},{"location":"Advanced/advanced_topics/#configuring-ingress-records","title":"Configuring Ingress Records","text":""},{"location":"Advanced/advanced_topics/#ingress-nginx","title":"ingress-nginx","text":""},{"location":"Advanced/advanced_topics/#coming-soon_1","title":"Coming Soon","text":""},{"location":"Advanced/advanced_topics/#nginx-ingress","title":"nginx-ingress","text":""},{"location":"Advanced/advanced_topics/#coming-soon_2","title":"Coming Soon","text":""},{"location":"Advanced/advanced_topics/#bare-metal-loadbalancer","title":"Bare Metal LoadBalancer","text":""},{"location":"Advanced/advanced_topics/#kube-vip","title":"kube-vip","text":"<p>kube-vip provides Kubernetes clusters with a virtual IP and load balancer for both the control plane (for building a highly-available cluster) and Kubernetes Services of type LoadBalancer without relying on any external hardware or software.</p>"},{"location":"Advanced/advanced_topics/#coming-soon_3","title":"Coming Soon","text":"<p>## Integration with Kerberos</p>"},{"location":"Advanced/advanced_topics/#coming-soon_4","title":"Coming Soon","text":""},{"location":"Advanced/airgapped/","title":"Air-Gapped Environments","text":""},{"location":"Advanced/airgapped/#obtaining-the-kinetica-images","title":"Obtaining the Kinetica Images","text":"Kinetica Images for an Air-Gapped Environment <p>If you are installing Kinetica with Helm in an air-gapped environment you will either need a Registry Proxy to pass the requests through or to download the images and push them to your internal Registry.</p> <p>For information on ways to transfer the files into an air-gapped environment See here.</p> <p>Please select the method to transfer the images: -</p> mindthegap containerd docker <p>It is possible to use  <code>mesosphere/mindthegap</code></p> <p>mindthegap</p> <p><code>mindthegap</code> provides utilities to manage air-gapped image bundles,  both creating image bundles and seeding images from a bundle into  an existing OCI registry or directly loading them to <code>containerd</code>.</p> <p>This makes it possible with <code>mindthegap</code> to</p> <ul> <li>create a single archive bundle of all the required images outside the  air-gapped environment</li> <li>run <code>mindthegap</code> using the archive bundle on the Kubernetes Nodes to bulk load the images into <code>contained</code> in a single command.</li> </ul> <p>Kinetica provides two <code>mindthegap</code> yaml files which list all the necessary images for Kinetica for Kubernetes.</p> <ul> <li>CPU only</li> <li> CPU &amp; nVidia CUDA GPU</li> </ul>"},{"location":"Advanced/airgapped/#required-container-images","title":"Required Container Images","text":""},{"location":"Advanced/airgapped/#dockerio-required-kinetica-images-for-all-installations","title":"docker.io (Required Kinetica Images for All Installations)","text":"<ul> <li>docker.io/kineticastagingcloud/kinetica-k8s-operator:v7.2.0-3.rc-3<ul> <li>docker.io/kineticastagingcloud/kinetica-k8s-cpu:v7.2.0-3.rc-3 or </li> <li>docker.io/kineticastagingcloud/kinetica-k8s-cpu-avx512:v7.2.0-3.rc-3 or </li> <li>docker.io/kineticastagingcloud/kinetica-k8s-gpu:v7.2.0-3.rc-3</li> </ul> </li> <li>docker.io/kineticastagingcloud/workbench-operator:v7.2.0-3.rc-3</li> <li>docker.io/kineticastagingcloud/workbench:v7.2.0-3.rc-3</li> <li>docker.io/kineticastagingcloud/kinetica-k8s-monitor:v7.2.0-3.rc-3</li> <li>docker.io/kineticastagingcloud/busybox:v7.2.0-3.rc-3</li> <li>docker.io/kineticastagingcloud/fluent-bit:v7.2.0-3.rc-3</li> <li>docker.io/kinetica/kagent:7.1.9.15.20230823123615.ga</li> </ul>"},{"location":"Advanced/airgapped/#nvcrio-required-kinetica-images-for-gpu-installations-using-kinetica-k8s-gpu","title":"nvcr.io (Required Kinetica Images for GPU Installations using <code>kinetica-k8s-gpu</code>)","text":"<ul> <li>nvcr.io/nvidia/gpu-operator:v23.9.1</li> </ul>"},{"location":"Advanced/airgapped/#registryk8sio-required-kinetica-images-for-gpu-installations-using-kinetica-k8s-gpu","title":"registry.k8s.io (Required Kinetica Images for GPU Installations using <code>kinetica-k8s-gpu</code>)","text":"<ul> <li>registry.k8s.io/nfd/node-feature-discovery:v0.14.2</li> </ul>"},{"location":"Advanced/airgapped/#dockerio-required-supporting-images","title":"docker.io (Required Supporting Images)","text":"<ul> <li>docker.io/bitnami/openldap:2.6.7</li> <li>docker.io/alpine/openssl:latest (used by bitnami/openldap)</li> <li>docker.io/otel/opentelemetry-collector-contrib:0.95.0</li> </ul>"},{"location":"Advanced/airgapped/#quayio-required-supporting-images","title":"quay.io (Required Supporting Images)","text":"<ul> <li>quay.io/brancz/kube-rbac-proxy:v0.14.2</li> </ul>"},{"location":"Advanced/airgapped/#optional-container-images","title":"Optional Container Images","text":"<p>These images are only required if certain features are enabled as part of the Helm installation: -</p> <ul> <li>CertManager</li> <li>ingress-ninx</li> </ul>"},{"location":"Advanced/airgapped/#quayio-optional-supporting-images","title":"quay.io (Optional Supporting Images)","text":"<ul> <li>quay.io/jetstack/cert-manager-cainjector:v1.13.3 (if optionally installing CertManager via Kinetica Helm Chart)</li> <li>quay.io/jetstack/cert-manager-controller:v1.13.3 (if optionally installing CertManager via Kinetica Helm Chart)</li> <li>quay.io/jetstack/cert-manager-webhook:v1.13.3 (if optionally installing CertManager via Kinetica Helm Chart)</li> </ul>"},{"location":"Advanced/airgapped/#registryk8sio-optional-supporting-images","title":"registry.k8s.io (Optional Supporting Images)","text":"<ul> <li>registry.k8s.io/ingress-nginx/controller:v1.9.4 (if optionally installing Ingress nGinx via Kinetica Helm Chart)</li> <li>registry.k8s.io/ingress-nginx/controller:v1.9.6@sha256:1405cc613bd95b2c6edd8b2a152510ae91c7e62aea4698500d23b2145960ab9c</li> </ul> <p>It is possible with <code>containerd</code> to pull images, save them and load them either into  a Container Registry in the air gapped environment or directly  into another <code>containerd</code> instance. </p> <p>If the target <code>containerd</code> is on a node running a Kubernetes Cluster then these images will be sourced by Kubernetes from the loaded images, via CRI, with no requirement  to pull them from an external source e.g. a Registry or Mirror.</p> <p><code>sudo</code> required</p> <p>Depending on how <code>containerd</code> has been installed and configured many of the example calls below may require running with <code>sudo</code></p> <p>It is possible with <code>docker</code> to pull images, save them and load them into an OCI Container Registry in the air gapped environment.</p> Pull a remote image (docker)<pre><code>docker pull --platformlinux/amd64 docker.io/kineticastagingcloud/kinetica-k8s-cpu:v7.2.0-3.rc-3\n</code></pre> Export a local image (docker)<pre><code>docker export --platformlinux/amd64 -o kinetica-k8s-cpu-v7.2.0-3.rc-3.tar \\\ndocker.io/kineticastagingcloud/kinetica-k8s-cpu:v7.2.0-3.rc-3\n</code></pre> <p>We can now transfer this archive (<code>kinetica-k8s-cpu-v7.2.0-3.rc-3.tar</code>) to the Kubernetes Node inside  the air-gapped environment.</p>"},{"location":"Advanced/airgapped/#which-kinetica-core-image-do-i-use","title":"Which Kinetica Core Image do I use?","text":"Container Image Intel (AMD64) Intel (AMD64 AVX512) Amd (AMD64) Graviton (aarch64) Apple Silicon (aarch64) kinetica-k8s-cpu (1) kinetica-k8s-cpu-avx512 kinetica-k8s-gpu (2) (2) (2) <ol> <li>It is preferable on an Intel AVX512 enabled CPU to use the kinetica-k8s-cpu-avx512 container image</li> <li>With a supported nVidia GPU.</li> </ol>"},{"location":"Advanced/airgapped/#install-mindthegap","title":"Install <code>mindthegap</code>","text":"Install mindthegap<pre><code>wget https://github.com/mesosphere/mindthegap/releases/download/v1.13.1/mindthegap_v1.13.1_linux_amd64.tar.gz\ntar zxvf mindthegap_v1.13.1_linux_amd64.tar.gz\n</code></pre>"},{"location":"Advanced/airgapped/#mindthegap-create-the-bundle","title":"mindthegap - Create the Bundle","text":"mindthegap create image-bundle<pre><code>mindthegap create image-bundle --images-file mtg.yaml --platform linux/amd64\n</code></pre> <p>where <code>--images-file</code> is either the CPU or GPU Kinetica <code>mindthegap</code> yaml file.</p>"},{"location":"Advanced/airgapped/#mindthegap-import-the-bundle","title":"mindthegap - Import the Bundle","text":""},{"location":"Advanced/airgapped/#mindthegap-import-to-containerd","title":"mindthegap - Import to <code>containerd</code>","text":"mindthegap import image-bundle<pre><code>mindthegap import image-bundle --image-bundle images.tar [--containerd-namespace k8s.io]\n</code></pre> <p>If <code>--containerd-namespace</code> is not specified, images will be imported into <code>k8s.io</code> namespace. </p> <p><code>sudo</code> required</p> <p>Depending on how <code>containerd</code> has been installed and configured it may require running the above command with <code>sudo</code></p>"},{"location":"Advanced/airgapped/#mindthegap-import-to-an-internal-oci-registry","title":"mindthegap - Import to an internal OCI Registry","text":"mindthegap import image-bundle<pre><code>mindthegap push bundle --bundle &lt;path/to/bundle.tar&gt; \\\n--to-registry &lt;registry.address&gt; \\\n[--to-registry-insecure-skip-tls-verify]\n</code></pre>"},{"location":"Advanced/airgapped/#containerd-using-containerd-to-pull-and-export-an-image","title":"containerd - Using <code>containerd</code> to pull and export an image","text":"<p>Similar to <code>docker pull</code> we can use <code>ctr image pull</code> so to pull the core Kinetica DB cpu based image</p> Pull a remote image (containerd)<pre><code>ctr image pull docker.io/kineticastagingcloud/kinetica-k8s-cpu:v7.2.0-3.rc-3\n</code></pre> <p>We now need to export the pulled image as an archive to the local filesystem.</p> Export a local image (containerd)<pre><code>ctr image export kinetica-k8s-cpu-v7.2.0-3.rc-3.tar \\\ndocker.io/kineticastagingcloud/kinetica-k8s-cpu:v7.2.0-3.rc-3\n</code></pre> <p>We can now transfer this archive (<code>kinetica-k8s-cpu-v7.2.0-3.rc-3.tar</code>) to the Kubernetes Node inside  the air-gapped environment.</p>"},{"location":"Advanced/airgapped/#containerd-using-containerd-to-import-an-image","title":"containerd - Using <code>containerd</code> to import an image","text":"<p>Using <code>containerd</code> to import an image on to a Kubernetes Node on which a Kinetica Cluster is running.</p> Import the Images<pre><code>ctr -n=k8s.io images import kinetica-k8s-cpu-v7.2.0-3.rc-3.tar\n</code></pre> <p><code>-n=k8s.io</code></p> <p>Whilst it is possible to use <code>ctr images import kinetica-k8s-cpu-v7.2.0-3.rc-3.tar</code>  to import the image in order for the image to be visible to Kubernets it is necessary  to specify <code>-n=k8s.io</code>.</p>"},{"location":"Advanced/airgapped/#containerd-verifying-the-image-is-available","title":"containerd - Verifying the image is available","text":"<p>To verify the image is loaded into <code>containerd</code> on the node run the following on the node: -</p> Verify containerd Images<pre><code>ctr image ls\n</code></pre> <p>To verify the image is visible to Kubernetes on the node run the following: -</p> Verify CRI Images<pre><code>crictl images\n</code></pre>"},{"location":"Advanced/airgapped/#docker-using-docker-to-import-an-image","title":"docker - Using <code>docker</code> to import an image","text":"<p>Using <code>docker</code> to import an image on to a Kubernetes Node on which a Kinetica Cluster is running.</p> Import the Images<pre><code>docker import --platformlinux/amd64 kinetica-k8s-cpu-v7.2.0-3.rc-3.tar registry:repository/kinetica-k8s-cpu:v7.2.0-3.rc-3\n</code></pre>"},{"location":"Advanced/alternative_charts/","title":"Using Alternative Helm Charts","text":"<p>If requested by Kinetica Support you can search and use pre-release versions of the Kinetica Helm Charts.</p>"},{"location":"Advanced/alternative_charts/#install-from-a-developmentpre-release-chart-version","title":"Install from a development/pre-release chart version","text":"<p>Find all alternative chart versions with:</p> Find alternative chart versions<pre><code>helm search repo kinetica-operators --devel --versions\n</code></pre> <p></p> <p>Then append <code>--devel --version [CHART-DEVEL-VERSION]</code> to the end of the Helm install command.</p> Helm install kinetica-operators<pre><code>helm -n kinetica-system install \\\nkinetica-operators kinetica-operators/kinetica-operators \\\n--create-namespace \\\n--devel \\\n--version 72.0 \\\n--values values.onPrem.k8s.yaml \\\n--set db.gpudbCluster.license=\"LICENSE-KEY\" \\\n--set dbAdminUser.password=\"PASSWORD\" \\\n--set global.defaultStorageClass=\"DEFAULT-STORAGE-CLASS\"\n</code></pre> <p> Advanced Topics</p> <p> Home</p>"},{"location":"Advanced/ingress_configuration/","title":"Ingress Configuration","text":"<ul> <li> <p> <code>ingress-nginx</code> Configuration</p> <p>How to enable Ingress with <code>ingress-nginx</code> for Kinetica DB.</p> <p> <code>ingress-nginx</code></p> </li> <li> <p> <code>nginx-ingress</code> Configuration</p> <p>How to enable Ingress with <code>nginx-ingress</code> for Kinetica DB.</p> <p> <code>nginx-ingress</code></p> </li> </ul> <p> Advanced Topics</p> <p> Home</p>"},{"location":"Advanced/ingress_nginx_config/","title":"<code>ingress-nginx</code> Ingress Configuration","text":"<p>To use an 'external' ingress-nginx controller i.e. not the one optionally installed  by the KInetica Helm chart it is necessary to disable ingress in the <code>KineticaCluster</code> CR.</p> <p>the field <code>spec.ingressController: nginx</code> should be set to <code>spec.ingressController: none</code>.</p> <p>It is then necessary to create the required Ingress CRs by hand. Below is a list of the Ingress paths that need to be exposed along with sample ingress-nginx CRs.</p>"},{"location":"Advanced/ingress_nginx_config/#required-ingress-routes","title":"Required Ingress Routes","text":""},{"location":"Advanced/ingress_nginx_config/#ingress-routes","title":"Ingress Routes","text":""},{"location":"Advanced/ingress_nginx_config/#gadmin-paths","title":"GAdmin Paths","text":"Path Service Port <code>/gadmin</code> <code>cluster-name-gadmin-service</code> <code>gadmin</code> (8080/TCP) <code>/tableau</code> <code>cluster-name-gadmin-service</code> <code>gadmin</code> (8080/TCP) <code>/files</code> <code>cluster-name^-gadmin-service</code> <code>gadmin</code>   (8080/TCP) <p>where <code>cluster-name</code> is the name of the Kinetica Cluster  i.e. what is in the <code>.spec.gpudbCluster.clusterName</code> in the KineticaCluster CR.</p>"},{"location":"Advanced/ingress_nginx_config/#workbench-paths","title":"Workbench Paths","text":"Path Service Port <code>/</code> <code>workbench-workbench-service</code> <code>workbench-port</code> (8000/TCP)"},{"location":"Advanced/ingress_nginx_config/#db-rank-0-paths","title":"DB <code>rank-0</code> Paths","text":"Path Service Port <code>/cluster-145025b8(/gpudb-0(/.*|$))</code> <code>cluster-145025b8-rank0-service</code> <code>httpd</code> (8082/TCP) <code>/cluster-145025b8/gpudb-0/hostmanager(.*)</code> <code>cluster-145025b8-rank0-service</code> <code>hostmanager</code> (9300/TCP)"},{"location":"Advanced/ingress_nginx_config/#db-rank-n-paths","title":"DB <code>rank-N</code> Paths","text":"Path Service Port <code>/cluster-145025b8(/gpudb-N(/.*|$))</code> <code>cluster-145025b8-rank1-service</code> <code>httpd</code> (8082/TCP) <code>/cluster-145025b8/gpudb-N/hostmanager(.*)</code> <code>cluster-145025b8-rank1-service</code> <code>hostmanager</code> (9300/TCP)"},{"location":"Advanced/ingress_nginx_config/#reveal-paths","title":"Reveal Paths","text":"Path Service Port <code>/reveal</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/caravel</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/static</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/logout</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/resetmypassword</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/dashboardmodelview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/dashboardmodelviewasync</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/slicemodelview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/slicemodelviewasync</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/sliceaddview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/databaseview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/databaseasync</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/databasetablesasync</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/tablemodelview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/csstemplatemodelview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/csstemplatemodelviewasync</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/users</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/roles</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/userstatschartview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/permissions</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/viewmenus</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/permissionviews</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/accessrequestsmodelview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/accessrequestsmodelviewasync</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/logmodelview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/logmodelviewasync</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/userinfoeditview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/tablecolumninlineview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/sqlmetricinlineview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP)"},{"location":"Advanced/ingress_nginx_config/#example-ingress-crs","title":"Example Ingress CRs","text":""},{"location":"Advanced/ingress_nginx_config/#example-gadmin-ingress-cr","title":"Example GAdmin Ingress CR","text":"Example GAdmin Ingress CR <p>Example GAdmin Ingress CR<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name:  cluster-name-gadmin-ingress #(1)!\n  namespace: gpudb\nspec:\n  ingressClassName: nginx\n  tls:\n    - hosts:\n        - cluster-name.example.com #(1)!\n      secretName: kinetica-tls\n  rules:\n    - host: cluster-name.example.com #(1)!\n      http:\n        paths:\n          - path: /gadmin\n            pathType: Prefix\n            backend:\n              service:\n                name: cluster-name-gadmin-service #(1)!\n                port:\n                  name: gadmin\n          - path: /tableau\n            pathType: Prefix\n            backend:\n              service:\n                name: cluster-name-gadmin-service #(1)!\n                port:\n                  name: gadmin\n          - path: /files\n            pathType: Prefix\n            backend:\n              service:\n                name: cluster-name-gadmin-service #(1)!\n                port:\n                  name: gadmin\n</code></pre>  1. where <code>cluster-name</code> is the name of the Kinetica Cluster</p>"},{"location":"Advanced/ingress_nginx_config/#example-rank-ingress-cr","title":"Example Rank Ingress CR","text":"Example Rank Ingress CR Example Rank Ingress CR<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: cluster-name-rank1-ingress\n  namespace: gpudb\nspec:\n  ingressClassName: nginx\n  tls:\n    - hosts:\n        - cluster-name.example.com\n      secretName: kinetica-tls\n  rules:\n    - host: cluster-name.example.com\n      http:\n        paths:\n          - path: /cluster-name(/gpudb-1(/.*|$))\n            pathType: Prefix\n            backend:\n              service:\n                name: cluster-name-rank1-service\n                port:\n                  name: httpd\n          - path: /cluster-name/gpudb-1/hostmanager(.*)\n            pathType: Prefix\n            backend:\n              service:\n                name: cluster-name-rank1-service\n                port:\n                  name: hostmanager\n</code></pre> <ol> <li>where <code>cluster-name</code> is the name of the Kinetica Cluster</li> </ol>"},{"location":"Advanced/ingress_nginx_config/#example-reveal-ingress-cr","title":"Example Reveal Ingress CR","text":"Example Reveal Ingress CR <p>Example Reveal Ingress CR<pre><code>    apiVersion: networking.k8s.io/v1\n    kind: Ingress\n    metadata:\n      name: cluster-name-reveal-ingress\n      namespace: gpudb\n    spec:\n      ingressClassName: nginx\n      tls:\n        - hosts:\n            - cluster-name.example.com\n          secretName: kinetica-tls\n      rules:\n        - host: cluster-name.example.com\n          http:\n            paths:\n              - path: /reveal\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /caravel\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /static\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /logout\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /resetmypassword\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /dashboardmodelview\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /dashboardmodelviewasync\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /slicemodelview\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /slicemodelviewasync\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /sliceaddview\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /databaseview\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /databaseasync\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /databasetablesasync\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /tablemodelview\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /tablemodelviewasync\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /csstemplatemodelview\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /csstemplatemodelviewasync\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /users\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /roles\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /userstatschartview\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /permissions\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /viewmenus\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /permissionviews\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /accessrequestsmodelview\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /accessrequestsmodelviewasync\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /logmodelview\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /logmodelviewasync\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /userinfoeditview\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /tablecolumninlineview\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n              - path: /sqlmetricinlineview\n                pathType: Prefix\n                backend:\n                  service:\n                    name: cluster-name-reveal-service\n                    port:\n                      name: reveal\n</code></pre> 1. where <code>cluster-name</code> is the name of the Kinetica Cluster</p>"},{"location":"Advanced/ingress_nginx_config/#exposing-the-postgres-proxy-port","title":"Exposing the Postgres Proxy Port","text":"<p>In order to access Kinetica's Postgres functionality some TCP (not HTTP) ports need to be open externally.</p> <p>For <code>ingress-nginx</code> a configuration file needs to be created to enable port 5432.</p> <p>tcp-services.yaml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tcp-services\n  namespace: kinetica-system # (1)!\ndata:\n  '5432': gpudb/kinetica-k8s-sample-rank0-service:5432 #(2)!\n  '9002': gpudb/kinetica-k8s-sample-rank0-service:9002 #(3)!\n</code></pre> 1. Change the namespace to the namespace your ingress-nginx controller is running in. e.g. <code>ingress-nginx</code>  2. This exposes the postgres proxy port on the default <code>5432</code> port. If you wish to change this to a non-standard port then it needs to be changed here but also in the Helm <code>values.yaml</code> to match. 3. This port is the Table Monitor port and should always be exposed alongside the Postgres Proxy.</p> <p> Ingress Configuration</p> <p>  Advanced Topics</p>"},{"location":"Advanced/ingress_urls/","title":"Ingress urls","text":""},{"location":"Advanced/ingress_urls/#gadmin-paths","title":"GAdmin Paths","text":"Path Service Port <code>/gadmin</code> <code>cluster-name-gadmin-service</code> <code>gadmin</code> (8080/TCP) <code>/tableau</code> <code>cluster-name-gadmin-service</code> <code>gadmin</code> (8080/TCP) <code>/files</code> <code>cluster-name^-gadmin-service</code> <code>gadmin</code>   (8080/TCP) <p>where <code>cluster-name</code> is the name of the Kinetica Cluster  i.e. what is in the <code>.spec.gpudbCluster.clusterName</code> in the KineticaCluster CR.</p>"},{"location":"Advanced/ingress_urls/#workbench-paths","title":"Workbench Paths","text":"Path Service Port <code>/</code> <code>workbench-workbench-service</code> <code>workbench-port</code> (8000/TCP)"},{"location":"Advanced/ingress_urls/#db-rank-0-paths","title":"DB <code>rank-0</code> Paths","text":"Path Service Port <code>/cluster-145025b8(/gpudb-0(/.*|$))</code> <code>cluster-145025b8-rank0-service</code> <code>httpd</code> (8082/TCP) <code>/cluster-145025b8/gpudb-0/hostmanager(.*)</code> <code>cluster-145025b8-rank0-service</code> <code>hostmanager</code> (9300/TCP)"},{"location":"Advanced/ingress_urls/#db-rank-n-paths","title":"DB <code>rank-N</code> Paths","text":"Path Service Port <code>/cluster-145025b8(/gpudb-N(/.*|$))</code> <code>cluster-145025b8-rank1-service</code> <code>httpd</code> (8082/TCP) <code>/cluster-145025b8/gpudb-N/hostmanager(.*)</code> <code>cluster-145025b8-rank1-service</code> <code>hostmanager</code> (9300/TCP)"},{"location":"Advanced/ingress_urls/#reveal-paths","title":"Reveal Paths","text":"Path Service Port <code>/reveal</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/caravel</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/static</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/logout</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/resetmypassword</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/dashboardmodelview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/dashboardmodelviewasync</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/slicemodelview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/slicemodelviewasync</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/sliceaddview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/databaseview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/databaseasync</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/databasetablesasync</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/tablemodelview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/csstemplatemodelview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/csstemplatemodelviewasync</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/users</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/roles</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/userstatschartview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/permissions</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/viewmenus</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/permissionviews</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/accessrequestsmodelview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/accessrequestsmodelviewasync</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/logmodelview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/logmodelviewasync</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/userinfoeditview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/tablecolumninlineview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP) <code>/sqlmetricinlineview</code> <code>cluster-name-reveal-service</code> <code>reveal</code> (8088/TCP)"},{"location":"Advanced/kinetica_images_list_for_airgapped_environments/","title":"Kinetica images list for airgapped environments","text":"Kinetica Images for an Air-Gapped Environment <p>If you are installing Kinetica with Helm in an air-gapped environment you will either need a Registry Proxy to pass the requests through or to download the images and push them to your internal Registry.</p> <p>For information on ways to transfer the files into an air-gapped environment See here.</p>"},{"location":"Advanced/kinetica_images_list_for_airgapped_environments/#required-container-images","title":"Required Container Images","text":""},{"location":"Advanced/kinetica_images_list_for_airgapped_environments/#dockerio-required-kinetica-images-for-all-installations","title":"docker.io (Required Kinetica Images for All Installations)","text":"<ul> <li>docker.io/kineticastagingcloud/kinetica-k8s-operator:v7.2.0-3.rc-3<ul> <li>docker.io/kineticastagingcloud/kinetica-k8s-cpu:v7.2.0-3.rc-3 or </li> <li>docker.io/kineticastagingcloud/kinetica-k8s-cpu-avx512:v7.2.0-3.rc-3 or </li> <li>docker.io/kineticastagingcloud/kinetica-k8s-gpu:v7.2.0-3.rc-3</li> </ul> </li> <li>docker.io/kineticastagingcloud/workbench-operator:v7.2.0-3.rc-3</li> <li>docker.io/kineticastagingcloud/workbench:v7.2.0-3.rc-3</li> <li>docker.io/kineticastagingcloud/kinetica-k8s-monitor:v7.2.0-3.rc-3</li> <li>docker.io/kineticastagingcloud/busybox:v7.2.0-3.rc-3</li> <li>docker.io/kineticastagingcloud/fluent-bit:v7.2.0-3.rc-3</li> <li>docker.io/kinetica/kagent:7.1.9.15.20230823123615.ga</li> </ul>"},{"location":"Advanced/kinetica_images_list_for_airgapped_environments/#nvcrio-required-kinetica-images-for-gpu-installations-using-kinetica-k8s-gpu","title":"nvcr.io (Required Kinetica Images for GPU Installations using <code>kinetica-k8s-gpu</code>)","text":"<ul> <li>nvcr.io/nvidia/gpu-operator:v23.9.1</li> </ul>"},{"location":"Advanced/kinetica_images_list_for_airgapped_environments/#registryk8sio-required-kinetica-images-for-gpu-installations-using-kinetica-k8s-gpu","title":"registry.k8s.io (Required Kinetica Images for GPU Installations using <code>kinetica-k8s-gpu</code>)","text":"<ul> <li>registry.k8s.io/nfd/node-feature-discovery:v0.14.2</li> </ul>"},{"location":"Advanced/kinetica_images_list_for_airgapped_environments/#dockerio-required-supporting-images","title":"docker.io (Required Supporting Images)","text":"<ul> <li>docker.io/bitnami/openldap:2.6.7</li> <li>docker.io/alpine/openssl:latest (used by bitnami/openldap)</li> <li>docker.io/otel/opentelemetry-collector-contrib:0.95.0</li> </ul>"},{"location":"Advanced/kinetica_images_list_for_airgapped_environments/#quayio-required-supporting-images","title":"quay.io (Required Supporting Images)","text":"<ul> <li>quay.io/brancz/kube-rbac-proxy:v0.14.2</li> </ul>"},{"location":"Advanced/kinetica_images_list_for_airgapped_environments/#optional-container-images","title":"Optional Container Images","text":"<p>These images are only required if certain features are enabled as part of the Helm installation: -</p> <ul> <li>CertManager</li> <li>ingress-ninx</li> </ul>"},{"location":"Advanced/kinetica_images_list_for_airgapped_environments/#quayio-optional-supporting-images","title":"quay.io (Optional Supporting Images)","text":"<ul> <li>quay.io/jetstack/cert-manager-cainjector:v1.13.3 (if optionally installing CertManager via Kinetica Helm Chart)</li> <li>quay.io/jetstack/cert-manager-controller:v1.13.3 (if optionally installing CertManager via Kinetica Helm Chart)</li> <li>quay.io/jetstack/cert-manager-webhook:v1.13.3 (if optionally installing CertManager via Kinetica Helm Chart)</li> </ul>"},{"location":"Advanced/kinetica_images_list_for_airgapped_environments/#registryk8sio-optional-supporting-images","title":"registry.k8s.io (Optional Supporting Images)","text":"<ul> <li>registry.k8s.io/ingress-nginx/controller:v1.9.4 (if optionally installing Ingress nGinx via Kinetica Helm Chart)</li> <li>registry.k8s.io/ingress-nginx/controller:v1.9.6@sha256:1405cc613bd95b2c6edd8b2a152510ae91c7e62aea4698500d23b2145960ab9c</li> </ul>"},{"location":"Advanced/kinetica_images_list_for_airgapped_environments/#which-kinetica-core-image-do-i-use","title":"Which Kinetica Core Image do I use?","text":"Container Image Intel (AMD64) Intel (AMD64 AVX512) Amd (AMD64) Graviton (aarch64) Apple Silicon (aarch64) kinetica-k8s-cpu (1) kinetica-k8s-cpu-avx512 kinetica-k8s-gpu (2) (2) (2) <ol> <li>It is preferable on an Intel AVX512 enabled CPU to use the kinetica-k8s-cpu-avx512 container image</li> <li>With a supported nVidia GPU.</li> </ol>"},{"location":"Advanced/kinetica_mac_arm_k8s/","title":"Kinetica DB on Kubernetes","text":"<p>This walkthrough will show how to install Kinetica DB on a Mac running OS X. The Kubernetes cluster will be running on VMs with Ubuntu Linux 22.04 ARM64. </p> <p>This solution is equivalent to a production bare metal installation and does  not use Docker or QEMU but rather Apple native Virtualization.</p> <p>The Kubernetes cluster will consist of one Master node <code>k8smaster1</code> and two Worker nodes <code>k8snode1</code> &amp; <code>k8snode2</code>.</p> <p>The virtualization platform is UTM. </p> <p>Obtain a Kinetica License Key</p> <p>A product license key will be required for install. Please contact Kinetica Support to request a trial key.</p> <p>Download and install UTM.</p>"},{"location":"Advanced/kinetica_mac_arm_k8s/#create-the-vms","title":"Create the VMs","text":""},{"location":"Advanced/kinetica_mac_arm_k8s/#k8smaster1","title":"<code>k8smaster1</code>","text":"<p>For this walkthrough the master node will be 4 vCPU, 8 GB RAM &amp; 40-64 GB disk.</p> <p>Start the creation of a new VM in UTM. Select <code>Virtualize</code></p> <p></p> <p>Select Linux as the VM OS.</p> <p></p> <p>On the Linux page - Select <code>Use Apple Virtualization</code> and an Ubuntu 22.04 (Arm64) ISO.</p> <p></p> <p>As this is the master Kubernetes node (VM) it can be smaller than the nodes hosting the Kinetica DB itself.</p> <p>Set the memory to 8 GB and the number of CPUs to 4.</p> <p></p> <p>Set the storage to between 40-64 GB.</p> <p></p> <p>This next step is optional if you wish to setup a shared folder between your Mac host &amp; the Linux VM.</p> <p></p> <p>The final step to create the VM is a summary. Please check the values shown and hit <code>Save</code></p> <p></p> <p>You should now see your new VM in the left hand pane of the UTM UI.</p> <p></p> <p>Go ahead and click the  button.</p> <p>Once the Ubuntu installer comes up follow the steps selecting whichever keyboard etc. you require.</p> <p>The only changes you need to make are: -</p> <ul> <li>Change the installation to <code>Ubuntu Server (minimized)</code></li> <li>Your server's name to <code>k8smaster1</code></li> <li>Enable OpenSSH server.</li> </ul> <p>and complete the installation.</p> <p>Reboot the VM, remove the ISO from the 'external' drive .  Log in to the VM and get the VMs IP address with</p> Bash<pre><code>ip a\n</code></pre> <p>Make a note of the IP for later use.</p>"},{"location":"Advanced/kinetica_mac_arm_k8s/#k8snode1-k8snode2","title":"<code>k8snode1</code> &amp; <code>k8snode2</code>","text":"<p>Repeat the same process to provision one or two nodes depending on how much memory you have available on the Mac.</p> <p>You need to change the RAM size to 16 GB. You can leave the vCPU count at 4.  For the disk size that depends on how much data you want to ingest.  It should however be at least 4x RAM size.</p> <p>Once installed again log in to the VM and get the VMs IP address with</p> Bash<pre><code>ip a\n</code></pre> <p>Note</p> <p>Make a note of the IP(s) for later use.</p> <p>Your VMs are complete</p> <p>Continue installing your new VMs by following  Bare Metal/VM Installation</p>"},{"location":"Advanced/kube_vip_loadbalancer/","title":"Kubernetes Cluster LoadBalancer for Bare Metal/VM Installations","text":"<p>For our example we are going to enable a Kubernetes based LoadBalancer to issue IP addresses to our Kubernetes Services of type <code>LoadBalancer</code> using <code>kube-vip</code>.</p>"},{"location":"Advanced/kube_vip_loadbalancer/#kube-vip","title":"<code>kube-vip</code>","text":"<p>We will install two components into our Kubernetes CLuster</p> <ul> <li>kube-vip-cloud-controller</li> <li>Kubernetes Load-Balancer Service</li> </ul>"},{"location":"Advanced/kube_vip_loadbalancer/#kube-vip-cloud-controller","title":"kube-vip-cloud-controller","text":"<p>Quote</p> <p>The kube-vip cloud provider can be used to populate  an IP address for Services of type LoadBalancer similar to what  public cloud providers allow through a Kubernetes CCM.</p> Install the kube-vip CCM<pre><code>kubectl apply -f https://raw.githubusercontent.com/kube-vip/kube-vip-cloud-provider/main/manifest/kube-vip-cloud-controller.yaml\n</code></pre> <p>Now we need to setup the required RBAC permissions: -</p> Bash<pre><code>kubectl apply -f https://kube-vip.io/manifests/rbac.yaml\n</code></pre> <p>The following ConfigMap will configure the <code>kube-vip-cloud-controller</code> to obtain IP addresses from the host networks DHCP server. i.e. the DHCP on the physical network that the host machine or VM is connected to.</p> YAML<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kubevip\n  namespace: kube-system\ndata:\n  cidr-global: 0.0.0.0/32\n</code></pre> <p>It is possible to specify IP address ranges see here.</p>"},{"location":"Advanced/kube_vip_loadbalancer/#kubernetes-load-balancer-service","title":"Kubernetes Load-Balancer Service","text":"YAML<pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-vip-ds\n    app.kubernetes.io/version: v0.7.2\n  name: kube-vip-ds\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-vip-ds\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-vip-ds\n        app.kubernetes.io/version: v0.7.2\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: Exists\n            - matchExpressions:\n              - key: node-role.kubernetes.io/control-plane\n                operator: Exists\n      containers:\n      - args:\n        - manager\n        env:\n        - name: vip_arp\n          value: \"true\"\n        - name: port\n          value: \"6443\"\n        - name: vip_interface\n          value: enp0s1\n        - name: vip_cidr\n          value: \"32\"\n        - name: dns_mode\n          value: first\n        - name: cp_enable\n          value: \"true\"\n        - name: cp_namespace\n          value: kube-system\n        - name: svc_enable\n          value: \"true\"\n        - name: svc_leasename\n          value: plndr-svcs-lock\n        - name: vip_leaderelection\n          value: \"true\"\n        - name: vip_leasename\n          value: plndr-cp-lock\n        - name: vip_leaseduration\n          value: \"5\"\n        - name: vip_renewdeadline\n          value: \"3\"\n        - name: vip_retryperiod\n          value: \"1\"\n        - name: address\n          value: 192.168.3.199\n        - name: prometheus_server\n          value: :2112\n        image: ghcr.io/kube-vip/kube-vip:v0.7.2\n        imagePullPolicy: Always\n        name: kube-vip\n        resources: {}\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n      hostNetwork: true\n      serviceAccountName: kube-vip\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      - effect: NoExecute\n        operator: Exists\n  updateStrategy: {}\n</code></pre> <p>Example showing DHCP allocated external IP address to the Ingress Controller.</p> <p></p> <p>  Home</p>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/","title":"Bare Metal/VM Installation - <code>kubeadm</code>","text":""},{"location":"Advanced/kubernetes_bare_metal_vm_install/#kubernetes-node-installation","title":"Kubernetes Node Installation","text":""},{"location":"Advanced/kubernetes_bare_metal_vm_install/#setup-the-kubernetes-nodes","title":"Setup the Kubernetes Nodes","text":""},{"location":"Advanced/kubernetes_bare_metal_vm_install/#edit-etchosts","title":"Edit <code>/etc/hosts</code>","text":"<p>SSH into each of the nodes and run the following: -</p> Edit `/etc/hosts<pre><code>sudo vi /etc/hosts\n\nx.x.x.x k8smaster1\nx.x.x.x k8snode1\nx.x.x.x k8snode2\n</code></pre> <p>where x.x.x.x is the IP Address of the corresponding nose.</p>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#disable-linux-swap","title":"Disable Linux Swap","text":"<p>Next we need to disable Swap on Linux: -</p> Disable Swap <p></p> Disable Swap<pre><code>sudo swapoff -a\n\nsudo vi /etc/fstab\n</code></pre> <p>comment out the swap entry in <code>/etc/fstab</code> on each node.</p>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#linux-system-configuration-changes","title":"Linux System Configuration Changes","text":"<p>We are using containerd as the container runtime but in order to do so we need to make some system level changes on Linux.</p> Linux System Configuration Changes <p></p> Linux System Configuration Changes<pre><code>cat &lt;&lt; EOF | sudo tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\n\nsudo modprobe br_netfilter\n\ncat &lt;&lt; EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n\nsudo sysctl --system\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#container-runtime-installation","title":"Container Runtime Installation","text":"<p>Run on all nodes (VMs)</p> <p>Run the following commands, until advised not to, on all of the VMs you created.</p>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#install-containerd","title":"Install <code>containerd</code>","text":"Install <code>containerd</code> Install `containerd`<pre><code>sudo apt update\n\nsudo apt install -y containerd\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#create-a-default-containerd-config","title":"Create a Default <code>containerd</code> Config","text":"Create a Default <code>containerd</code> Config Create a Default `containerd` Config<pre><code>sudo mkdir -p /etc/containerd\n\nsudo containerd config default | sudo tee /etc/containerd/config.toml\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#enable-system-cgroup","title":"Enable System CGroup","text":"<p>Change the SystemdCgroup value to true in the containerd configuration file and restart the service</p> Enable System CGroup <p></p> Enable System CGroup<pre><code>sudo sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml\n\nsudo systemctl restart containerd\nsudo systemctl enable containerd\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#install-pre-requisiteutility-packages","title":"Install Pre-requisite/Utility packages","text":"Install Pre-requisite/Utility packages Install Pre-requisite/Utility packages<pre><code>sudo apt update\n\nsudo apt install -y apt-transport-https ca-certificates curl gpg git\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#download-the-kubernetes-public-signing-key","title":"Download the Kubernetes public signing key","text":"Download the Kubernetes public signing key Download the Kubernetes public signing key<pre><code>curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#add-the-kubernetes-package-repository","title":"Add the Kubernetes Package Repository","text":"Add the Kubernetes Package Repository<pre><code>echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#install-the-kubernetes-installation-and-management-tools","title":"Install the Kubernetes Installation and Management Tools","text":"Install the Kubernetes Installation and Management Tools Install the Kubernetes Installation and Management Tools<pre><code>sudo apt update\n\nsudo apt install -y kubeadm=1.29.0-1.1  kubelet=1.29.0-1.1  kubectl=1.29.0-1.1 \n\nsudo apt-mark hold kubeadm kubelet kubectl\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#initialize-the-kubernetes-cluster","title":"Initialize the Kubernetes Cluster","text":"<p>Initialize the Kubernetes Cluster by using kubeadm on the <code>k8smaster1</code> control plane node.</p> <p>Note</p> <p>You will need an IP Address range for the Kubernetes Pods. This range is provided to <code>kubeadm</code> as part of the initialization. For our cluster of three nodes, given the default number of pods supported by a node (110) we need a CIDR of at least 330 distinct IP Addresses. Therefore, for this example we will use a <code>--pod-network-cidr</code> of <code>10.1.1.0/22</code> which allows for 1007 usable IPs. The reason for this is each node will get <code>/24</code> of the <code>/22</code> total.</p> <p>The <code>apiserver-advertise-address</code> should be the IP Address of the <code>k8smaster1</code> VM.</p> Initialize the Kubernetes Cluster <p></p> Initialize the Kubernetes Cluster<pre><code>sudo kubeadm init --pod-network-cidr 10.1.1.0/22 --apiserver-advertise-address 192.168.2.180 --kubernetes-version 1.29.2\n</code></pre> <p>You should now deploy a pod network to the cluster. Run <code>kubectl apply -f [podnetwork].yaml</code> with one of the options listed at: Cluster Administration Addons</p> <p>Make a note of the portion of the shell output which gives the join command which we will need to add our worker nodes to the master.</p> <p>Copy the <code>kudeadm join</code> command</p> <p>Then you can join any number of worker nodes by running the following on each as root:</p> Copy the `kudeadm join` command<pre><code>kubeadm join 192.168.2.180:6443 --token wonuiv.v93rkizr6wvxwe6l \\\n--discovery-token-ca-cert-hash sha256:046ffa6303e6b281285a636e856b8e9e51d8c755248d9d013e15ae5c5f6bb127\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#setup-kubeconfig","title":"Setup <code>kubeconfig</code>","text":"<p>Before we add the worker nodes we can setup the <code>kubeconfig</code> so we will be able to use <code>kubectl</code> going forwards.</p> Setup <code>kubeconfig</code> <p></p> Setup `kubeconfig`<pre><code>sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#connect-list-the-kubernetes-cluster-nodes","title":"Connect &amp; List the Kubernetes Cluster Nodes","text":"<p>We can now run <code>kubectl</code> to connect to the Kubernetes API Server to display the nodes in the newly created Kubernetes CLuster.</p> Connect &amp; List the Kubernetes Cluster Nodes <p></p> Connect &amp; List the Kubernetes Cluster Nodes<pre><code>kubectl get nodes\n</code></pre> <p>STATUS = NotReady</p> <p>From the <code>kubectl</code> output the status of the <code>k8smaster1</code> node is showing as <code>NotReady</code> as we have yet to install the Kubernetes Network to the cluster.</p> <p>We will be installing <code>cilium</code> as that provider in a future step.</p> <p>Warning</p> <p>At this point we should complete the installations of the worker nodes to this same point before continuing.</p>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#join-the-worker-nodes-to-the-cluster","title":"Join the Worker Nodes to the Cluster","text":"<p>Once installed we run the join on the worker nodes. Note that the command which was output from the <code>kubeadm init</code> needs to run with <code>sudo</code></p> Join the Worker Nodes to the Cluster<pre><code>sudo kubeadm join 192.168.2.180:6443 --token wonuiv.v93rkizr6wvxwe6l \\\n    --discovery-token-ca-cert-hash sha256:046ffa6303e6b281285a636e856b8e9e51d8c755248d9d013e15ae5c5f6bb127\n</code></pre> <code>kubectl get nodes</code> <p></p> <p>Now we can again run</p> `kubectl get nodes`<pre><code>kubectl get nodes\n</code></pre> <p>Now we can see all the nodes are present in the Kubernetes Cluster.</p> <p>Run on Head Node only</p> <p>From now the following cpmmands need to be run on the Master Node only..</p>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#install-kubernetes-networking","title":"Install Kubernetes Networking","text":"<p>We now need to install a Kubernetes CNI (Container Network Interface) to enable the pod network.</p> <p>We will use Cilium as the CNI for our cluster.</p> Installing the Cilium CLI<pre><code>curl -LO https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-arm64.tar.gz\nsudo tar xzvfC cilium-linux-arm64.tar.gz /usr/local/bin\nrm cilium-linux-arm64.tar.gz\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#install-cilium","title":"Install <code>cilium</code>","text":"<p>You can now install Cilium with the following command:</p> Install `cilium`<pre><code>cilium install\ncilium status \n</code></pre> <p>If <code>cilium status</code> shows errors you may need to wait until the Cilium pods have started.</p> <p>You can check progress with</p> Bash<pre><code>kubectl get po -A\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#check-cilium-status","title":"Check <code>cilium</code> Status","text":"<p>Once Cilium the Cilium pods are running we can check the status of Cilium again by using</p> Check <code>cilium</code> Status <p></p> Check `cilium` Status<pre><code>cilium status \n</code></pre> <p>We can now recheck the Kubernetes Cluster Nodes</p> <p></p> Bash<pre><code>kubectl get nodes\n</code></pre> <p>and they should have <code>Status Ready</code></p>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#kubernetes-node-preparation","title":"Kubernetes Node Preparation","text":""},{"location":"Advanced/kubernetes_bare_metal_vm_install/#label-kubernetes-nodes","title":"Label Kubernetes Nodes","text":"<p>Now we go ahead and label the nodes. Kinetica uses node labels in production clusters where there are separate 'node groups'  configured so that the Kinetica Infrastructure pods are deployed on a smaller VM type and the DB itself is deployed on larger nodes or gpu enabled nodes.</p> <p>If we were using a Cloud Provider Kubernetes these are synonymous with EKS Node Groups or AKS VMSS which would be created with the same two labels on two node groups.</p> Label Kubernetes Nodes<pre><code>kubectl label node k8snode1 app.kinetica.com/pool=infra\nkubectl label node k8snode2 app.kinetica.com/pool=compute\n</code></pre> <p>additionally in our case as we have created a new cluster the 'role' of the worker nodes is not set so we can also set that. In many cases the role is already set to <code>worker</code> but here we have some latitude.</p> <p></p> Bash<pre><code>kubectl label node k8snode1 kubernetes.io/role=kinetica-infra\nkubectl label node k8snode2 kubernetes.io/role=kinetica-compute\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#install-storage-class","title":"Install Storage Class","text":"<p>Install a local path provisioner storage class. In this case we are using the Rancher Local Path provisioner</p> Install Storage Class <p></p> Install Storage Class<pre><code>kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.26/deploy/local-path-storage.yaml\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#set-default-storage-class","title":"Set Default Storage Class","text":"Set Default Storage Class Set Default Storage Class<pre><code>kubectl patch storageclass local-path -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n</code></pre> <p>Kubernetes Cluster Provision Complete</p> <p>Your basre Kubernetes Cluster is now complete and ready to have the Kinetica DB installed on it using the Helm Chart.</p>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#install-kinetica-for-kubernetes-using-helm","title":"Install Kinetica for Kubernetes using Helm","text":""},{"location":"Advanced/kubernetes_bare_metal_vm_install/#add-the-helm-repository","title":"Add the Helm Repository","text":"Add the Helm Repository Add the Helm Repository<pre><code>helm repo add kinetica-operators https://kineticadb.github.io/charts\nhelm repo update\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#download-a-starter-helm-valuesyaml","title":"Download a Starter Helm <code>values.yaml</code>","text":"<p>Now we need to obtain a starter <code>values.yaml</code> file to pass to our Helm install. We can download one from the <code>github.com/kineticadb/charts</code> repo.</p> Download a Starter Helm <code>values.yaml</code> <p></p> Download a Starter Helm `values.yaml`<pre><code>    wget https://raw.githubusercontent.com/kineticadb/charts/master/kinetica-operators/values.onPrem.k8s.yaml\n</code></pre> <p>Obtain a Kinetica License Key</p> <p>A product license key will be required for install. Please contact Kinetica Support to request a trial key.</p>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#helm-install-kinetica","title":"Helm Install Kinetica","text":"#### Helm Install Kinetica Helm install kinetica-operators<pre><code>helm -n kinetica-system upgrade -i \\\nkinetica-operators kinetica-operators/kinetica-operators \\\n--create-namespace \\\n--values values.onPrem.k8s.yaml \\\n--set db.gpudbCluster.license=\"LICENSE-KEY\" \\\n--set dbAdminUser.password=\"PASSWORD\" \\\n--set global.defaultStorageClass=\"local-path\"\n</code></pre>"},{"location":"Advanced/kubernetes_bare_metal_vm_install/#monitor-kinetica-startup","title":"Monitor Kinetica Startup","text":"<p>After a few moments, follow the progression of the main database pod startup with:</p> Monitor the Kinetica installation progress<pre><code>kubectl -n gpudb get po gpudb-0 -w\n</code></pre> <p>Kinetica DB Provision Complete</p> <p>Once you see <code>gpudb-0 3/3 Running</code> the database is up and running.</p> <p>Software LoadBalancer</p> <p>If you require a software based LoadBalancer to allocate IP address to the  Ingress Controller or exposed Kubernetes Services then see here</p> <p>This is usually apparent if your ingress or other Kubernetes Services with the type <code>LoadBalancer</code> are stuck in the <code>Pending</code> state.</p> <p>  Home</p>"},{"location":"Advanced/minio_s3_dev_test/","title":"Using Minio for S3 Storage in Dev/Test","text":"<p>If you require a new Minio installation</p> <p>Please follow the installation instructions found  here  to install the Minio Operator and to create your first tenant.</p>"},{"location":"Advanced/minio_s3_dev_test/#create-minio-tenant","title":"Create Minio Tenant","text":"<p>In our example below we have created a tenant <code>kinetica</code> in the <code>gpudb</code> namespace using  the Kinetica storage class <code>kinetica-k8s-sample-storageclass</code>.</p> <p></p> <p>In that tenant we created a bucket <code>kinetica-cold-storage</code> and in that bucket we created the path <code>gpudb/cold-storage</code>.</p> <p></p> <p>Once you have a tenant up and running we can configure Kinetica for Kubernetes to use  it as the DB Cold Storage tier.</p> <p>Backup/Restore Storage</p> <p>Minio can also be used as the S3 storage for Velero. This enables Backup/Restore functionality via the <code>KineticaBackup</code> &amp; <code>KineticaRestore</code> CRs.</p>"},{"location":"Advanced/minio_s3_dev_test/#configuring-kinetica-to-use-minio","title":"Configuring Kinetica to use Minio","text":""},{"location":"Advanced/minio_s3_dev_test/#cold-storage","title":"Cold Storage","text":"<p>In order to configure the Cold Storage Tier for the Database it is necessary to add a  <code>coldStorageTier</code> to the <code>KineticaCluster</code> CR. As we are using S3 Buckets for storage we then require <code>coldStorageS3</code> entry which allows us to set the <code>awsSecretAccessKey</code> &amp; <code>awsAccessKeyId</code> which were generated when the tenant was created in Minio. </p> <p>If we look in the <code>gpudb</code> name space we can see that Minio created a  Kubernetes service called <code>minio</code> exposed on port <code>443</code>.   </p> <p>In the <code>coldStorageS3</code> we need to add an <code>endpoint</code> field which contains the <code>minio</code> service name and the namespace <code>gpudb</code> i.e. <code>s3://https://minio.gpudb.svc.cluster.local</code>.</p>"},{"location":"Advanced/minio_s3_dev_test/#kineticacluster-coldstoragetier-s3-configurationspec-gpudbcluster-config-tieredstorage-coldstoragetier-coldstorages3-name-basepath-gpudbcold-storage-bucketname-kinetica-cold-storage-usevirtualaddressing-false-usemanagedcredentials-false-endpoint-s3httpsminiogpudbsvcclusterlocal-awssecretaccesskey-6rlaooddp3kstwpdhf47xlhrepdbqdav-awsaccesskeyid-vvlp5rhbqqzcyphg-tieredstrategy-default-vram-1-ram-5-persist-5-cold0-10","title":"`KineticaCluster coldStorageTier` S3 Configuration<pre><code>spec:\n  gpudbCluster:\n    config:\n      tieredStorage:\n        coldStorageTier:\n          coldStorageS3:\n            name: ''\n            basePath: gpudb/cold-storage/\n            bucketName: kinetica-cold-storage\n            useVirtualAddressing: false\n            useManagedCredentials: false\n            endpoint: s3://https://minio.gpudb.svc.cluster.local\n            awsSecretAccessKey: 6rLaOOddP3KStwPDhf47XLHREPdBqdav\n            awsAccessKeyId: VvlP5rHbQqzcYPHG\n      tieredStrategy:\n        default: VRAM 1, RAM 5, PERSIST 5, COLD0 10\n</code></pre>","text":""},{"location":"Advanced/minio_s3_dev_test/#velero-backups","title":"Velero Backups","text":""},{"location":"Advanced/minio_s3_dev_test/#coming-soon","title":"Coming Soon","text":"<p>  Home</p>"},{"location":"Advanced/nginx_ingress_config/","title":"<code>nginx-ingress</code> Ingress Configuration","text":""},{"location":"Advanced/nginx_ingress_config/#coming-soon","title":"Coming Soon","text":"<p> Ingress Configuration</p> <p>  Advanced Topics</p>"},{"location":"Advanced/velero_backup_restore/","title":"Velero for Backup/Restore","text":""},{"location":"Advanced/velero_backup_restore/#coming-soon","title":"Coming Soon","text":"<p>  Home</p>"},{"location":"Architecture/","title":"Architecture","text":"<p>Kinetica is a distributed, vectorized, memory-first, columnar database with tiered storage that is optimized for high speed and performance \u2013 particularly on streaming analytics and geospatial workloads.</p> <p>Kinetica has been uniquely designed for fast and flexible analytics on large volumes of changing data with incredible performance.</p> <ul> <li> <p> Kinetica Database Architecture</p> <p>Install the Kinetica DB with helm and get up and running in minutes</p> <p> Core Database Architecture</p> </li> <li> <p> Kinetica for Kubernetes Architecture</p> <p>Install the Kinetica DB with helm and get up and running in minutes</p> <p> Kubernetes Architecture</p> </li> </ul>"},{"location":"Architecture/db_architecture/","title":"Architecture","text":"<p>Kinetica is a distributed, vectorized, memory-first, columnar database with tiered storage that is optimized for high speed and performance \u2013 particularly on streaming analytics and geospatial workloads.</p> <p>Kinetica has been uniquely designed for fast and flexible analytics on large volumes of changing data with incredible performance.</p>"},{"location":"Architecture/db_architecture/#database-architecture","title":"Database Architecture","text":""},{"location":"Architecture/db_architecture/#scale-out-architecture","title":"Scale-out Architecture","text":"<p>Kinetica has a distributed architecture that has been designed for data processing at scale. A standard cluster consists of identical nodes run on commodity hardware. A single node is chosen to be the head aggregation node.</p> <p> A cluster can be scaled up at any time to increase storage capacity and processing power, with near-linear scale processing improvements for most operations. Sharding of data can be done automatically, or specified and optimized by the user.</p>"},{"location":"Architecture/db_architecture/#distributed-ingest-query","title":"Distributed Ingest &amp; Query","text":"<p>Kinetica uses a shared-nothing data distribution across worker nodes. The head node receives a query and breaks it down into small tasks that can be spread across worker nodes. To avoid bottlenecks at the head node, ingestion can also be organized in parallel by all the worker nodes. Kinetica is able to distribute data client-side before sending it to designated worker nodes. This streamlines communication and processing time.</p> <p>For the client application, there is no need to be aware of how many nodes are in the cluster, where they are, or how the data is distributed across them!</p> <p></p>"},{"location":"Architecture/db_architecture/#column-oriented","title":"Column Oriented","text":"<p>Columnar data structures lend themselves to low-latency reads of data. But from a user's perspective, Kinetica behaves very similarly to a standard relational database \u2013 with tables of rows and columns and it can be queried with SQL or through APIs. Available column types include the standard base types (int, long, float, double, string, &amp; bytes), as well as numerous sub-types supporting date/time, geospatial, and other data forms.</p> <p></p>"},{"location":"Architecture/db_architecture/#vectorized-functions","title":"Vectorized Functions","text":"<p>Vectorization is Kinetica\u2019s secret sauce and the key feature that underpins its blazing fast performance.</p> <p>Advanced vectorized kernels are optimized to use vectorized CPUs and GPUs for faster performance. The query engine automatically assigns tasks to the processor where they will be most performant. Aggregations, filters, window functions, joins and geospatial rendering are some of the capabilities that see performance improvements.</p> <p></p>"},{"location":"Architecture/db_architecture/#memory-first-tiered-storage","title":"Memory-First, Tiered Storage","text":"<p>Tiered storage makes it possible to optimize where data lives for performance and cost. Recent data (such as all data where the timestamp is within the last 2 weeks) can be held in-memory, while older data can be moved to disk, or even to external storage services.</p> <p>Kinetica operates on an entire data corpus by intelligently managing data across GPU memory, system memory, SIMD, disk / SSD, HDFS, and cloud storage like S3 for optimal performance.</p> <p>Kinetica can also query and process data stored in data lakes, joining it with data managed by Kinetica in highly parallelized queries.</p>"},{"location":"Architecture/db_architecture/#performant-key-value-lookup","title":"Performant Key-Value Lookup","text":"<p>Kinetica is able to generate distributed key-value lookups, from columnar data, for high-performance and concurrency. Sharding logic is embedded directly within client APIs enabling linear scale-out as clients can lookup data directly from the node where the data lives.</p>"},{"location":"Architecture/kinetica_for_kubernetes_architecture/","title":"Kubernetes Architecture","text":""},{"location":"Architecture/kinetica_for_kubernetes_architecture/#coming-soon","title":"Coming Soon","text":""},{"location":"GettingStarted/","title":"Getting Started","text":"<ul> <li> <p> Set up in 15 minutes (local install)</p> <p>Install the Kinetica DB locally on <code>Kind</code> or <code>k3s</code> with <code>helm</code> to get up and running in minutes (Dev/Test).</p> <p> Quickstart</p> </li> <li> <p> Prepare to Install</p> <p>What you need to know &amp; do before beginning an installation.</p> <p> Preparation and Prerequisites</p> </li> <li> <p> Production Installation</p> <p>Install the Kinetica DB with helm to get up and running quickly (Production).</p> <p> Installation</p> </li> <li> <p> Channel Your Inner Ninja</p> <p>Advanced Installation Topics which go beyond the basic installation.</p> <p> Advanced Topics</p> </li> </ul>"},{"location":"GettingStarted/aks/","title":"Azure AKS Specifics","text":"<p>This page covers any Microsoft Azure AKS cluster installation specifics.</p>"},{"location":"GettingStarted/eks/","title":"Amazon EKS Specifics","text":"<p>This page covers any Amazon EKS kubernetes cluster installation specifics.</p>"},{"location":"GettingStarted/eks/#ebs-csi-driver","title":"EBS CSI driver","text":"<p>Warning</p> <p>Make sure you have enabled the ebs-csi driver in your EKS cluster. This is required for the default storage class to work.</p> <p>Please refer to this AWS documentation for more information.</p>"},{"location":"GettingStarted/installation/","title":"Kinetica for Kubernetes Installation","text":"<ul> <li> <p> CPU Only Installation </p> <p>Install the Kinetica DB to run on Intel, AMD or ARM CPUs with no GPU acceleration.</p> <p> CPU</p> </li> <li> <p> CPU &amp; GPU Installation</p> <p>Install the Kinetica DB to run on nodes with nVidia GPU acceleration.</p> <p> GPU</p> </li> </ul> <p> Home</p>"},{"location":"GettingStarted/installation_cpu/","title":"Installation - CPU Only","text":"<p>For managed Kubernetes solutions (AKS, EKS, GKE) or on-prem (kubeadm) Kubernetes variants,  follow this generic guide to install the Kinetica Operators, Database and Workbench.</p> <p>Preparation &amp; Prequisites</p> <p>Please make sure you have followed the Preparation &amp; Prequisites steps</p>"},{"location":"GettingStarted/installation_cpu/#4-install-the-helm-chart","title":"4. Install the helm chart","text":"<p>Run the following Helm install command after substituting values from section 3</p> Helm install kinetica-operators<pre><code>helm -n kinetica-system install \\\nkinetica-operators kinetica-operators/kinetica-operators \\\n--create-namespace \\\n--values values.onPrem.k8s.yaml \\\n--set db.gpudbCluster.license=\"LICENSE-KEY\" \\\n--set dbAdminUser.password=\"PASSWORD\" \\\n--set global.defaultStorageClass=\"DEFAULT-STORAGE-CLASS\"\n</code></pre>"},{"location":"GettingStarted/installation_cpu/#5-check-installation-progress","title":"5. Check installation progress","text":"<p>After a few moments, follow the progression of the main database pod startup with:</p> Monitor the Kinetica installation progress<pre><code>kubectl -n gpudb get po gpudb-0 -w\n</code></pre> <p>until it reaches <code>\"gpudb-0  3/3  Running\"</code> at which point the database should be ready and all other software installed in the cluster. You may have to run this command in a different terminal if the <code>helm</code> command from step 4 has not yet returned to the system prompt. Once running, you can quit this kubectl watch command using ctrl-c.</p>"},{"location":"GettingStarted/installation_cpu/#6-accessing-the-kinetica-installation","title":"6. Accessing the Kinetica installation","text":""},{"location":"GettingStarted/installation_cpu/#target-platform-specifics","title":"Target Platform Specifics","text":"cloudlocal - devbare metal - prod <p>If you are installing into a managed Kubernetes environment and the NGINX ingress controller that is installed as part of this install creates a LoadBalancer service, you may need to associate the LoadBalancer with the domain you plan to use.</p> <p>As of now, the kinetica-operator chart installs NGINX ingress controller.  So after the installation is complete, you may need to edit the KineticaCluster Custom Resource  and Workbench Custom Resource with the correct domain name.</p> <p>Option 1: Use the LoadBalancer domain  Set your FQDN in Kinetica<pre><code>kubectl get svc -n kinetica-system\n# look at the loadbalancer dns name, copy it\n\nkubectl -n gpudb edit $(kubectl -n gpudb get kc -o name)\n# replace local.kinetica with the loadbalancer dns name\nkubectl -n gpudb edit $(kubectl -n gpudb get wb -o name)\n# replace local.kinetica with the loadbalancer dns name\n# save and exit\n# you should be able to access the workbench from the loadbalancer dns name\n</code></pre></p> <p>Option 2: Use your custom domain  Create a record in your DNS server pointing to the LoadBalancer DNS. Then edit the KineticaCluster Custom Resource and Workbench Custom Resource with the correct domain name, as mentioned above.</p> <p>Installing on a local machine which does not have a domain name, you can add the following entry to your <code>/etc/hosts</code> file or equivalent:</p> Configure local acces - /etc/hosts<pre><code>127.0.0.1  local.kinetica\n</code></pre> <p>Note</p> <p>The default chart configuration points to <code>local.kinetica</code> but this is configurable.</p> <p>Installing on a bare metal machines which do not have an external hardware loadbalancer requires an  Ingress controller along with a software loadbalancer in order to be accessible. </p> <p>Kinetica for Kubernetes has been tested with  kube-vip</p>"},{"location":"GettingStarted/installation_gpu/","title":"Installation - CPU with GPU Acceleration","text":"<p>For managed Kubernetes solutions (AKS, EKS, GKE) or on-prem (kubeadm) Kubernetes variants, follow this generic guide to install the Kinetica Operators, Database and Workbench.</p> <p>Preparation &amp; Prequisites</p> <p>Please make sure you have followed the Preparation &amp; Prequisites steps</p>"},{"location":"GettingStarted/installation_gpu/#4-install-the-helm-chart","title":"4. Install the helm chart","text":"<p>Run the following Helm install command after substituting values from section 3</p> Helm install kinetica-operators<pre><code>helm -n kinetica-system install \\\nkinetica-operators kinetica-operators/kinetica-operators \\\n--create-namespace \\\n--values values.onPrem.k8s.yaml \\\n--set db.gpudbCluster.license=\"LICENSE-KEY\" \\\n--set dbAdminUser.password=\"PASSWORD\" \\\n--set global.defaultStorageClass=\"DEFAULT-STORAGE-CLASS\"\n</code></pre>"},{"location":"GettingStarted/installation_gpu/#5-check-installation-progress","title":"5. Check installation progress","text":"<p>After a few moments, follow the progression of the main database pod startup with:</p> Monitor the Kinetica installation progress<pre><code>kubectl -n gpudb get po gpudb-0 -w\n</code></pre> <p>until it reaches <code>\"gpudb-0  3/3  Running\"</code> at which point the database should be ready and all other software installed in the cluster. You may have to run this command in a different terminal if the <code>helm</code> command from step 4 has not yet returned to the system prompt. Once running, you can quit this kubectl watch command using ctrl-c.</p>"},{"location":"GettingStarted/installation_gpu/#6-accessing-the-kinetica-installation","title":"6. Accessing the Kinetica installation","text":""},{"location":"GettingStarted/installation_gpu/#target-platform-specifics","title":"Target Platform Specifics","text":"cloudlocal - devbare metal - prod <p>If you are installing into a managed Kubernetes environment and the NGINX ingress controller that is installed as part of this install creates a LoadBalancer service, you may need to associate the LoadBalancer with the domain you plan to use.</p> <p>As of now, the kinetica-operator chart installs NGINX ingress controller.  So after the installation is complete, you may need to edit the KineticaCluster Custom Resource  and Workbench Custom Resource with the correct domain name.</p> <p>Option 1: Use the LoadBalancer domain  Set your FQDN in Kinetica<pre><code>kubectl get svc -n kinetica-system\n# look at the loadbalancer dns name, copy it\n\nkubectl -n gpudb edit $(kubectl -n gpudb get kc -o name)\n# replace local.kinetica with the loadbalancer dns name\nkubectl -n gpudb edit $(kubectl -n gpudb get wb -o name)\n# replace local.kinetica with the loadbalancer dns name\n# save and exit\n# you should be able to access the workbench from the loadbalancer dns name\n</code></pre></p> <p>Option 2: Use your custom domain  Create a record in your DNS server pointing to the LoadBalancer DNS. Then edit the KineticaCluster Custom Resource and Workbench Custom Resource with the correct domain name, as mentioned above.</p> <p>Installing on a local machine which does not have a domain name, you can add the following entry to your <code>/etc/hosts</code> file or equivalent:</p> Configure local acces - /etc/hosts<pre><code>127.0.0.1  local.kinetica\n</code></pre> <p>Note</p> <p>The default chart configuration points to <code>local.kinetica</code> but this is configurable.</p> <p>Installing on a bare metal machines which do not have an external hardware loadbalancer requires an Ingress controller along with a software loadbalancer in order to be accessible.</p> <p>Kinetica for Kubernetes has been tested with  kube-vip</p>"},{"location":"GettingStarted/k3s/","title":"k3s Installation Specifics","text":"<p>This page covers any k3s kubernetes cluster installation specifics.</p>"},{"location":"GettingStarted/kind/","title":"KinD Installation Specifics","text":"<p>This page covers any kind kubernetes cluster installation specifics.</p>"},{"location":"GettingStarted/preparation_and_prerequisites/","title":"Preparation &amp; Prerequisites","text":"<p>Checks &amp; steps to ensure a smooth installation.</p> <p>Obtain a Kinetica License Key</p> <p>A product license key will be required for install. Please contact Kinetica Support to request a trial key.</p> <p>Failing to provide a license key at installation time will prevent the DB from starting.</p>"},{"location":"GettingStarted/preparation_and_prerequisites/#preparation-and-prerequisites","title":"Preparation and prerequisites","text":"<p>Free Resources</p> <p>Your Kubernetes cluster version should be &gt;= 1.22.x and have a minimum of 8 CPU, 8GB Ram and SSD or SATA 7200RPM hard drive(s) with 4X memory capacity.</p> GPU Support <p>For GPU enabled clusters the cards below have been tested in large-scale production environments and  provide the best performance for the database.</p> GPU Driver P4/P40/P100 525.X (or higher) V100 525.X (or higher) T4 525.X (or higher) A10/A40/A100 525.X (or higher)"},{"location":"GettingStarted/preparation_and_prerequisites/#kubernetes-cluster-connectivity","title":"Kubernetes Cluster Connectivity","text":"<p>Installation requires Helm3 and access to an on-prem or CSP managed Kubernetes cluster. and the Kubernetes CLI kubectl.</p> <p>The context for the desired target cluster must be selected from your <code>~/.kube/config</code> file and set via the <code>KUBECONFIG</code> environment variable or <code>kubectl ctx</code> (if installed). Check to see if you have the correct context with,</p> show the current kubernetes context<pre><code>kubectl config current-context\n</code></pre> <p>and that you can access this cluster correctly with,</p> list kubernetes cluster nodes<pre><code>kubectl get nodes\n</code></pre> <p></p> <p>If you do not see a list of nodes for your K8s cluster the helm installation will not work. Please check your Kubernetes installation or access credentials (kubeconfig).</p> Kinetica Images for an Air-Gapped Environment <p>If you are installing Kinetica with Helm in an air-gapped environment you will either need a Registry Proxy to pass the requests through or to download the images and push them to your internal Registry.</p> <p>For information on ways to transfer the files into an air-gapped environment See here.</p>"},{"location":"GettingStarted/preparation_and_prerequisites/#required-container-images","title":"Required Container Images","text":""},{"location":"GettingStarted/preparation_and_prerequisites/#dockerio-required-kinetica-images-for-all-installations","title":"docker.io (Required Kinetica Images for All Installations)","text":"<ul> <li>docker.io/kineticastagingcloud/kinetica-k8s-operator:v7.2.0-3.rc-3<ul> <li>docker.io/kineticastagingcloud/kinetica-k8s-cpu:v7.2.0-3.rc-3 or </li> <li>docker.io/kineticastagingcloud/kinetica-k8s-cpu-avx512:v7.2.0-3.rc-3 or </li> <li>docker.io/kineticastagingcloud/kinetica-k8s-gpu:v7.2.0-3.rc-3</li> </ul> </li> <li>docker.io/kineticastagingcloud/workbench-operator:v7.2.0-3.rc-3</li> <li>docker.io/kineticastagingcloud/workbench:v7.2.0-3.rc-3</li> <li>docker.io/kineticastagingcloud/kinetica-k8s-monitor:v7.2.0-3.rc-3</li> <li>docker.io/kineticastagingcloud/busybox:v7.2.0-3.rc-3</li> <li>docker.io/kineticastagingcloud/fluent-bit:v7.2.0-3.rc-3</li> <li>docker.io/kinetica/kagent:7.1.9.15.20230823123615.ga</li> </ul>"},{"location":"GettingStarted/preparation_and_prerequisites/#nvcrio-required-kinetica-images-for-gpu-installations-using-kinetica-k8s-gpu","title":"nvcr.io (Required Kinetica Images for GPU Installations using <code>kinetica-k8s-gpu</code>)","text":"<ul> <li>nvcr.io/nvidia/gpu-operator:v23.9.1</li> </ul>"},{"location":"GettingStarted/preparation_and_prerequisites/#registryk8sio-required-kinetica-images-for-gpu-installations-using-kinetica-k8s-gpu","title":"registry.k8s.io (Required Kinetica Images for GPU Installations using <code>kinetica-k8s-gpu</code>)","text":"<ul> <li>registry.k8s.io/nfd/node-feature-discovery:v0.14.2</li> </ul>"},{"location":"GettingStarted/preparation_and_prerequisites/#dockerio-required-supporting-images","title":"docker.io (Required Supporting Images)","text":"<ul> <li>docker.io/bitnami/openldap:2.6.7</li> <li>docker.io/alpine/openssl:latest (used by bitnami/openldap)</li> <li>docker.io/otel/opentelemetry-collector-contrib:0.95.0</li> </ul>"},{"location":"GettingStarted/preparation_and_prerequisites/#quayio-required-supporting-images","title":"quay.io (Required Supporting Images)","text":"<ul> <li>quay.io/brancz/kube-rbac-proxy:v0.14.2</li> </ul>"},{"location":"GettingStarted/preparation_and_prerequisites/#optional-container-images","title":"Optional Container Images","text":"<p>These images are only required if certain features are enabled as part of the Helm installation: -</p> <ul> <li>CertManager</li> <li>ingress-ninx</li> </ul>"},{"location":"GettingStarted/preparation_and_prerequisites/#quayio-optional-supporting-images","title":"quay.io (Optional Supporting Images)","text":"<ul> <li>quay.io/jetstack/cert-manager-cainjector:v1.13.3 (if optionally installing CertManager via Kinetica Helm Chart)</li> <li>quay.io/jetstack/cert-manager-controller:v1.13.3 (if optionally installing CertManager via Kinetica Helm Chart)</li> <li>quay.io/jetstack/cert-manager-webhook:v1.13.3 (if optionally installing CertManager via Kinetica Helm Chart)</li> </ul>"},{"location":"GettingStarted/preparation_and_prerequisites/#registryk8sio-optional-supporting-images","title":"registry.k8s.io (Optional Supporting Images)","text":"<ul> <li>registry.k8s.io/ingress-nginx/controller:v1.9.4 (if optionally installing Ingress nGinx via Kinetica Helm Chart)</li> <li>registry.k8s.io/ingress-nginx/controller:v1.9.6@sha256:1405cc613bd95b2c6edd8b2a152510ae91c7e62aea4698500d23b2145960ab9c</li> </ul>"},{"location":"GettingStarted/preparation_and_prerequisites/#which-kinetica-core-image-do-i-use","title":"Which Kinetica Core Image do I use?","text":"Container Image Intel (AMD64) Intel (AMD64 AVX512) Amd (AMD64) Graviton (aarch64) Apple Silicon (aarch64) kinetica-k8s-cpu (1) kinetica-k8s-cpu-avx512 kinetica-k8s-gpu (2) (2) (2) <ol> <li>It is preferable on an Intel AVX512 enabled CPU to use the kinetica-k8s-cpu-avx512 container image</li> <li>With a supported nVidia GPU.</li> </ol>"},{"location":"GettingStarted/preparation_and_prerequisites/#label-the-kubernetes-nodes","title":"Label the Kubernetes    Nodes","text":"<p>Kinetica requires some of the Kubernetes Nodes to be labeled as it splits some of the  components into different deployment 'pools'. This enables different physical node types to be present in the Kubernetes Cluster allowing us to target which Kinetica components go where.</p> <p>e.g. for a GPU installation some nodes in the cluster will have GPUs and others are CPU only. We can put the DB on the GPU nodes and our infrastructure components on CPU only nodes.</p>  cpu gpu <p>The Kubernetes cluster nodes selected to host the Kinetica infrastructure pods  i.e. non-DB Pods require the following label <code>app.kinetica.com/pool=infra</code>.</p> <p></p> Label the Infrastructure Nodes<pre><code>    kubectl label node k8snode1 app.kinetica.com/pool=infra\n</code></pre> <p>whilst the Kubernetes cluster nodes selected to host the Kinetica DB Pods  require the following label <code>app.kinetica.com/pool=compute</code>.</p> Label the Database Nodes<pre><code>    kubectl label node k8snode2 app.kinetica.com/pool=compute\n</code></pre> <p>The Kubernetes cluster nodes selected to host the Kinetica infrastructure pods  i.e. non-DB Pods require the following label <code>app.kinetica.com/pool=infra</code>.</p> <p></p> Label the Infrastructure Nodes<pre><code>    kubectl label node k8snode1 app.kinetica.com/pool=infra\n</code></pre> <p>whilst the Kubernetes cluster nodes selected to host the Kinetica DB Pods  require the following label <code>app.kinetica.com/pool=compute-gpu</code>.</p> Label the Database Nodes<pre><code>    kubectl label node k8snode2 app.kinetica.com/pool=compute-gpu\n</code></pre> <p>Pods Not Scheduling</p> <p>If the Kubernetes are not labeled you may have a situation where Kinetica pods not schedule and sit in a 'Pending' state.</p>"},{"location":"GettingStarted/preparation_and_prerequisites/#install-the-kinetica-operators-chart","title":"Install the kinetica-operators chart","text":"<p>This chart will install the Kinetica K8s operators together with a default configured database and workbench UI.</p>"},{"location":"GettingStarted/preparation_and_prerequisites/#1-add-the-kinetica-chart-repository","title":"1. Add the Kinetica chart repository","text":"<p>Add the repo locally as kinetica-operators:</p> Helm repo add<pre><code>helm repo add kinetica-operators https://kineticadb.github.io/charts\n</code></pre> <p></p>"},{"location":"GettingStarted/preparation_and_prerequisites/#2-obtain-the-default-helm-values-file","title":"2. Obtain the default Helm values file","text":"<p>For the generic Kubernetes install use the following values file without modification. Advanced users with specific requirements may need to adjust parameters in this file.</p> Helm values.yaml download<pre><code>wget https://raw.githubusercontent.com/kineticadb/charts/master/kinetica-operators/values.onPrem.k8s.yaml\n</code></pre>"},{"location":"GettingStarted/preparation_and_prerequisites/#3-determine-the-following-prior-to-the-chart-install","title":"3. Determine the following prior to the chart install","text":"<p>Default Admin User</p> <p>the default admin user in the Helm chart is <code>kadmin</code> but this is configurable. Non-ASCII characters and typographical symbols in the password must be escaped with a \"\\\". For example, <code>--set dbAdminUser.password=\"MyPassword\\!\"</code></p> <ol> <li>Obtain a LICENSE-KEY as described in the introduction above.</li> <li>Choose a PASSWORD for the initial administrator user</li> <li>As the storage class name varies between K8s flavor and/or there can be multiple,    this must be prescribed in the chart installation.    Obtain the DEFAULT-STORAGE-CLASS name with the command:</li> </ol> <p></p> Find the default storageclass<pre><code>kubectl get sc -o name \n</code></pre> <p></p> <p>use the name found after the /, For example, in <code>storageclass.storage.k8s.io/local-path</code> use \"local-path\" as the parameter.</p> <p>Amazon EKS</p> <p>If installing on Amazon EKS See here</p>"},{"location":"GettingStarted/preparation_and_prerequisites/#planning-access-to-your-kinetica-cluster","title":"Planning access to your Kinetica Cluster","text":"<p>Existing Ingress Controller?</p> <p>If you have an existing Ingress Controller in your Kubernetes cluster and do not want Kinetica to install an <code>ingresss-nginx</code> to expose it's endpoints then you can disable <code>ingresss-nginx</code> installation in the <code>values.yaml</code> by editing the file and setting <code>install: true</code> to <code>install: false</code>: -</p> Text Only<pre><code>```` yaml\nnodeSelector: {}\ntolerations: []\naffinity: {}\n\ningressNginx:\n    install: false\n````\n</code></pre>"},{"location":"GettingStarted/quickstart/","title":"Quickstart","text":"<p>For the quickstart we have examples for Kind or k3s.</p> <ul> <li>Kind - is suitable for CPU only installations.</li> <li>k3s - is suitable for CPU or GPU installations.</li> </ul> <p>Kubernetes &gt;= 1.25</p> <p>The current version of the chart supports kubernetes version 1.25 and above.</p>"},{"location":"GettingStarted/quickstart/#please-select-your-target-kubernetes-variant","title":"Please select your target Kubernetes variant:","text":"kind k3s <p>Default User</p> <p>Username as per the values file mentioned above is <code>kadmin</code> and password is <code>Kinetica1234!</code></p>"},{"location":"GettingStarted/quickstart/#kind-kubernetes-in-docker-kindsigsk8sio","title":"Kind (kubernetes in docker kind.sigs.k8s.io)","text":"<p>This installation in a kind cluster is for trying out the operators and the database in a non-production environment.</p> <p>CPU Only</p> <p>This method currently only supports installing a CPU version of the database.</p> <p>Please contact Kinetica Support to request a trial key.</p>"},{"location":"GettingStarted/quickstart/#create-kind-cluster-129","title":"Create Kind Cluster 1.29","text":"Create a new Kind Cluster<pre><code>wget https://raw.githubusercontent.com/kineticadb/charts/7.2.0-3.rc-3/kinetica-operators/kind.yaml\nkind create cluster --name kinetica --config kind.yaml\n</code></pre> List Kind clusters<pre><code> kind get clusters\n</code></pre> <p>Set Kubernetes Context</p> <p>Please set your Kubernetes Context to <code>kind-kinetica</code> before performing the following steps. </p>"},{"location":"GettingStarted/quickstart/#kind-install-kinetica-operators-including-a-sample-db-to-try-out","title":"Kind - Install kinetica-operators including a sample db to try out","text":"<p>Review the values file charts/kinetica-operators/values.onPrem.kind.yaml.  This is trying to install the operators and a simple db with workbench  installation for a non production try out.</p> <p>As you can see it is trying to create an ingress pointing towards local.kinetica.  If you have a domain pointing to your machine, replace it with the correct domain name.</p>"},{"location":"GettingStarted/quickstart/#kind-install-the-kinetica-operators-chart","title":"Kind - Install the  Kinetica-Operators Chart","text":"Get &amp; install the Kinetica-Operators Chart<pre><code>wget https://raw.githubusercontent.com/kineticadb/charts/7.2.0-3.rc-3/kinetica-operators/values.onPrem.kind.yaml\n\nhelm -n kinetica-system upgrade -i kinetica-operators kinetica-operators/kinetica-operators --create-namespace --values values.onPrem.kind.yaml --set db.gpudbCluster.license=\"your_license_key\" --set dbAdminUser.password=\"your_password\"\n</code></pre> <p>or if you have been asked by the Kinetica Support team to try a development version</p> Using a development version<pre><code>helm search repo kinetica-operators --devel --versions\n\nhelm -n kinetica-system upgrade -i kinetica-operators kinetica-operators/kinetica-operators/ --create-namespace --values values.onPrem.kind.yaml --set db.gpudbCluster.license=\"your_license_key\" --set dbAdminUser.password=\"your_password\" --devel --version 7.2.0-2.rc-2\n</code></pre> <p>Accessing the Workbench</p> <p>You should be able to access the workbench at http://local.kinetica</p>"},{"location":"GettingStarted/quickstart/#k3s-k3sio","title":"k3s (k3s.io)","text":""},{"location":"GettingStarted/quickstart/#install-k3s-129","title":"Install k3s 1.29","text":"Install k3s<pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable=traefik  --node-name kinetica-master --token 12345\" K3S_KUBECONFIG_OUTPUT=~/.kube/config_k3s K3S_KUBECONFIG_MODE=644 INSTALL_K3S_VERSION=v1.29.2+k3s1 sh -\n</code></pre>"},{"location":"GettingStarted/quickstart/#k3s-install-kinetica-operators-including-a-sample-db-to-try-out","title":"K3s - Install kinetica-operators including a sample db to try out","text":"<p>Review the values file <code>charts/kinetica-operators/values.onPrem.k3s.yaml</code>.  This is trying to install the operators and a simple db with workbench installation  for a non production try out.</p> <p>As you can see it is trying to create an ingress pointing towards <code>local.kinetica</code>.  If you have a domain pointing to your machine, replace it with the correct domain name.</p> <p>If you are on a local machine which is not having a domain name,  you add the following entry to your <code>/etc/hosts</code> file or equivalent.</p> Configure local acces - /etc/hosts<pre><code>127.0.0.1  local.kinetica\n</code></pre>"},{"location":"GettingStarted/quickstart/#k3s-install-the-kinetica-operators-chart-cpu","title":"K3S - Install the  Kinetica-Operators Chart (CPU)","text":"Bash<pre><code>wget https://raw.githubusercontent.com/kineticadb/charts/master/kinetica-operators/values.onPrem.k3s.yaml\n\nhelm -n kinetica-system install kinetica-operators kinetica-operators/kinetica-operators --create-namespace --values values.onPrem.k3s.yaml --set db.gpudbCluster.license=\"your_license_key\" --set dbAdminUser.password=\"your_password\"\n</code></pre> <p>or if you have been asked by the Kinetica Support team to try a development version</p> Using a development version<pre><code>helm search repo kinetica-operators --devel --versions\n\nhelm -n kinetica-system install kinetica-operators kinetica-operators/kinetica-operators --create-namespace --values values.onPrem.k3s.yaml --set db.gpudbCluster.license=\"your_license_key\" --set dbAdminUser.password=\"your_password\" --devel --version 7.2.0-2.rc-2\n</code></pre>"},{"location":"GettingStarted/quickstart/#k3s-install-the-kinetica-operators-chart-gpu","title":"K3S - Install the  Kinetica-Operators Chart (GPU)","text":"<p>If you wish to try out the GPU capabilities, you can use the following values file,  provided you are in a nvidia gpu capable machine.</p> k3s GPU Installation<pre><code>wget https://raw.githubusercontent.com/kineticadb/charts/master/kinetica-operators/values.onPrem.k3s.gpu.yaml\n\nhelm -n kinetica-system install kinetica-operators charts/kinetica-operators/ --create-namespace --values values.onPrem.k3s.gpu.yaml --set db.gpudbCluster.license=\"your_license_key\" --set dbAdminUser.password=\"your_password\"\n</code></pre> <p>Accessing the Workbench</p> <p>You should be able to access the workbench at http://local.kinetica</p>"},{"location":"GettingStarted/quickstart/#uninstall-k3s","title":"Uninstall k3s","text":"uninstall k3s<pre><code>/usr/local/bin/k3s-uninstall.sh\n</code></pre>"},{"location":"Help/changing_the_fqdn/","title":"How to change the Clusters FQDN","text":""},{"location":"Help/changing_the_fqdn/#coming-soon","title":"Coming Soon","text":""},{"location":"Help/help_and_tutorials/","title":"Help &amp; Tutorials","text":"<ul> <li> <p> Tutorials</p> <p> Tutorials</p> </li> <li> <p> Help</p> <p> Help</p> </li> </ul>"},{"location":"Help/help_and_tutorials/#coming-soon","title":"Coming Soon","text":""},{"location":"Help/help_index/","title":"Creating Users, Roles, Schemas and other Kinetica DB Objects","text":""},{"location":"KubernetesResources/","title":"Managing the Kinetica System using Kubernetes Resources","text":"<ul> <li> <p> DB Clusters</p> <p>Core Kinetica Database Cluster Management.</p> <p> KineticaCluster</p> </li> <li> <p> DB Users</p> <p>Kinetica Database User Management.</p> <p> KineticaUser</p> </li> <li> <p> DB Roles</p> <p>Kinetica Database Role Management.</p> <p> KineticaRole</p> </li> <li> <p> DB Schemas</p> <p>Kinetica Database Schema Management.</p> <p> KineticaSchema</p> </li> <li> <p> DB Grants</p> <p>Kinetica Database Grant Management.</p> <p> KineticaGrant</p> </li> <li> <p> DB Resource Groups</p> <p>Kinetica Database Resource Group Management.</p> <p> KineticaResourceGroup</p> </li> <li> <p> DB Administration</p> <p>Kinetica Database Administration.</p> <p> KineticaAdmin</p> </li> <li> <p> DB Backups</p> <p>Kinetica Database Backup Management.</p> <p>Note</p> <p>This requires Velero to be installed on the Kubernetes Cluster.</p> <p> KineticaBackup</p> </li> <li> <p> DB Restore</p> <p>Kinetica Database Restoration.</p> <p>Note</p> <p>This requires Velero to be installed on the Kubernetes Cluster.</p> <p> KineticaRestore</p> </li> </ul> <p>  Home</p>"},{"location":"KubernetesResources/cluster_backup/","title":"CKinetica Cluster Backup","text":""},{"location":"KubernetesResources/cluster_backup/#coming-soon","title":"Coming Soon","text":""},{"location":"KubernetesResources/cluster_restore/","title":"CKinetica Cluster Restore","text":""},{"location":"KubernetesResources/cluster_restore/#coming-soon","title":"Coming Soon","text":""},{"location":"KubernetesResources/role_management/","title":"Role Management","text":"<p>Management of roles is done with the <code>KineticaRole</code> CRD. </p> <p>kubectl Usage</p> <p>From the <code>kubectl</code> command line they are referenced by <code>kineticaroles</code> or the short form is <code>kr</code>.</p>"},{"location":"KubernetesResources/role_management/#list-roles","title":"List Roles","text":"<p>To list the roles deployed to a Kinetica DB installation we can use the following from the command-line: -</p> <p><code>kubectl -n gpudb get kineticaroles</code> or <code>kubectl -n gpudb get kr</code></p> <p>where the namespace <code>-n gpudb</code> matches the namespace of the Kinetica DB installation.</p> <p>This outputs</p> Name Ring Name Role Resource Group Name LDAP DB db-users kinetica-k8s-sample db_users OK OK global-admins kinetica-k8s-sample global_admins OK OK"},{"location":"KubernetesResources/role_management/#name","title":"Name","text":"<p>The name of the Kubernetes CR i.e. the <code>metadata.name</code> this is not necessarily the name of the user.</p>"},{"location":"KubernetesResources/role_management/#ring-name","title":"Ring Name","text":"<p>The name of the <code>KineticaCluster</code> the user is created in.</p>"},{"location":"KubernetesResources/role_management/#role-name","title":"Role Name","text":"<p>The name of the role as contained with LDAP &amp; the DB.</p>"},{"location":"KubernetesResources/role_management/#role-creation","title":"Role Creation","text":"test-role-2.yaml<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaRole\nmetadata:\n    name: test-role-2\n    namespace: gpudb\nspec:\n    ringName: kineticacluster-sample\n    role:\n        name: \"test_role2\"\n</code></pre>"},{"location":"KubernetesResources/role_management/#role-deletion","title":"Role Deletion","text":"<p>To delete a role from the Kinetica Cluster simply delete the Role CR from Kubernetes: -</p> Delete User<pre><code>kubectl -n gpudb delete kr user-fred-smith \n</code></pre>"},{"location":"KubernetesResources/user_management/","title":"User Management","text":"<p>Management of users is done with the <code>KineticaUser</code> CRD. </p> <p>kubectl Usage</p> <p>From the <code>kubectl</code> command line they are referenced by <code>kineticausers</code> or the short form is <code>ku</code>.</p>"},{"location":"KubernetesResources/user_management/#list-users","title":"List Users","text":"<p>To list the users deployed to a Kinetica DB installation we can use the following from the  command-line: -</p> <p><code>kubectl -n gpudb get kineticausers</code> or <code>kubectl -n gpudb get ku</code></p> <p>where the namespace <code>-n gpudb</code> matches the namespace of the Kinetica DB installation.</p> <p>This outputs </p> Name Action Ring Name UID Last Name Given Name Display Name LDAP DB Reveal kadmin upsert kinetica-k8s-sample kadmin Account Admin Admin Account OK OK OK"},{"location":"KubernetesResources/user_management/#name","title":"Name","text":"<p>The name of the Kubernetes CR i.e. the <code>metadata.name</code> this is not necessarily the name of the user.</p>"},{"location":"KubernetesResources/user_management/#action","title":"Action","text":"<p>There are two actions possible on a <code>KineticaUser</code>. The first is <code>upsert</code> which is for user creation or modification. The second is <code>change-password</code> which shows when a user password reset has been performed.</p>"},{"location":"KubernetesResources/user_management/#ring-name","title":"Ring Name","text":"<p>The name of the <code>KineticaCluster</code> the user is created in.</p>"},{"location":"KubernetesResources/user_management/#uid","title":"UID","text":"<p>The unique, user id to use in LDAP &amp; the DB to reference this user.</p>"},{"location":"KubernetesResources/user_management/#last-name","title":"Last Name","text":"<p>Last Name refers to last name or surname. </p> <p><code>sn</code> in LDAP terms.</p>"},{"location":"KubernetesResources/user_management/#given-name","title":"Given Name","text":"<p>Given Name is the Firstname also called Christian name. </p> <p><code>givenName</code> in LDAP terms.</p>"},{"location":"KubernetesResources/user_management/#display-name","title":"Display Name","text":"<p>The name shown on any UI representation.</p>"},{"location":"KubernetesResources/user_management/#ldap","title":"LDAP","text":"<p>Identifies if the user has been successfully created within LDAP. </p> <ul> <li>'' - if empty the user has not yet been created in LDAP</li> <li>'OK' - shows the user has been successfully created within LDAP</li> <li>'Failed' - shows there was a failure adding the user to LDAP</li> </ul>"},{"location":"KubernetesResources/user_management/#db","title":"DB","text":"<p>Identifies if the user has been successfully created within the DB.</p> <ul> <li>'' - if empty the user has not yet been created in the DB</li> <li>'OK' - shows the user has been successfully created within the DB</li> <li>'Failed' - shows there was a failure adding the user to the DB</li> </ul>"},{"location":"KubernetesResources/user_management/#reveal","title":"Reveal","text":"<p>Identifies if the user has been successfully created within Reveal.</p> <ul> <li>'' - if empty the user has not yet been created in Reveal</li> <li>'OK' - shows the user has been successfully created within Reveal</li> <li>'Failed' - shows there was a failure adding the user to Reveal</li> </ul>"},{"location":"KubernetesResources/user_management/#user-creation","title":"User Creation","text":"<p>User creation requires two Kubernetes CRs to be submitted to Kubernetes and processed by the Kinetica DB Operator.</p> <ul> <li>User Secret (Password)</li> <li>Kinetica User</li> </ul> <p>Creation Sequence</p> <p>It is preferable to create the User Secret prior to creating the <code>KineticaUser</code>.</p> <p>Secret Deletion</p> <p>The User Secret will be deleted once the <code>KineticaUser</code> is created by the operator. The users password will be stored in LDAP and not be present in Kubernetes.</p>"},{"location":"KubernetesResources/user_management/#user-secret","title":"User Secret","text":"<p>In this example a user Fred Smith will be created.</p> fred-smith-secret.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: fred-smith-secret\n  namespace: gpudb\nstringData:\n  password: testpassword\n</code></pre> Create the User Password Secret<pre><code>kubectl apply -f fred-smith-secret.yaml\n</code></pre>"},{"location":"KubernetesResources/user_management/#kineticauser","title":"<code>KineticaUser</code>","text":"user-fred-smith.yaml<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaUser\nmetadata:\n  name: user-fred-smith\n  namespace: gpudb\nspec:\n  ringName: kineticacluster-sample\n  uid: fred\n  action: upsert\n  reveal: true\n  upsert:\n    userPrincipalName: fred.smith@example.com\n    givenName: Fred\n    displayName: FredSmith\n    lastName: Smith\n    passwordSecret: fred-smith-secret\n</code></pre>"},{"location":"KubernetesResources/user_management/#user-deletion","title":"User Deletion","text":"<p>To delete a user from the Kinetica Cluster simply delete the User CR from Kubernetes: -</p> Delete User<pre><code>kubectl -n gpudb delete ku user-fred-smith \n</code></pre>"},{"location":"KubernetesResources/user_management/#change-password","title":"Change Password","text":"<p>To change a users password we use the <code>change-password</code> action rather than the <code>upsert</code> action we used previously.</p> <p>Creation Sequence</p> <p>It is preferable to create the User Secret prior to creating the <code>KineticaUser</code>.</p> <p>Secret Deletion</p> <p>The User Secret will be deleted once the <code>KineticaUser</code> is created by the operator. The users password will be stored in LDAP and not be present in Kubernetes.</p> fred-smith-change-pwd-secret.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: fred-smith-change-pwd-secret\n  namespace: gpudb\nstringData:\n  password: testpassword\n</code></pre> Create the User Password Secret<pre><code>kubectl apply -f fred-smith-change-pwd-secret.yaml\n</code></pre> user-fred-smith-change-password.yaml<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaUser\nmetadata:\n  name: user-fred-smith-change-password\n  namespace: gpudb\nspec:\n  ringName: kineticacluster-sample\n  uid: fred\n  action: change-password\n  changePassword:\n    passwordSecret: fred-smith-change-pwd-secret\n</code></pre>"},{"location":"KubernetesResources/user_management/#advanced-topics","title":"Advanced Topics","text":""},{"location":"KubernetesResources/user_management/#limit-user-resources","title":"Limit User Resources","text":""},{"location":"KubernetesResources/user_management/#data-limit","title":"Data Limit","text":"<p>KIFs user data size limit.</p> dataLimit<pre><code>spec:\n  upsert:\n    dataLimit: 10Gi\n</code></pre>"},{"location":"KubernetesResources/user_management/#user-kifs-usage","title":"User Kifs Usage","text":"<p>Kifs Enablement</p> <p>In order to use the Kifs user features below there is a requirement that Kifs  is enabled on the Kinetica DB.</p>"},{"location":"KubernetesResources/user_management/#home-directory","title":"Home Directory","text":"<p>When creating a new user it is possible to create that user a 'home' directory within  the Kifs filesystem by using the <code>createHomeDirectory</code> option.</p> createHomeDirectory<pre><code>spec:\n  upsert:\n    createHomeDirectory: true\n</code></pre>"},{"location":"KubernetesResources/user_management/#limit-directory-storage","title":"Limit Directory Storage","text":"<p>It is possible to limit the amount of Kifs file storage the user has by adding <code>kifsDataLimit</code> to the user creation yaml and setting the value to a Kubernetes Quantity e.g. <code>2Gi</code></p> kifsDataLimit<pre><code>spec:\n  upsert:\n    kifsDataLimit: 2Gi\n</code></pre>"},{"location":"Monitoring/logs/","title":"Log Collection &amp; Display","text":""},{"location":"Monitoring/logs/#coming-soon","title":"Coming Soon","text":""},{"location":"Monitoring/metrics_and_monitoring/","title":"MetricsCollection &amp; Display","text":""},{"location":"Monitoring/metrics_and_monitoring/#coming-soon","title":"Coming Soon","text":""},{"location":"Operations/","title":"Operations","text":"<ul> <li> <p> Metrics</p> <p>Collecting and storing metrics as time series data.</p> <p> Metrics</p> </li> <li> <p> Logs</p> <p>Log aggregation.</p> <p> Logs</p> </li> <li> <p> Metric &amp; Log Distribution</p> <p>Metrics &amp; Logs can be distributed to other systems using OpenTelemetry.</p> <p> OpenTelemety</p> </li> <li> <p> Backup &amp; Restore</p> <p>Backup &amp; Restore of the Kinetica DB.</p> <p> Backup &amp; Restore</p> </li> </ul>"},{"location":"Operations/backup_and_restore/","title":"Kinetica DB Backup &amp; Restore","text":"<p>Velero Installation</p> <p>Backup &amp; Restore requires that v is installed into the Kubernetes Cluster.</p>"},{"location":"Operations/backup_and_restore/#coming-soon","title":"Coming Soon","text":""},{"location":"Operations/otel/","title":"OTEL Integration for Metric &amp; Log Distribution","text":""},{"location":"Operations/otel/#coming-soon","title":"Coming Soon","text":""},{"location":"Operators/k3s/","title":"Overview","text":"<p>Kinetica Operators can be installed in any on-prem kubernetes cluster. This document provides instructions to install the operators in k3s. If you are on another distribution, you should be able to change the values file to suit your environment.</p> <p>You will need a license key for this to work. Please contact Kinetica Support.</p>"},{"location":"Operators/k3s/#kinetica-on-k3s-k3sio","title":"Kinetica on k3s (k3s.io)","text":"<p>Current version of the chart supports kubernetes version 1.25 and above.</p>"},{"location":"Operators/k3s/#install-k3s-129","title":"Install k3s 1.29","text":"Bash<pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable=traefik  --node-name kinetica-master --token 12345\" K3S_KUBECONFIG_OUTPUT=~/.kube/config_k3s K3S_KUBECONFIG_MODE=644 INSTALL_K3S_VERSION=v1.29.2+k3s1 sh -\n</code></pre>"},{"location":"Operators/k3s/#k3s-install-kinetica-operators-including-a-sample-db-to-try-out","title":"K3s -Install kinetica-operators including a sample db to try out","text":"<p>Review the values file charts/kinetica-operators/values.onPrem.k3s.yaml. This is trying to install the operators and a simple db with workbench installation for a non production try out.</p> <p>As you can see it is trying to create an ingress pointing towards local.kinetica. If you have a domain pointing to your machine, replace it with the correct domain name.</p> <p>If you are on a local machine which is not having a domain name, you add the following entry to your /etc/hosts file or equivalent.</p> Text Only<pre><code>127.0.0.1 local.kinetica\n</code></pre>"},{"location":"Operators/k3s/#k3s-install-the-kinetica-operators-chart","title":"K3s - Install the kinetica-operators chart","text":"Bash<pre><code>wget https://raw.githubusercontent.com/kineticadb/charts/master/kinetica-operators/values.onPrem.k3s.yaml\n\nhelm -n kinetica-system install kinetica-operators kinetica-operators/kinetica-operators --create-namespace --values values.onPrem.k3s.yaml --set db.gpudbCluster.license=\"your_license_key\" --set dbAdminUser.password=\"your_password\"\n\n# if you want to try out a development version,\nhelm search repo kinetica-operators --devel --versions\n\nhelm -n kinetica-system install kinetica-operators kinetica-operators/kinetica-operators --create-namespace --values values.onPrem.k3s.yaml --set db.gpudbCluster.license=\"your_license_key\" --set dbAdminUser.password=\"your_password\" --devel --version 7.2.0-2.rc-2\n</code></pre>"},{"location":"Operators/k3s/#k3s-install-the-kinetica-operators-chart-gpu-capable-machine","title":"K3s - Install the kinetica-operators chart (GPU Capable Machine)","text":"<p>If you wish to try out the GPU capabilities, you can use the following values file, provided you are in a nvidia gpu capable machine.</p> Bash<pre><code>wget https://raw.githubusercontent.com/kineticadb/charts/master/kinetica-operators/values.onPrem.k3s.gpu.yaml\n\nhelm -n kinetica-system install kinetica-operators charts/kinetica-operators/ --create-namespace --values values.onPrem.k3s.gpu.yaml --set db.gpudbCluster.license=\"your_license_key\" --set dbAdminUser.password=\"your_password\"\n</code></pre> <p>You should be able to access the workbench at http://local.kinetica</p> <p>Username as per the values file mentioned above is kadmin and password is Kinetica1234!</p>"},{"location":"Operators/k3s/#uninstall-k3s","title":"Uninstall k3s","text":"Bash<pre><code>/usr/local/bin/k3s-uninstall.sh\n</code></pre>"},{"location":"Operators/k8s/","title":"Overview","text":"<p>For managed Kubernetes solutions (AKS, EKS, GKE) or other on-prem K8s flavors,  follow this generic guide to install the Kinetica Operators, Database and Workbench.  A product license key will be required for install. Please contact Kinetica Support  to request a trial key.</p>"},{"location":"Operators/k8s/#preparation-and-prerequisites","title":"Preparation and prerequisites","text":"<p>Installation requires Helm3 and access to an on-prem or CSP managed Kubernetes cluster.  kubectl is optional but highly recommended. The context for the desired target cluster must be selected from your <code>~/.kube/config</code> file  or set via the <code>KUBECONFIG</code> environment variable. Check to see if you have the correct context with,</p> Bash<pre><code>kubectl config current-context\n</code></pre> <p>and that you can access this cluster correctly with,</p> Bash<pre><code>kubectl get nodes\n</code></pre> <p>If you do not see a list of nodes for your K8s cluster the helm installation will not work. Please check your Kubernetes installation or access credentials (kubeconfig).</p>"},{"location":"Operators/k8s/#install-the-kinetica-operators-chart","title":"Install the kinetica-operators chart","text":"<p>This chart will install the Kinetica K8s operators together with a default configured database and workbench UI.</p> <p>If you are installing into a managed Kubernetes environment and the NGINX ingress controller that is installed as part of this install creates a LoadBalancer service, you may need to associate the LoadBalancer with the domain you plan to use.</p> <p>Alternatively, if you are installing on a local machine which does not have a domain name, you can add the following entry to your <code>/etc/hosts</code> file or equivalent:</p> Bash<pre><code>127.0.0.1  local.kinetica\n</code></pre> <p>Note that the default chart configuration points to <code>local.kinetica</code> but this is configurable.</p>"},{"location":"Operators/k8s/#1-add-the-kinetica-chart-repository","title":"1. Add the Kinetica chart repository","text":"<p>Add the repo locally as kinetica-operators:</p> Bash<pre><code>helm repo add kinetica-operators https://kineticadb.github.io/charts\n</code></pre>"},{"location":"Operators/k8s/#2-obtain-the-default-helm-values-file","title":"2. Obtain the default Helm values file","text":"<p>For the generic Kubernetes install use the following values file without modification. Advanced users with specific requirements may need to adjust parameters in this file.</p> Bash<pre><code>wget https://raw.githubusercontent.com/kineticadb/charts/master/kinetica-operators/values.onPrem.k8s.yaml\n</code></pre>"},{"location":"Operators/k8s/#3-determine-the-following-prior-to-the-chart-install","title":"3. Determine the following prior to the chart install","text":"<p>(a) Obtain a LICENSE-KEY as described in the introduction above. (b) Choose a PASSWORD for the initial administrator user (Note: the default in the chart for this user is <code>kadmin</code> but this is configurable). Non-ASCII characters and typographical symbols in the password must be escaped with a \"\\\". For example, <code>--set dbAdminUser.password=\"MyPassword\\!\"</code> \u00a9 As storage class name varies between K8s flavor and/or there can be multiple, this must be prescribed in the chart installation. Obtain DEFAULT-STORAGE-CLASS name with the command:</p> Bash<pre><code>kubectl get sc -o name \n</code></pre> <p>use the name found after the /, For example, in <code>\"storageclass.storage.k8s.io/TheName\"</code> use \"TheName\" as the parameter.</p>"},{"location":"Operators/k8s/#4-install-the-helm-chart","title":"4. Install the helm chart","text":"<p>Run the following Helm install command after substituting values from section 3 above:</p> Bash<pre><code>helm -n kinetica-system install \\\nkinetica-operators kinetica-operators/kinetica-operators \\\n--create-namespace \\\n--values values.onPrem.k8s.yaml \\\n--set db.gpudbCluster.license=\"LICENSE-KEY\" \\\n--set dbAdminUser.password=\"PASSWORD\" \\\n--set global.defaultStorageClass=\"DEFAULT-STORAGE-CLASS\"\n</code></pre>"},{"location":"Operators/k8s/#5-check-installation-progress","title":"5. Check installation progress","text":"<p>After a few moments, follow the progression of the main database pod startup with:</p> Bash<pre><code>kubectl -n gpudb get po gpudb-0 -w\n</code></pre> <p>until it reaches <code>\"gpudb-0  3/3  Running\"</code> at which point the database should be ready and all other software installed in the cluster. You may have to run this command in a different terminal if the <code>helm</code> command from step 4 has not yet returned to the system prompt. Once running, you can quit this kubectl watch command using ctrl-c.</p>"},{"location":"Operators/k8s/#6-accessing-the-kinetica-installation","title":"6. Accessing the Kinetica installation","text":""},{"location":"Operators/k8s/#optional-install-a-development-chart-version","title":"(Optional) Install a development chart version","text":"<p>Find all alternative chart versions with:</p> Bash<pre><code>helm search repo kinetica-operators --devel --versions\n</code></pre> <p>Then append <code>--devel --version [CHART-DEVEL-VERSION]</code> to the end of the Helm install command in section 4 above.</p>"},{"location":"Operators/k8s/#k8s-flavour-specific-notes","title":"K8s Flavour specific notes","text":""},{"location":"Operators/k8s/#eks","title":"EKS","text":""},{"location":"Operators/k8s/#ebs-csi-driver","title":"EBS CSI driver","text":"<p>Make sure you have enabled the ebs-csi driver in your EKS cluster. This is required for the default storage class to work. Please refer to this AWS documentation for more information.</p>"},{"location":"Operators/k8s/#ingress","title":"Ingress","text":"<p>As of now, the kinetica-operator chart installs NGINX ingress controller. So after the installation is complete, you may need to edit the KineticaCluster Custom Resource and Workbench Custom Resource with the correct domain name.</p>"},{"location":"Operators/k8s/#option-1-use-the-loadbalancer-domain","title":"Option 1: Use the LoadBalancer domain","text":"Bash<pre><code>kubectl get svc -n kinetica-system\n# look at the loadbalancer dns name, copy it\n\nkubectl -n gpudb edit $(kubectl -n gpudb get kc -o name)\n# replace local.kinetica with the loadbalancer dns name\nkubectl -n gpudb edit $(kubectl -n gpudb get wb -o name)\n# replace local.kinetica with the loadbalancer dns name\n# save and exit\n# you should be able to access the workbench from the loadbalancer dns name\n</code></pre>"},{"location":"Operators/k8s/#option-1-use-your-custom-domain","title":"Option 1: Use your custom domain","text":"<p>Create a record in your DNS server pointing to the LoadBalancer DNS. Then edit the KineticaCluster Custom Resource and Workbench Custom Resource with the correct domain name, as mentioned above.</p>"},{"location":"Operators/kind/","title":"Overview","text":"<p>This installation in a kind cluster is for trying out the operators and the database in a non production environment. This method currently only supports installing a CPU version of the database.</p> <p>You will need a license key for this to work. Please contact Kinetica Support.</p>"},{"location":"Operators/kind/#kind-kubernetes-in-docker-kindsigsk8sio","title":"Kind (kubernetes in docker kind.sigs.k8s.io)","text":""},{"location":"Operators/kind/#create-kind-cluster-129","title":"Create Kind Cluster 1.29","text":"Bash<pre><code>kind create cluster --config charts/kinetica-operators/kind.yaml\n</code></pre>"},{"location":"Operators/kind/#kind-install-kinetica-operators-including-a-sample-db-to-try-out","title":"Kind - Install kinetica-operators including a sample db to try out","text":"<p>Review the values file charts/kinetica-operators/values.onPrem.kind.yaml. This is trying to install the operators and a simple db with workbench installation for a non production try out.</p> <p>As you can see it is trying to create an ingress pointing towards local.kinetica. If you have a domain pointing to your machine, replace it with the correct domain name.</p>"},{"location":"Operators/kind/#kind-install-the-kinetica-operators-chart","title":"Kind - Install the kinetica-operators chart","text":"Bash<pre><code>wget https://raw.githubusercontent.com/kineticadb/charts/master/kinetica-operators/values.onPrem.kind.yaml\n\nhelm -n kinetica-system install kinetica-operators kinetica-operators/kinetica-operators --create-namespace --values values.onPrem.kind.yaml --set db.gpudbCluster.license=\"your_license_key\" --set dbAdminUser.password=\"your_password\"\n\n# if you want to try out a development version,\nhelm search repo kinetica-operators --devel --versions\nhelm -n kinetica-system install kinetica-operators kinetica-operators/kinetica-operators/ --create-namespace --values values.onPrem.kind.yaml --set db.gpudbCluster.license=\"your_license_key\" --set dbAdminUser.password=\"your_password\" --devel --version 7.2.0-2.rc-2\n</code></pre> <p>You should be able to access the workbench at http://local.kinetica</p> <p>Username as per the values file mentioned above is kadmin and password is Kinetica1234!</p>"},{"location":"Operators/kinetica-operators/","title":"Kinetica DB Operator Helm Charts","text":"<p>To install all the required operators in a single command perform the following: -</p> Bash<pre><code>helm install -n kinetica-system \\\nkinetica-operators kinetica-operators/kinetica-operators --create-namespace\n</code></pre> <p>This will install all the Kubernetes Operators required into the <code>kinetica-system</code> namespace and create the namespace if it is not currently present.</p> <p>Note</p> <p>Depending on what target platform you are installing to it may be necessary to supply an additional parameter  pointing to a values file to successfully provision the DB.</p> Bash<pre><code>helm install -n kinetica-system -f values.yaml --set provider=aks \\\nkinetica-operators kinetica-operators/kinetica-operators --create-namespace\n</code></pre> <p>The command above uses a custom <code>values.yaml</code> for helm and sets the install platform to Microsoft Azure AKS.</p> <p>Currently supported <code>providers</code> are: -</p> <ul> <li><code>aks</code> - Microsoft Azure AKS</li> <li><code>eks</code> - Amazon AWS EKS</li> <li><code>local</code> - Generic 'On-Prem' Kubernetes Clusters e.g. one deployed using <code>kubeadm</code></li> </ul> <p>Example Helm <code>values.yaml</code> for different Cloud Providers/On-Prem installations: -</p> Azure AKSAmazon EKSOn-Prem values.yaml<pre><code>namespace: kinetica-system\n\ndb:\n  serviceAccount: {}\n  image:\n    # Kinetica DB Operator installer image\n    repository: \"registry.harbor.kinetica.com/kinetica/kinetica-k8s-operator\"\n    #  Kinetica DB Operator installer image tag\n    tag: \"\"\n\n  parameters:\n    # &lt;base64 encode of kubeconfig&gt; of the Kubernetes Cluster to deploy to\n    kubeconfig: \"\"\n    # The storage class to use for PVCs\n    storageClass: \"managed-premium\"\n\n  storageClass:\n    persist:\n      # Workbench Operator Persistent Volume Storage Class\n      provisioner: \"disk.csi.azure.com\"\n    procs:\n      # Workbench Operator Procs Volume Storage Class\n      provisioner: \"disk.csi.azure.com\"\n    cache:\n      # Workbench Operator Cache Volume Storage Class\n      provisioner: \"disk.csi.azure.com\"\n</code></pre> <p>15 <code>storageClass: \"managed-premium\"</code> - sets the appropriate <code>storageClass</code> for Microsoft Azure AKS Persistent Volume (PV)</p> <p>20 <code>provisioner: \"disk.csi.azure.com\"</code> - sets the appropriate disk provisioner for the DB (Persist) filesystem for Microsoft Azure</p> <p>23 <code>provisioner: \"disk.csi.azure.com\"</code> - sets the appropriate disk provisioner for the DB Procs filesystem for Microsoft Azure</p> <p>26 <code>provisioner: \"disk.csi.azure.com\"</code> - sets the appropriate disk provisioner for the DB Cache filesystem for Microsoft Azure</p> values.yaml<pre><code>namespace: kinetica-system\n\ndb:\n  serviceAccount: {}\n  image:\n    # Kinetica DB Operator installer image\n    repository: \"registry.harbor.kinetica.com/kinetica/kinetica-k8s-operator\"\n    #  Kinetica DB Operator installer image tag\n    tag: \"\"\n\n  parameters:\n    # &lt;base64 encode of kubeconfig&gt; of the Kubernetes Cluster to deploy to\n    kubeconfig: \"\"\n    # The storage class to use for PVCs\n    storageClass: \"gp2\"\n\n  storageClass:\n    persist:\n      # Workbench Operator Persistent Volume Storage Class\n      provisioner: \"kubernetes.io/aws-ebs\"\n    procs:\n      # Workbench Operator Procs Volume Storage Class\n      provisioner: \"kubernetes.io/aws-ebs\"\n    cache:\n      # Workbench Operator Cache Volume Storage Class\n      provisioner: \"kubernetes.io/aws-ebs\"\n</code></pre> <p>15 <code>storageClass: \"gp2\"</code> - sets the appropriate <code>storageClass</code> for Amazon EKS Persistent Volume (PV)</p> <p>20 <code>provisioner: \"kubernetes.io/aws-ebs\"</code> - sets the appropriate disk provisioner for the DB (Persist) filesystem for Microsoft Azure</p> <p>23 <code>provisioner: \"kubernetes.io/aws-ebs\"</code> - sets the appropriate disk provisioner for the DB Procs filesystem for Microsoft Azure</p> <p>26 <code>provisioner: \"kubernetes.io/aws-ebs\"</code> - sets the appropriate disk provisioner for the DB Cache filesystem for Microsoft Azure</p> values.yaml<pre><code>namespace: kinetica-system\n\ndb:\n  serviceAccount: {}\n  image:\n    # Kinetica DB Operator installer image\n    repository: \"registry.harbor.kinetica.com/kinetica/kinetica-k8s-operator\"\n    #  Kinetica DB Operator installer image tag\n    tag: \"\"\n\n  parameters:\n    # &lt;base64 encode of kubeconfig&gt; of the Kubernetes Cluster to deploy to\n    kubeconfig: \"\"\n    # the type of installation e.g. aks, eks, local\n    environment: \"local\"\n    # The storage class to use for PVCs\n    storageClass: \"standard\"\n\n  storageClass:\n    procs: {}\n    persist: {}\n    cache: {}\n</code></pre> <p>15 <code>environment: \"local\"</code> - tells the DB Operator to deploy the DB as a 'local' instance to the Kubernetes Cluster</p> <p>17 <code>storageClass: \"standard\"</code> - sets the appropriate <code>storageClass</code> for the On-Prem Persistent Volume Provisioner</p> <p>storageClass</p> <p>The <code>storageClass</code> should be present in the target environment. </p> <p>A list of available <code>storageClass</code> can be obtained using: -</p> Bash<pre><code>kubectl get sc\n</code></pre>"},{"location":"Operators/kinetica-operators/#components","title":"Components","text":"<p>The <code>kinetica-db</code> Helm Chart wraps the deployment of a number of sub-components: -</p> <ul> <li>Porter Operator</li> <li>Kinetica Database Operator</li> <li>Kinetica Workbench Operator</li> </ul> <p>Installation/Upgrading/Deletion of the Kinetica Operators is done via two CRs which leverage porter.sh as the orchestrator. The corresponding Porter Operator, DB Operator &amp; Workbench Operator CRs are submitted by running the appropriate helm command i.e.</p> <ul> <li>install</li> <li>upgrade</li> <li>uninstall</li> </ul>"},{"location":"Operators/kinetica-operators/#porter-operator","title":"Porter Operator","text":""},{"location":"Operators/kinetica-operators/#database-operator","title":"Database Operator","text":"<p>The Kinetica DB Operator installation CR for the porter.sh operator is: -</p> YAML<pre><code>apiVersion: porter.sh/v1\nkind: Installation\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: kinetica-operators\n    meta.helm.sh/release-namespace: kinetica-system\n  labels:\n    app.kubernetes.io/instance: kinetica-operators\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kinetica-operators\n    app.kubernetes.io/version: 0.1.0\n    helm.sh/chart: kinetica-operators-0.1.0\n    installVersion: 0.38.10\n  name: kinetica-operators-operator-install\n  namespace: kinetica-system\nspec:\n  action: install\n  agentConfig:\n    volumeSize: '0'\n  parameters:\n    environment: local\n    storageclass: managed-premium\n  reference: docker.io/kinetica/kinetica-k8s-operator:v7.1.9-7.rc3\n</code></pre>"},{"location":"Operators/kinetica-operators/#workbench-operator","title":"Workbench Operator","text":"<p>The Kinetica Workbench installation CR for the porter.sh operator is: -</p> YAML<pre><code>apiVersion: porter.sh/v1\nkind: Installation\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: kinetica-operators\n    meta.helm.sh/release-namespace: kinetica-system\n  labels:\n    app.kubernetes.io/instance: kinetica-operators\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kinetica-operators\n    app.kubernetes.io/version: 0.1.0\n    helm.sh/chart: kinetica-operators-0.1.0\n    installVersion: 0.38.10\n  name: kinetica-operators-wb-operator-install\n  namespace: kinetica-system\nspec:\n  action: install\n  agentConfig:\n    volumeSize: '0'\n  parameters:\n    environment: local\n  reference: docker.io/kinetica/workbench-operator:v7.1.9-7.rc3\n</code></pre>"},{"location":"Operators/kinetica-operators/#overriding-images-tags","title":"Overriding Images Tags","text":"Bash<pre><code>helm install -n kinetica-system kinetica-operators kinetica-operators/kinetica-operators \\\n--create-namespace \\\n--set provider=aks  \n--set dbOperator.image.tag=v7.1.9-7.rc3 \\\n--set dbOperator.image.repository=docker.io/kinetica/kinetica-k8s-operator \\\n--set wbOperator.image.repository=docker.io/kinetica/workbench-operator \\\n--set wbOperator.image.tag=v7.1.9-7.rc3\n</code></pre>"},{"location":"Reference/","title":"Reference Section","text":"<ul> <li> <p> Kinetica Operators Helm</p> <p>Kinetica Operators Helm charts &amp; values file reference data.</p> <p> Charts</p> </li> <li> <p> Kinetica Core DB CRDs-</p> <p>Kinetica DB Kubernetes CRD &amp; ConfigMap reference data.</p> <p> Cluster CRDs</p> </li> <li> <p> Kinetica Workbench CRDs</p> <p>Kinetica Workbench Kubernetes CRD &amp; ConfigMap reference data.</p> <p> Workbench</p> </li> </ul>"},{"location":"Reference/database/","title":"Kinetica Database Configuration","text":"<ul> <li>kubectl (yaml)</li> </ul>"},{"location":"Reference/database/#kineticacluster","title":"KineticaCluster","text":"<p>To deploy a new Database Instance into a Kubernetes cluster...</p> kubectl <p>Using kubetctl a CustomResource of type <code>KineticaCluster</code> is used to define a new Kinetica DB Cluster in a yaml file.</p> <p>The basic Group, Version, Kind or GVK to instantiate a Kinetica DB Cluster is as follows: -</p> kineticacluster.yaml<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\n</code></pre>"},{"location":"Reference/database/#metadata","title":"Metadata","text":"<p>to which we add a <code>metadata:</code> block for the name of the DB CR along with the <code>namespace</code> into which we are targetting the installation of the DB cluster.</p> kineticacluster.yaml<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\nmetadata:\n  name: my-kinetica-db-cr\n  namespace: gpudb\nspec:\n</code></pre>"},{"location":"Reference/database/#spec","title":"Spec","text":"<p>Under the <code>spec:</code> section of the KineticaCLuster CR we have a number of sections supporting different aspects of the deployed DB cluster:-</p> <ul> <li>gpudbCluster</li> <li>autoSuspend</li> <li>gadmin</li> </ul>"},{"location":"Reference/database/#gpudbcluster","title":"gpudbCluster","text":"<p>Configuartion items specific to the DB itself.</p> kineticacluster.yaml - gpudbCluster<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\nmetadata:\n  name: my-kinetica-db-cr\n  namespace: gpudb\nspec:\n  gpudbCluster:\n</code></pre>"},{"location":"Reference/database/#gpudbcluster_1","title":"gpudbCluster","text":"cluster name &amp; size<pre><code>clusterName: kinetica-cluster \nclusterSize: \n  tshirtSize: M \n  tshirtType: LargeCPU \nfqdn: kinetica-cluster.saas.kinetica.com\nhaRingName: default\nhasPools: false    \n</code></pre> <p><code>1. clusterName</code> - the user defined name of the Kinetica DB Cluster</p> <p><code>2. clusterSize</code> - block that defines the number of DB Ranks to run</p> <p><code>3. tshirtSize</code> - sets the cluster size to a defined size based upon the t-shirt size. Valid sizes are: -</p> <ul> <li><code>XS</code> -   1 DB Rank</li> <li><code>S</code> -    2 DB Ranks</li> <li><code>M</code> -    4 DB Ranks</li> <li><code>L</code> -    8 DB Ranks</li> <li><code>XL</code> -   16 DB Ranks</li> <li><code>XXL</code> -  32 DB Ranks</li> <li><code>XXXL</code> - 64 DB Ranks</li> </ul> <p><code>4. tshirtType</code> - block that defines the tyoe DB Ranks to run: -</p> <ul> <li><code>SmallCPU</code> - </li> <li><code>LargeCPU</code> -</li> <li><code>SmallGPU</code> - </li> <li><code>LargeGPU</code> -</li> </ul> <p><code>5. fqdn</code> - The fully qualified URL for the DB cluster. Used on the Ingress records for any exposed services.</p> <p><code>6. haRingName</code> - Default: <code>default</code></p> <p><code>7. hasPools</code> - Whether to enable the separate node 'pools' for \"infra\", \"compute\" pod scheduling.                 Default: false                 +optional</p>"},{"location":"Reference/database/#autosuspend","title":"autoSuspend","text":"<p>The DB Cluster autosuspend section allows for the spinning down of the core DB Pods to release the underlying Kubernetes nodes to reduce infrastructure costs when the DB is not in use. </p> kineticacluster.yaml - autoSuspend<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\nmetadata:\n  name: my-kinetica-db-cr\n  namespace: gpudb\nspec:\n  autoSuspend:\n    enabled: false\n    inactivityDuration: 1h0m0s\n</code></pre> <p><code>7.</code> the start of the <code>autoSuspend</code> definition</p> <p><code>8.</code> <code>enabled</code> when set to <code>true</code> auto suspend of the DB cluster is enabled otherwise set to <code>false</code> and no      automatic suspending of the DB takes place.  If omitted it defaults to <code>false</code></p> <p><code>9.</code> <code>inactivityDuration</code> the duration after which if no DB activity has taken place the DB will be suspended</p> <p>Horizontal Pod Autoscaler</p> <p>In order for <code>autoSuspend</code> to work correctly the Kubernetes Horizontal Pod Autoscaler needs to be deployed to the cluster.</p>"},{"location":"Reference/database/#gadmin","title":"gadmin","text":"<p>GAdmin the Database Administration Console</p> <p></p> kineticacluster.yaml - gadmin<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\nmetadata:\n  name: my-kinetica-db-cr\n  namespace: gpudb\nspec:\n  gadmin:\n    containerPort:\n      containerPort: 8080\n      name: gadmin\n      protocol: TCP\n    isEnabled: true\n</code></pre> <p><code>7.</code> <code>gadmin</code> configuration block definition</p> <p><code>8.</code> <code>containerPort</code> configuration block i.e. where <code>gadmin</code> is exposed on the DB Pod</p> <p><code>9.</code> <code>containerPort</code> the port number as an integer. Default: <code>8080</code></p> <p><code>10.</code> <code>name</code> the name of the port being exposed. Default:  <code>gadmin</code></p> <p><code>11.</code> <code>protocol</code> network protocal used. Default: <code>TCP</code></p> <p><code>12.</code> <code>isEnabled</code> whether <code>gadmin</code> is exposed from the DB pod. Default: <code>true</code></p>"},{"location":"Reference/database/#kineticauser","title":"KineticaUser","text":""},{"location":"Reference/database/#kineticagrant","title":"KineticaGrant","text":""},{"location":"Reference/database/#kineticaschema","title":"KineticaSchema","text":""},{"location":"Reference/database/#kineticaresourcegroup","title":"KineticaResourceGroup","text":""},{"location":"Reference/helm_kinetica_operators/","title":"Helm Chart Reference","text":""},{"location":"Reference/helm_kinetica_operators/#coming-soon","title":"Coming Soon","text":""},{"location":"Reference/kinetica_cluster_admins/","title":"Kinetica Cluster Admins Reference","text":""},{"location":"Reference/kinetica_cluster_admins/#full-kineticaclusteradmin-cr-structure","title":"Full KineticaClusterAdmin CR Structure","text":"kineticaclusteradmins.app.kinetica.com_sample.yaml<pre><code># APIVersion defines the versioned schema of this representation of an\n# object. Servers should convert recognized schemas to the latest\n# internal value, and may reject unrecognized values. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\napiVersion: app.kinetica.com/v1\n# Kind is a string value representing the REST resource this object\n# represents. Servers may infer this from the endpoint the client\n# submits requests to. Cannot be updated. In CamelCase. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\nkind: KineticaClusterAdmin\nmetadata: {}\n# KineticaClusterAdminSpec defines the desired state of\n# KineticaClusterAdmin\nspec:\n  # ForceDBStatus - Force a Status of the DB.\n  forceDbStatus: string\n  # Name - The name of the cluster to target.\n  kineticaClusterName: string\n  # Offline - Pause/Resume of the DB.\n  offline:\n    # Set to true if desired state is offline. The supported values are:\n    # true false\n    offline: false\n    # Optional parameters. The default value is an empty map (\n    # {} ). Supported Parameters: flush_to_disk Flush to disk when\n    # going offline The supported values are: true false\n    options: {}\n  # Rebalance of the DB.\n  rebalance:\n    # Optional parameters. The default value is an empty map (\n    # {} ). Supported Parameters: rebalance_sharded_data        If true,\n    # sharded data will be rebalanced approximately equally across the\n    # cluster. Note that for clusters with large amounts of sharded\n    # data, this data transfer could be time-consuming and result in\n    # delayed query responses. The default value is true. The supported\n    # values are: true false rebalance_unsharded_data   If true,\n    # unsharded data (a.k.a. randomly-sharded) will be rebalanced\n    # approximately equally across the cluster. Note that for clusters\n    # with large amounts of unsharded data, this data transfer could be\n    # time-consuming and result in delayed query responses. The default\n    # value is true. The supported values are: true false\n    # table_includes                Comma-separated list of unsharded table names\n    # to rebalance. Not applicable to sharded tables because they are\n    # always rebalanced. Cannot be used simultaneously with\n    # table_excludes. This parameter is ignored if\n    # rebalance_unsharded_data is false.\n    # table_excludes                Comma-separated list of unsharded table names\n    # to not rebalance. Not applicable to sharded tables because they\n    # are always rebalanced. Cannot be used simultaneously with\n    # table_includes. This parameter is ignored if rebalance_\n    # unsharded_data is false. aggressiveness               Influences how much\n    # data is moved at a time during rebalance. A higher aggressiveness\n    # will complete the rebalance faster. A lower aggressiveness will\n    # take longer but allow for better interleaving between the\n    # rebalance and other queries. Valid values are constants from 1\n    # (lowest) to 10 (highest). The default value is '1'.\n    # compact_after_rebalance   Perform compaction of deleted records\n    # once the rebalance completes to reclaim memory and disk space.\n    # Default is true, unless repair_incorrectly_sharded_data is set to\n    # true. The default value is true. The supported values are: true\n    # false compact_only                If set to true, ignore rebalance options\n    # and attempt to perform compaction of deleted records to reclaim\n    # memory and disk space without rebalancing first. The default\n    # value is false. The supported values are: true false\n    # repair_incorrectly_sharded_data       Scans for any data sharded\n    # incorrectly and re-routes the data to the correct location. Only\n    # necessary if /admin/verifydb reports an error in sharding\n    # alignment. This can be done as part of a typical rebalance after\n    # expanding the cluster or in a standalone fashion when it is\n    # believed that data is sharded incorrectly somewhere in the\n    # cluster. Compaction will not be performed by default when this is\n    # enabled. If this option is set to true, the time necessary to\n    # rebalance and the memory used by the rebalance may increase. The\n    # default value is false. The supported values are: true false\n    options: {}\n  # RegenerateDBConfig - Force regenerate of DB ConfigMap. true -\n  # restarts DB Pods after config generation false - writes new\n  # configuration without restarting the DB Pods\n  regenerateDBConfig:\n    # Restart - Scales down the DB STS and back up once the DB\n    # Configuration has been regenerated.\n    restart: false\n# KineticaClusterAdminStatus defines the observed state of\n# KineticaClusterAdmin\nstatus:\n  # Phase - The current phase/state of the Admin request\n  phase: string\n  # Processed - Indicates if the admin request has already been\n  # processed. Avoids the request being rerun in the case the Operator\n  # gets restarted.\n  processed: false\n</code></pre>"},{"location":"Reference/kinetica_cluster_backups/","title":"Kinetica Cluster Backups Reference","text":""},{"location":"Reference/kinetica_cluster_backups/#full-kineticaclusterbackup-cr-structure","title":"Full KineticaClusterBackup CR Structure","text":"kineticaclusterbackups.app.kinetica.com_sample.yaml<pre><code># APIVersion defines the versioned schema of this representation of an\n# object. Servers should convert recognized schemas to the latest\n# internal value, and may reject unrecognized values. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\napiVersion: app.kinetica.com/v1\n# Kind is a string value representing the REST resource this object\n# represents. Servers may infer this from the endpoint the client\n# submits requests to. Cannot be updated. In CamelCase. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\nkind: KineticaClusterBackup \nmetadata: {}\n# Fields specific to the linked backup engine\nprovider:\n  # Name of the backup/restore provider. FOR INTERNAL USE ONLY.\n  backupProvider: \"velero\"\n  # Name of the backup in the linked BackupProvider. FOR INTERNAL USE\n  # ONLY.\n  linkedItemName: \"\"\n# BackupSpec defines the specification for a Velero backup.\nspec:\n  # DefaultVolumesToRestic specifies whether restic should be used to\n  # take a backup of all pod volumes by default.\n  defaultVolumesToRestic: true\n  # ExcludedNamespaces contains a list of namespaces that are not\n  # included in the backup.\n  excludedNamespaces: [\"string\"]\n  # ExcludedResources is a slice of resource names that are not included\n  # in the backup.\n  excludedResources: [\"string\"]\n  # Hooks represent custom behaviors that should be executed at\n  # different phases of the backup.\n  hooks:\n    # Resources are hooks that should be executed when backing up\n    # individual instances of a resource.\n    resources:\n    - excludedNamespaces: [\"string\"]\n      # ExcludedResources specifies the resources to which this hook\n      # spec does not apply.\n      excludedResources: [\"string\"]\n      # IncludedNamespaces specifies the namespaces to which this hook\n      # spec applies. If empty, it applies to all namespaces.\n      includedNamespaces: [\"string\"]\n      # IncludedResources specifies the resources to which this hook\n      # spec applies. If empty, it applies to all resources.\n      includedResources: [\"string\"]\n      # LabelSelector, if specified, filters the resources to which this\n      # hook spec applies.\n      labelSelector:\n        # matchExpressions is a list of label selector requirements. The\n        # requirements are ANDed.\n        matchExpressions:\n        - key: string\n          # operator represents a key's relationship to a set of values.\n          # Valid operators are In, NotIn, Exists and DoesNotExist.\n          operator: string\n          # values is an array of string values. If the operator is In\n          # or NotIn, the values array must be non-empty. If the\n          # operator is Exists or DoesNotExist, the values array must\n          # be empty. This array is replaced during a strategic merge\n          # patch.\n          values: [\"string\"]\n        # matchLabels is a map of {key,value} pairs. A single\n        # {key,value} in the matchLabels map is equivalent to an\n        # element of matchExpressions, whose key field is \"key\", the\n        # operator is \"In\", and the values array contains only \"value\".\n        # The requirements are ANDed.\n        matchLabels: {}\n      # Name is the name of this hook.\n      name: string\n      # PostHooks is a list of BackupResourceHooks to execute after\n      # storing the item in the backup. These are executed after\n      # all \"additional items\" from item actions are processed.\n      post:\n      - exec:\n          # Command is the command and arguments to execute.\n          command: [\"string\"]\n          # Container is the container in the pod where the command\n          # should be executed. If not specified, the pod's first\n          # container is used.\n          container: string\n          # OnError specifies how Velero should behave if it encounters\n          # an error executing this hook.\n          onError: string\n          # Timeout defines the maximum amount of time Velero should\n          # wait for the hook to complete before considering the\n          # execution a failure.\n          timeout: string\n      # PreHooks is a list of BackupResourceHooks to execute prior to\n      # storing the item in the backup. These are executed before\n      # any \"additional items\" from item actions are processed.\n      pre:\n      - exec:\n          # Command is the command and arguments to execute.\n          command: [\"string\"]\n          # Container is the container in the pod where the command\n          # should be executed. If not specified, the pod's first\n          # container is used.\n          container: string\n          # OnError specifies how Velero should behave if it encounters\n          # an error executing this hook.\n          onError: string\n          # Timeout defines the maximum amount of time Velero should\n          # wait for the hook to complete before considering the\n          # execution a failure.\n          timeout: string\n  # IncludeClusterResources specifies whether cluster-scoped resources\n  # should be included for consideration in the backup.\n  includeClusterResources: true\n  # IncludedNamespaces is a slice of namespace names to include objects\n  # from. If empty, all namespaces are included.\n  includedNamespaces: [\"string\"]\n  # IncludedResources is a slice of resource names to include in the\n  # backup. If empty, all resources are included.\n  includedResources: [\"string\"]\n  # LabelSelector is a metav1.LabelSelector to filter with when adding\n  # individual objects to the backup. If empty or nil, all objects are\n  # included. Optional.\n  labelSelector:\n    # matchExpressions is a list of label selector requirements. The\n    # requirements are ANDed.\n    matchExpressions:\n    - key: string\n      # operator represents a key's relationship to a set of values.\n      # Valid operators are In, NotIn, Exists and DoesNotExist.\n      operator: string\n      # values is an array of string values. If the operator is In or\n      # NotIn, the values array must be non-empty. If the operator is\n      # Exists or DoesNotExist, the values array must be empty. This\n      # array is replaced during a strategic merge patch.\n      values: [\"string\"]\n    # matchLabels is a map of {key,value} pairs. A single {key,value} in\n    # the matchLabels map is equivalent to an element of\n    # matchExpressions, whose key field is \"key\", the operator is \"In\",\n    # and the values array contains only \"value\". The requirements are\n    # ANDed.\n    matchLabels: {} metadata: labels: {}\n  # OrderedResources specifies the backup order of resources of specific\n  # Kind. The map key is the Kind name and value is a list of resource\n  # names separated by commas. Each resource name has\n  # format \"namespace/resourcename\".  For cluster resources, simply\n  # use \"resourcename\".\n  orderedResources: {}\n  # SnapshotVolumes specifies whether to take cloud snapshots of any\n  # PV's referenced in the set of objects included in the Backup.\n  snapshotVolumes: true\n  # StorageLocation is a string containing the name of a\n  # BackupStorageLocation where the backup should be stored.\n  storageLocation: string\n  # TTL is a time.Duration-parseable string describing how long the\n  # Backup should be retained for.\n  ttl: string\n  # VolumeSnapshotLocations is a list containing names of\n  # VolumeSnapshotLocations associated with this backup.\n  volumeSnapshotLocations: [\"string\"] status:\n  # ClusterSize the current number of ranks &amp; type i.e. CPU or GPU of\n  # the cluster when the backup took place.\n  clusterSize:\n    # ClusterSizeEnum - T-Shirt size of the Kinetica DB Cluster i.e. a\n    # representation of the number of nodes in a simple to understand\n    # T-Short size scheme. This indicates the size of the cluster i.e.\n    # the number of nodes. It does not identify the size of the cloud\n    # provider nodes. For node size see ClusterTypeEnum. Supported\n    # Values are: - XS S M L XL XXL XXXL\n    tshirtSize: string\n    # ClusterTypeEnum - An Enum of the node types of a KineticaCluster\n    # e.g. CPU, GPU along with the Cloud Provider node size e.g. size\n    # of the VM.\n    tshirtType: string coldTierBackup: string\n  # CompletionTimestamp records the time a backup was completed.\n  # Completion time is recorded even on failed backups. Completion time\n  # is recorded before uploading the backup object. The server's time\n  # is used for CompletionTimestamps\n  completionTimestamp: string\n  # Errors is a count of all error messages that were generated during\n  # execution of the backup.  The actual errors are in the backup's log\n  # file in object storage.\n  errors: 1\n  # Expiration is when this Backup is eligible for garbage-collection.\n  expiration: string\n  # FormatVersion is the backup format version, including major, minor,\n  # and patch version.\n  formatVersion: string\n  # Phase is the current state of the Backup.\n  phase: string\n  # Progress contains information about the backup's execution progress.\n  # Note that this information is best-effort only -- if Velero fails\n  # to update it during a backup for any reason, it may be\n  # inaccurate/stale.\n  progress:\n    # ItemsBackedUp is the number of items that have actually been\n    # written to the backup tarball so far.\n    itemsBackedUp: 1\n    # TotalItems is the total number of items to be backed up. This\n    # number may change throughout the execution of the backup due to\n    # plugins that return additional related items to back up, the\n    # velero.io/exclude-from-backup label, and various other filters\n    # that happen as items are processed.\n    totalItems: 1\n  # StartTimestamp records the time a backup was started. Separate from\n  # CreationTimestamp, since that value changes on restores. The\n  # server's time is used for StartTimestamps\n  startTimestamp: string\n  # ValidationErrors is a slice of all validation errors\n  # (if applicable).\n  validationErrors: [\"string\"]\n  # Version is the backup format major version. Deprecated: Please see\n  # FormatVersion\n  version: 1\n  # VolumeSnapshotsAttempted is the total number of attempted volume\n  # snapshots for this backup.\n  volumeSnapshotsAttempted: 1\n  # VolumeSnapshotsCompleted is the total number of successfully\n  # completed volume snapshots for this backup.\n  volumeSnapshotsCompleted: 1\n  # Warnings is a count of all warning messages that were generated\n  # during execution of the backup. The actual warnings are in the\n  # backup's log file in object storage.\n  warnings: 1\n</code></pre>"},{"location":"Reference/kinetica_cluster_grants/","title":"Kinetica Cluster Grants CRD Reference","text":""},{"location":"Reference/kinetica_cluster_grants/#full-kineticagrant-cr-structure","title":"Full KineticaGrant CR Structure","text":"kineticagrants.app.kinetica.com_sample.yaml<pre><code># APIVersion defines the versioned schema of this representation of an\n# object. Servers should convert recognized schemas to the latest\n# internal value, and may reject unrecognized values. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\napiVersion: app.kinetica.com/v1\n# Kind is a string value representing the REST resource this object\n# represents. Servers may infer this from the endpoint the client\n# submits requests to. Cannot be updated. In CamelCase. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\nkind: KineticaGrant \nmetadata: {}\n# KineticaGrantSpec defines the desired state of KineticaGrant\nspec:\n  # Grants system-level and/or table permissions to a user or role.\n  addGrantAllOnSchemaRequest:\n    # Name of the user or role that will be granted membership in input\n    # parameter role. Must be an existing user or role.\n    member: string\n    # Optional parameters. The default value is an empty map ( {} ).\n    options: {}\n    # SchemaName - name of the schema on which to perform the Grant All\n    schemaName: string\n  # Grants system-level and/or table permissions to a user or role.\n  addGrantPermissionRequest:\n    # Optional parameters. The default value is an empty map ( {} ).\n    options: {}\n    # Permission to grant to the user or role. Supported\n    # Values    Description system_admin    Full access to all data and\n    # system functions. system_user_admin   Access to administer users\n    # and roles that do not have system_admin permission.\n    # system_write  Read and write access to all tables.\n    # system_read   Read-only access to all tables.\n    systemPermission:\n      # UID of the user or role to which the permission will be granted.\n      # Must be an existing user or role.\n      name: string\n      # Optional parameters. The default value is an empty map (\n      # {} ). Supported Parameters: resource_group  Name of an existing\n      # resource group to associate with this role.\n      options: {}\n      # Permission to grant to the user or role. Supported\n      # Values  Description table_admin Full read/write and\n      # administrative access to the table. table_insert    Insert access\n      # to the table. table_update  Update access to the table.\n      # table_delete    Delete access to the table. table_read  Read access\n      # to the table.\n      permission: string\n    # Permission to grant to the user or role. Supported\n    # Values    Description&lt;br/&gt; system_admin   Full access to all data and\n    # system functions.&lt;br/&gt; system_user_admin  Access to administer\n    # users and roles that do not have system_admin permission.&lt;br/&gt;\n    # system_write  Read and write access to all tables.&lt;br/&gt;\n    # system_read   Read-only access to all tables.&lt;br/&gt;\n    tablePermissions:\n    - filter_expression: \"\"\n      # UID of the user or role to which the permission will be granted.\n      # Must be an existing user or role.\n      name: string\n      # Optional parameters. The default value is an empty map (\n      # {} ). Supported Parameters: resource_group  Name of an existing\n      # resource group to associate with this role.\n      options: {}\n      # Permission to grant to the user or role. Supported\n      # Values  Description table_admin Full read/write and\n      # administrative access to the table. table_insert    Insert access\n      # to the table. table_update  Update access to the table.\n      # table_delete    Delete access to the table. table_read  Read access\n      # to the table.\n      permission: string\n      # Name of the table for which the Permission is to be granted\n      table_name: string\n  # Grants membership in a role to a user or role.\n  addGrantRoleRequest:\n    # Name of the user or role that will be granted membership in input\n    # parameter role. Must be an existing user or role.\n    member: string\n    # Optional parameters. The default value is an empty map ( {} ).\n    options: {}\n    # Name of the role in which membership will be granted. Must be an\n    # existing role.\n    role: string\n  # Debug debug the call\n  debug: false\n  # RingName is the name of the kinetica ring that this user belongs\n  # to.\n  ringName: string\n# KineticaGrantStatus defines the observed state of KineticaGrant\nstatus:\n  # DBStringResponse - The GPUdb server embeds the endpoint response\n  # inside a standard response structure which contains status\n  # information and the actual response to the query.\n  db_response: data: string\n    # This embedded JSON represents the result of the endpoint\n    data_str: string\n    # API Call Specific\n    data_type: string\n    # Empty if success or an error message\n    message: string\n    # 'OK' or 'ERROR'\n    status: string \n    ldap_response: string\n</code></pre>"},{"location":"Reference/kinetica_cluster_reference/","title":"Core DB CRDs","text":"<ul> <li> <p> DB Clusters</p> <p>Core Kinetica Database Cluster Management CRD &amp; sample CR.</p> <p> KineticaCluster</p> </li> <li> <p> DB Users</p> <p>Kinetica Database User Management CRD &amp; sample CR.</p> <p> KineticaUser</p> </li> <li> <p> DB Roles</p> <p>Kinetica Database Role Management CRD &amp; sample CR.</p> <p> KineticaRole</p> </li> <li> <p> DB Schemas</p> <p>Kinetica Database Schema Management CRD &amp; sample CR.</p> <p> KineticaSchema</p> </li> <li> <p> DB Grants</p> <p>Kinetica Database Grant Management CRD &amp; sample CR.</p> <p> KineticaGrant</p> </li> <li> <p> DB Resource Groups</p> <p>Kinetica Database Resource Group Management CRD &amp; sample CR.</p> <p> KineticaResourceGroup</p> </li> <li> <p> DB Administration</p> <p>Kinetica Database Administration CRD &amp; sample CR.</p> <p> KineticaAdmin</p> </li> <li> <p> DB Backups</p> <p>Kinetica Database Backup Management CRD &amp; sample CR.</p> <p>Note</p> <p>This requires Velero to be installed on the Kubernetes Cluster.</p> <p> KineticaBackup</p> </li> <li> <p> DB Restore</p> <p>Kinetica Database Restore CRD &amp; sample CR.</p> <p>Note</p> <p>This requires Velero to be installed on the Kubernetes Cluster.</p> <p> KineticaRestore</p> </li> </ul>"},{"location":"Reference/kinetica_cluster_resource_groups/","title":"Kinetica Cluster Resource Groups CRD Reference","text":""},{"location":"Reference/kinetica_cluster_resource_groups/#full-kineticaresourcegroup-cr-structure","title":"Full KineticaResourceGroup CR Structure","text":"kineticaclusterresourcegroups.app.kinetica.com_sample.yaml<pre><code># APIVersion defines the versioned schema of this representation of an\n# object. Servers should convert recognized schemas to the latest\n# internal value, and may reject unrecognized values. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\napiVersion: app.kinetica.com/v1\n# Kind is a string value representing the REST resource this object\n# represents. Servers may infer this from the endpoint the client\n# submits requests to. Cannot be updated. In CamelCase. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\nkind: KineticaClusterResourceGroup \nmetadata: {}\n# KineticaClusterResourceGroupSpec defines the desired state of\n# KineticaClusterResourceGroup\nspec: \n  db_create_resource_group_request:\n    # AdjoiningResourceGroup -\n    adjoining_resource_group: \"\"\n    # Name - name of the DB ResourceGroup\n    # https://docs.kinetica.com/7.1/azure/sql/resource_group/?search-highlight=resource+group#id-baea5b60-769c-5373-bff1-53f4f1ca5c21\n    name: string\n    # Options - DB Options used when creating the ResourceGroup\n    options: {}\n    # Ranking - Indicates the relative ranking among existing resource\n    # groups where this new resource group will be placed. When using\n    # before or after, specify which resource group this one will be\n    # inserted before or after in input parameter\n    # adjoining_resource_group. The supported values are: first last\n    # before after\n    ranking: \"\"\n  # RingName is the name of the kinetica ring that this user belongs\n  # to.\n  ringName: string\n# KineticaClusterResourceGroupStatus defines the observed state of\n# KineticaClusterResourceGroup\nstatus: \n  provisioned: string\n</code></pre>"},{"location":"Reference/kinetica_cluster_restores/","title":"Kinetica Cluster Restores Reference","text":""},{"location":"Reference/kinetica_cluster_restores/#full-kineticaclusterrestore-cr-structure","title":"Full KineticaClusterRestore CR Structure","text":"kineticaclusterrestores.app.kinetica.com_sample.yaml<pre><code># APIVersion defines the versioned schema of this representation of an\n# object. Servers should convert recognized schemas to the latest\n# internal value, and may reject unrecognized values. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\napiVersion: app.kinetica.com/v1\n# Kind is a string value representing the REST resource this object\n# represents. Servers may infer this from the endpoint the client\n# submits requests to. Cannot be updated. In CamelCase. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\nkind: KineticaClusterRestore \nmetadata: {}\n# RestoreSpec defines the specification for a Velero restore.\nspec:\n  # BackupName is the unique name of the Velero backup to restore from.\n  backupName: string\n  # ExcludedNamespaces contains a list of namespaces that are not\n  # included in the restore.\n  excludedNamespaces: [\"string\"]\n  # ExcludedResources is a slice of resource names that are not included\n  # in the restore.\n  excludedResources: [\"string\"]\n  # IncludeClusterResources specifies whether cluster-scoped resources\n  # should be included for consideration in the restore. If null,\n  # defaults to true.\n  includeClusterResources: true\n  # IncludedNamespaces is a slice of namespace names to include objects\n  # from. If empty, all namespaces are included.\n  includedNamespaces: [\"string\"]\n  # IncludedResources is a slice of resource names to include in the\n  # restore. If empty, all resources in the backup are included.\n  includedResources: [\"string\"]\n  # LabelSelector is a metav1.LabelSelector to filter with when\n  # restoring individual objects from the backup. If empty or nil, all\n  # objects are included. Optional.\n  labelSelector:\n    # matchExpressions is a list of label selector requirements. The\n    # requirements are ANDed.\n    matchExpressions:\n    - key: string\n      # operator represents a key's relationship to a set of values.\n      # Valid operators are In, NotIn, Exists and DoesNotExist.\n      operator: string\n      # values is an array of string values. If the operator is In or\n      # NotIn, the values array must be non-empty. If the operator is\n      # Exists or DoesNotExist, the values array must be empty. This\n      # array is replaced during a strategic merge patch.\n      values: [\"string\"]\n    # matchLabels is a map of {key,value} pairs. A single {key,value} in\n    # the matchLabels map is equivalent to an element of\n    # matchExpressions, whose key field is \"key\", the operator is \"In\",\n    # and the values array contains only \"value\". The requirements are\n    # ANDed.\n    matchLabels: {}\n  # NamespaceMapping is a map of source namespace names to target\n  # namespace names to restore into. Any source namespaces not included\n  # in the map will be restored into namespaces of the same name.\n  namespaceMapping: {}\n  # RestorePVs specifies whether to restore all included PVs from\n  # snapshot (via the cloudprovider).\n  restorePVs: true\n  # ScheduleName is the unique name of the Velero schedule to restore\n  # from. If specified, and BackupName is empty, Velero will restore\n  # from the most recent successful backup created from this schedule.\n  scheduleName: string status: coldTierRestore: \"\"\n  # CompletionTimestamp records the time the restore operation was\n  # completed. Completion time is recorded even on failed restore. The\n  # server's time is used for StartTimestamps\n  completionTimestamp: string\n  # Errors is a count of all error messages that were generated during\n  # execution of the restore. The actual errors are stored in object\n  # storage.\n  errors: 1\n  # FailureReason is an error that caused the entire restore to fail.\n  failureReason: string\n  # Phase is the current state of the Restore\n  phase: string\n  # Progress contains information about the restore's execution\n  # progress. Note that this information is best-effort only -- if\n  # Velero fails to update it during a restore for any reason, it may\n  # be inaccurate/stale.\n  progress:\n    # ItemsRestored is the number of items that have actually been\n    # restored so far\n    itemsRestored: 1\n    # TotalItems is the total number of items to be restored. This\n    # number may change throughout the execution of the restore due to\n    # plugins that return additional related items to restore\n    totalItems: 1\n  # StartTimestamp records the time the restore operation was started.\n  # The server's time is used for StartTimestamps\n  startTimestamp: string\n  # ValidationErrors is a slice of all validation errors(if applicable)\n  validationErrors: [\"string\"]\n  # Warnings is a count of all warning messages that were generated\n  # during execution of the restore. The actual warnings are stored in\n  # object storage.\n  warnings: 1\n</code></pre>"},{"location":"Reference/kinetica_cluster_roles/","title":"Kinetica Cluster Roles CRD","text":""},{"location":"Reference/kinetica_cluster_roles/#full-kineticarole-cr-structure","title":"Full KineticaRole CR Structure","text":"kineticaroles.app.kinetica.com_sample.yaml<pre><code># APIVersion defines the versioned schema of this representation of an\n# object. Servers should convert recognized schemas to the latest\n# internal value, and may reject unrecognized values. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\napiVersion: app.kinetica.com/v1\n# Kind is a string value representing the REST resource this object\n# represents. Servers may infer this from the endpoint the client\n# submits requests to. Cannot be updated. In CamelCase. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\nkind: KineticaRole \nmetadata: {}\n# KineticaRoleSpec defines the desired state of KineticaRole\nspec:\n  # AlterRoleRequest Kinetica DB REST API Request Format Object.\n  alter_role:\n    # Action - Modification operation to be applied to the role.\n    action: string\n    # Role UID - Name of the role to be altered. Must be an existing\n    # role.\n    name: string\n    # Optional parameters. The default value is an empty map ( {} ).\n    options: {}\n    # Value - The value of the modification, depending on input\n    # parameter action.\n    value: string\n  # Debug debug the call\n  debug: false\n  # RingName is the name of the kinetica ring that this user belongs\n  # to.\n  ringName: string\n  # AddRoleRequest Kinetica DB REST API Request Format Object.\n  role:\n    # User UID\n    name: string\n    # Optional parameters. The default value is an empty map (\n    # {} ). Supported Parameters: resource_group    Name of an existing\n    # resource group to associate with this role.\n    options: {}\n    # ResourceGroupName of an existing resource group to associate with\n    # this role\n    resourceGroupName: \"\"\n# KineticaRoleStatus defines the observed state of KineticaRole\nstatus:\n  # DBStringResponse - The GPUdb server embeds the endpoint response\n  # inside a standard response structure which contains status\n  # information and the actual response to the query.\n  db_response: data: string\n    # This embedded JSON represents the result of the endpoint\n    data_str: string\n    # API Call Specific\n    data_type: string\n    # Empty if success or an error message\n    message: string\n    # 'OK' or 'ERROR'\n    status: string \n    ldap_response: string\n</code></pre>"},{"location":"Reference/kinetica_cluster_schemas/","title":"Kinetica Cluster Schemas CRD Reference","text":""},{"location":"Reference/kinetica_cluster_schemas/#full-kinetica-cluster-schemas-cr-structure","title":"Full Kinetica Cluster Schemas CR Structure","text":"kineticaclusterschemas.app.kinetica.com_sample.yaml<pre><code># APIVersion defines the versioned schema of this representation of an\n# object. Servers should convert recognized schemas to the latest\n# internal value, and may reject unrecognized values. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\napiVersion: app.kinetica.com/v1\n# Kind is a string value representing the REST resource this object\n# represents. Servers may infer this from the endpoint the client\n# submits requests to. Cannot be updated. In CamelCase. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\nkind: KineticaClusterSchema \nmetadata: {}\n# KineticaClusterSchemaSpec defines the desired state of\n# KineticaClusterSchema\nspec: \n  db_create_schema_request:\n    # Name - the name of the resource group to create in the DB\n    name: string\n    # Optional parameters. The default value is an empty map (\n    # {} ). Supported Parameters: \"max_cpu_concurrency\", \"max_data\"\n    options: {}\n  # RingName is the name of the kinetica ring that this user belongs\n  # to.\n  ringName: string\n# KineticaClusterSchemaStatus defines the observed state of\n# KineticaClusterSchema\nstatus: \n  provisioned: string\n</code></pre>"},{"location":"Reference/kinetica_cluster_users/","title":"Kinetica Cluster Users CRD Reference","text":""},{"location":"Reference/kinetica_cluster_users/#full-kineticauser-cr-structure","title":"Full KineticaUser CR Structure","text":"kineticausers.app.kinetica.com_sample.yaml<pre><code># APIVersion defines the versioned schema of this representation of an\n# object. Servers should convert recognized schemas to the latest\n# internal value, and may reject unrecognized values. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\napiVersion: app.kinetica.com/v1\n# Kind is a string value representing the REST resource this object\n# represents. Servers may infer this from the endpoint the client\n# submits requests to. Cannot be updated. In CamelCase. More info:\n# https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\nkind: KineticaUser\nmetadata: {}\n# KineticaUserSpec defines the desired state of KineticaUser\nspec:\n  # Action field contains UserActionEnum field indicating whether it is\n  # an Upsert or Change Password operation. For deletion delete the\n  # KineticaUser CR and a finalizer will remove the user from LDAP.\n  action: string\n  # ChangePassword specific fields\n  changePassword:\n    # PasswordSecret - Not the actual user password but the name of a\n    # Kubernetes Secret containing a Data element with a Password\n    # attribute. The secret is removed on user creation. Must be in the\n    # same namespace as the Kinetica Cluster. Must contain the\n    # following fields: - oldPassword newPassword\n    passwordSecret: string\n  # Debug debug the call\n  debug: false\n  # GroupID - Organisation or Team Id the user belongs to.\n  groupId: string\n  # Create the user in Reveal\n  reveal: true\n  # RingName is the name of the kinetica ring that this user belongs\n  # to.\n  ringName: string\n  # UID is the username (not UUID UID).\n  uid: string\n  # Upsert specific fields\n  upsert:\n    # CreateHomeDirectory - when true, a home directory in KiFS is\n    # created for this user The default value is true. The supported\n    # values are: true false\n    createHomeDirectory: true\n    # DB Memory user data size limit\n    dataLimit: \"10Gi\"\n    # DisplayName\n    displayName: string\n    # GivenName is Firstname also called Christian name. givenName in\n    # LDAP terms.\n    givenName: string\n    # KIFs user data size limit\n    kifsDataLimit: \"2Gi\"\n    # LastName refers to last name or surname. sn in LDAP terms.\n    lastName: string\n    # Options -\n    options: {}\n    # PasswordSecret - Not the actual user password but the name of a\n    # Kubernetes Secret containing a Data element with a Password\n    # attribute. The secret is removed on user creation. Must be in the\n    # same namespace as the Kinetica Cluster.\n    passwordSecret: string\n    # UPN or UserPrincipalName - e.g. guyt@cp.com  \n    # Looks like an email address.\n    userPrincipalName: string\n  # UUID is the user unique UUID from the Control Plane.\n  uuid: string\n# KineticaUserStatus defines the observed state of KineticaUser\nstatus:\n  # DBStringResponse - The GPUdb server embeds the endpoint response\n  # inside a standard response structure which contains status\n  # information and the actual response to the query.\n  db_response: data: string\n    # This embedded JSON represents the result of the endpoint\n    data_str: string\n    # API Call Specific\n    data_type: string\n    # Empty if success or an error message\n    message: string\n    # 'OK' or 'ERROR'\n    status: string \n    ldap_response: string \n    reveal_admin: string\n</code></pre>"},{"location":"Reference/kinetica_clusters/","title":"Macro Syntax Error","text":"<p>File: <code>Reference/kinetica_clusters.md</code></p> <p>Line 1129 in Markdown file: unexpected '.' Markdown<pre><code>          publicURL: \"https://:8082/gpudb-{{.Rank}}\"\n</code></pre></p>"},{"location":"Reference/kinetica_workbench/","title":"Workbench CRD Reference","text":""},{"location":"Reference/kinetica_workbench/#coming-soon","title":"Coming Soon","text":""},{"location":"Reference/workbench/","title":"Kinetica Workbench Configuration","text":"<ul> <li>kubectl (yaml)</li> <li>Helm Chart</li> </ul>"},{"location":"Reference/workbench/#workbench","title":"Workbench","text":"kubectl <p>Using kubetctl a CustomResource of type <code>KineticaCluster</code> is used to define a new Kinetica DB Cluster in a yaml file.</p> <p>The basic Group, Version, Kind or GVK to instantiate a Kinetica Workbench is as follows: -</p> Workbench GVK<pre><code>apiVersion: workbench.com.kinetica/v1\nkind: Workbench\n</code></pre>"},{"location":"Reference/workbench/#metadata","title":"Metadata","text":"<p>to which we add a <code>metadata:</code> block for the name of the DB CR along with the <code>namespace</code> into which we are targetting the installation of the DB cluster.</p> Workbench metadata<pre><code>apiVersion: workbench.com.kinetica/v1\nkind: Workbench\nmetadata:\n  name: workbench-kinetica-cluster\n  namespace: gpudb\n</code></pre> <p>The simplest valid Workbench CR looks as follows: -</p> workbench.yaml<pre><code>apiVersion: workbench.com.kinetica/v1\nkind: Workbench\nmetadata:\n  name: workbench-kinetica-cluster\n  namespace: gpudb\nspec:\n  executeSqlLimit: 10000\n  fqdn: kinetica-cluster.saas.kinetica.com\n  image: kineticastagingcloud/workbench:v7.1.9-8.rc1\n  letsEncrypt:\n    enabled: false\n  userIdleTimeout: 60\n  ingressController: nginx-ingress\n</code></pre> <p><code>1. clusterName</code> - the user defined name of the Kinetica DB Cluster</p> <p><code>2. clusterSize</code> - block that defines the number of DB Ranks to run</p> helm"},{"location":"Setup/","title":"Kinetica for Kubernetes Setup","text":"<ul> <li> <p> Prepare to Install</p> <p>What you need to know &amp; do before beginning an installation.</p> <p> Preparation and Prerequisites</p> </li> <li> <p> Set up in 15 minutes</p> <p>Install the Kinetica DB with Helm and get up and running in minutes.</p> <p> Installation</p> </li> <li> <p> Beyond a Simple Installation</p> <p>It is possible using the Helm Charts and Kinetica CRDs to customize your installation in a number of ways.</p> <p> Advanced Topics</p> </li> </ul>"},{"location":"Support/","title":"Support","text":"<ul> <li> <p> Taking the next steps</p> <p>Further tutorials or help on configuring Kinetica in different environments. </p> <p> Help &amp; Tutorials</p> </li> <li> <p> Locating Issues</p> <p>In the unlikely event you require information on how to troubleshoot your installation, help can be found here.</p> <p> Troubleshooting</p> </li> </ul>"},{"location":"Troubleshooting/troubleshooting/","title":"Troubleshooting","text":""},{"location":"Troubleshooting/troubleshooting/#coming-soon","title":"Coming Soon","text":""},{"location":"blog/","title":"Blog","text":""}]}