{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>This repository provides the helm chart for deploying the Kubernetes Operators for Database and Workbench. Once the operators are installed, you should be able to use the Kinetica Database and Workbench CRDs to deploy and manage Kinetica clusters and workbenches.</p>"},{"location":"#add-kinetica-helm-repository","title":"Add Kinetica Helm Repository","text":"<p>To add the Kinetica Helm repoistory to Helm 3:-</p> Bash<pre><code>helm repo add kinetica-operators https://kineticadb.github.io/charts\nhelm repo update\n\n# if you get a 404 error on the .tgz file, you may do the following\n# you could get this if you had previously added the repo and the chart has been updated in development\nhelm repo remove kinetica-operators\nhelm repo add kinetica-operators https://kineticadb.github.io/charts\n</code></pre> <p>Installation values will depend on the target kubernetes platform you are using.</p> <p>This chart provides out of the box support for trying out in K3s and Kind clusters. For k3s, you should be able to use either the GPU and CPU version of the databases. In these platforms, a non production configuration of the Kinetica Database and Workbench is also deployed for you to get started. However, you should be able to change the k3s values file to deploy in other platforms as well. For fine grained configuration of the Database or the Workbench, refer to the Database and Workbench documentation.</p> <p>We use the same chart for our SaaS and AWS Marketplace offerings. If you want to try out in SaaS or AWS Marketplace, follow those links, you need not use this chart directly.</p> <p>Current version of the chart supports kubernetes version 1.25 and above.</p>"},{"location":"#k3s-k3sio","title":"k3s (k3s.io)","text":"<p>Refer to  Kinetica on K3s</p>"},{"location":"#kind-kubernetes-in-docker-kindsigsk8sio","title":"Kind (kubernetes in docker kind.sigs.k8s.io)","text":"<p>Refer to  Kinetica on Kind</p>"},{"location":"#k8s-any-flavour-kubernetesio","title":"K8s - Any flavour (kubernetes.io)","text":"<p>Refer to  Kinetica on K8s</p>"},{"location":"Database/database/","title":"Kinetica Database Configuration","text":"<ul> <li>kubectl (yaml)</li> </ul>"},{"location":"Database/database/#kineticacluster","title":"KineticaCluster","text":"<p>To deploy a new Database Instance into a Kubernetes cluster...</p> kubectl <p>Using kubetctl a CustomResource of type <code>KineticaCluster</code> is used to define a new Kinetica DB Cluster in a yaml file.</p> <p>The basic Group, Version, Kind or GVK to instantiate a Kinetica DB Cluster is as follows: -</p> kineticacluster.yaml<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\n</code></pre>"},{"location":"Database/database/#metadata","title":"Metadata","text":"<p>to which we add a <code>metadata:</code> block for the name of the DB CR along with the <code>namespace</code> into which we are targetting the installation of the DB cluster.</p> kineticacluster.yaml<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\nmetadata:\n  name: my-kinetica-db-cr\n  namespace: gpudb\nspec:\n</code></pre>"},{"location":"Database/database/#spec","title":"Spec","text":"<p>Under the <code>spec:</code> section of the KineticaCLuster CR we have a number of sections supporting different aspects of the deployed DB cluster:-</p> <ul> <li>gpudbCluster</li> <li>autoSuspend</li> <li>gadmin</li> </ul>"},{"location":"Database/database/#gpudbcluster","title":"gpudbCluster","text":"<p>Configuartion items specific to the DB itself.</p> kineticacluster.yaml - gpudbCluster<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\nmetadata:\n  name: my-kinetica-db-cr\n  namespace: gpudb\nspec:\n  gpudbCluster:\n</code></pre>"},{"location":"Database/database/#gpudbcluster_1","title":"gpudbCluster","text":"cluster name &amp; size<pre><code>clusterName: kinetica-cluster \nclusterSize: \n  tshirtSize: M \n  tshirtType: LargeCPU \nfqdn: kinetica-cluster.saas.kinetica.com\nhaRingName: default\nhasPools: false    \n</code></pre> <p><code>1. clusterName</code> - the user defined name of the Kinetica DB Cluster</p> <p><code>2. clusterSize</code> - block that defines the number of DB Ranks to run</p> <p><code>3. tshirtSize</code> - sets the cluster size to a defined size based upon the t-shirt size. Valid sizes are: -</p> <ul> <li><code>XS</code> -   1 DB Rank</li> <li><code>S</code> -    2 DB Ranks</li> <li><code>M</code> -    4 DB Ranks</li> <li><code>L</code> -    8 DB Ranks</li> <li><code>XL</code> -   16 DB Ranks</li> <li><code>XXL</code> -  32 DB Ranks</li> <li><code>XXXL</code> - 64 DB Ranks</li> </ul> <p><code>4. tshirtType</code> - block that defines the tyoe DB Ranks to run: -</p> <ul> <li><code>SmallCPU</code> - </li> <li><code>LargeCPU</code> -</li> <li><code>SmallGPU</code> - </li> <li><code>LargeGPU</code> -</li> </ul> <p><code>5. fqdn</code> - The fully qualified URL for the DB cluster. Used on the Ingress records for any exposed services.</p> <p><code>6. haRingName</code> - Default: <code>default</code></p> <p><code>7. hasPools</code> - Whether to enable the separate node 'pools' for \"infra\", \"compute\" pod scheduling.                 Default: false                 +optional</p>"},{"location":"Database/database/#autosuspend","title":"autoSuspend","text":"<p>The DB Cluster autosuspend section allows for the spinning down of the core DB Pods to release the underlying Kubernetes nodes to reduce infrastructure costs when the DB is not in use. </p> kineticacluster.yaml - autoSuspend<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\nmetadata:\n  name: my-kinetica-db-cr\n  namespace: gpudb\nspec:\n  autoSuspend:\n    enabled: false\n    inactivityDuration: 1h0m0s\n</code></pre> <p><code>7.</code> the start of the <code>autoSuspend</code> definition</p> <p><code>8.</code> <code>enabled</code> when set to <code>true</code> auto suspend of the DB cluster is enabled otherwise set to <code>false</code> and no      automatic suspending of the DB takes place.  If omitted it defaults to <code>false</code></p> <p><code>9.</code> <code>inactivityDuration</code> the duration after which if no DB activity has taken place the DB will be suspended</p> <p>Horizontal Pod Autoscaler</p> <p>In order for <code>autoSuspend</code> to work correctly the Kubernetes Horizontal Pod Autoscaler needs to be deployed to the cluster.</p>"},{"location":"Database/database/#gadmin","title":"gadmin","text":"<p>GAdmin the Database Administration Console</p> <p></p> kineticacluster.yaml - gadmin<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\nmetadata:\n  name: my-kinetica-db-cr\n  namespace: gpudb\nspec:\n  gadmin:\n    containerPort:\n      containerPort: 8080\n      name: gadmin\n      protocol: TCP\n    isEnabled: true\n</code></pre> <p><code>7.</code> <code>gadmin</code> configuration block definition</p> <p><code>8.</code> <code>containerPort</code> configuration block i.e. where <code>gadmin</code> is exposed on the DB Pod</p> <p><code>9.</code> <code>containerPort</code> the port number as an integer. Default: <code>8080</code></p> <p><code>10.</code> <code>name</code> the name of the port being exposed. Default:  <code>gadmin</code></p> <p><code>11.</code> <code>protocol</code> network protocal used. Default: <code>TCP</code></p> <p><code>12.</code> <code>isEnabled</code> whether <code>gadmin</code> is exposed from the DB pod. Default: <code>true</code></p>"},{"location":"Database/database/#example-db-cr","title":"Example DB CR","text":"YAML<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\nmetadata:\n  name: kinetica-cluster\n  namespace: gpudb\nspec:\n  autoSuspend:\n    enabled: false\n    inactivityDuration: 1h0m0s\n  debug: false\n  gadmin:\n    isEnabled: true\n  gpudbCluster:\n    clusterName: kinetica-cluster\n    clusterSize:\n      tshirtSize: M\n      tshirtType: LargeCPU\n    config:\n      graph:\n        enable: true\n      postgresProxy:\n        enablePostgresProxy: true\n      textSearch:\n        enableTextSearch: true\n      kifs:\n        enable: false\n      ml:\n        enable: false\n      tieredStorage:\n        globalTier:\n          colocateDisks: true\n          concurrentWaitTimeout: 120\n          encryptDataAtRest: true\n        persistTier:\n          default:\n            highWatermark: 90\n            limit: 4Ti\n            lowWatermark: 50\n            name: ''\n            path: default\n            provisioner: docker.io/hostpath\n            volumeClaim:\n              metadata: {}\n              spec:\n                resources: {}\n                storageClassName: kinetica-db-persist\n              status: {}\n      tieredStrategy:\n        default: VRAM 1, RAM 5, PERSIST 5\n        predicateEvaluationInterval: 60\n    fqdn: kinetica-cluster.saas.kinetica.com\n    haRingName: default\n    hasPools: false\n    hostManagerPort:\n      containerPort: 9300\n      name: hostmanager\n      protocol: TCP\n    image: docker.io/kineticastagingcloud/kinetica-k8s-db:v7.1.9-8.rc1\n    imagePullPolicy: IfNotPresent\n    letsEncrypt:\n      enabled: false\n    license: &gt;-\n\n    metricsRegistryRepositoryTag:\n      imagePullPolicy: IfNotPresent\n      registry: docker.io\n      repository: kineticastagingcloud/fluent-bit\n      sha: ''\n      tag: v7.1.9-8.rc1\n    podManagementPolicy: Parallel\n    ranksPerNode: 1\n    replicas: 4\n  hostManagerMonitor:\n    monitorRegistryRepositoryTag:\n      imagePullPolicy: IfNotPresent\n      registry: docker.io\n      repository: kineticastagingcloud/kinetica-k8s-monitor\n      sha: ''\n      tag: v7.1.9-8.rc1\n    readinessProbe:\n      failureThreshold: 20\n      initialDelaySeconds: 5\n      periodSeconds: 10\n    startupProbe:\n      failureThreshold: 20\n      initialDelaySeconds: 5\n      periodSeconds: 10\n  infra: on-prem\n  ingressController: nginx-ingress\n  ldap:\n    host: openldap\n    isInLocalK8S: true\n    isLDAPS: false\n    namespace: gpudb\n    port: 389\n  payAsYouGo: false\n  reveal:\n    containerPort:\n      containerPort: 8088\n      name: reveal\n      protocol: TCP\n    isEnabled: true\n  supportingImages:\n    busybox:\n      imagePullPolicy: IfNotPresent\n      registry: docker.io\n      repository: kineticastagingcloud/busybox\n      sha: ''\n      tag: v7.1.9-8.rc1\n    socat:\n      imagePullPolicy: IfNotPresent\n      registry: docker.io\n      repository: kineticastagingcloud/socat\n      sha: ''\n      tag: v7.1.9-8.rc1\n</code></pre>"},{"location":"Database/database/#kineticauser","title":"KineticaUser","text":""},{"location":"Database/database/#kineticagrant","title":"KineticaGrant","text":""},{"location":"Database/database/#kineticaschema","title":"KineticaSchema","text":""},{"location":"Database/database/#kineticaresourcegroup","title":"KineticaResourceGroup","text":""},{"location":"Database/database_reference/","title":"Database CRD/CR Reference","text":""},{"location":"Database/grant_reference/","title":"Kinetica DB User Permission Grant CRD/CR Reference","text":""},{"location":"Database/resource_group_reference/","title":"Kinetica DB User Resource Group CRD/CR Reference","text":""},{"location":"Database/schema_reference/","title":"Kinetica DB User Schema CRD/CR Reference","text":""},{"location":"Database/user_reference/","title":"Kinetica DB User CRD/CR Reference","text":""},{"location":"Operators/k3s/","title":"Overview","text":"<p>Kinetica Operators can be installed in any on-prem kubernetes cluster. This document provides instructions to install the operators in k3s. If you are on another distribution, you should be able to change the values file to suit your environment.</p> <p>You will need a license key for this to work. Please contact Kinetica Support.</p>"},{"location":"Operators/k3s/#kinetica-on-k3s-k3sio","title":"Kinetica on k3s (k3s.io)","text":"<p>Current version of the chart supports kubernetes version 1.25 and above.</p>"},{"location":"Operators/k3s/#install-k3s-129","title":"Install k3s 1.29","text":"Bash<pre><code>curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--disable=traefik  --node-name kinetica-master --token 12345\" K3S_KUBECONFIG_OUTPUT=~/.kube/config_k3s K3S_KUBECONFIG_MODE=644 INSTALL_K3S_VERSION=v1.29.1+k3s2 sh -\n</code></pre>"},{"location":"Operators/k3s/#k3s-install-kinetica-operators-including-a-sample-db-to-try-out","title":"K3s -Install kinetica-operators including a sample db to try out","text":"<p>Review the values file charts/kinetica-operators/values.onPrem.k3s.yaml. This is trying to install the operators and a simple db with workbench installation for a non production try out.</p> <p>As you can see it is trying to create an ingress pointing towards local.kinetica. If you have a domain pointing to your machine, replace it with the correct domain name.</p> <p>If you are on a local machine which is not having a domain name, you add the following entry to your /etc/hosts file or equivalent.</p> Text Only<pre><code>127.0.0.1 local.kinetica\n</code></pre>"},{"location":"Operators/k3s/#k3s-install-the-kinetica-operators-chart","title":"K3s - Install the kinetica-operators chart","text":"Bash<pre><code>wget https://raw.githubusercontent.com/kineticadb/charts/master/kinetica-operators/values.onPrem.k3s.yaml\n\nhelm -n kinetica-system install kinetica-operators kinetica-operators/kinetica-operators --create-namespace --values values.onPrem.k3s.yaml --set db.gpudbCluster.license=\"your_license_key\" --set dbAdminUser.password=\"your_password\"\n\n# if you want to try out a development version,\nhelm search repo kinetica-operators --devel --versions\n\nhelm -n kinetica-system install kinetica-operators kinetica-operators/kinetica-operators --create-namespace --values values.onPrem.k3s.yaml --set db.gpudbCluster.license=\"your_license_key\" --set dbAdminUser.password=\"your_password\" --devel --version 7.2.0-2.rc-2\n</code></pre>"},{"location":"Operators/k3s/#k3s-install-the-kinetica-operators-chart-gpu-capable-machine","title":"K3s - Install the kinetica-operators chart (GPU Capable Machine)","text":"<p>If you wish to try out the GPU capabilities, you can use the following values file, provided you are in a nvidia gpu capable machine.</p> Bash<pre><code>wget https://raw.githubusercontent.com/kineticadb/charts/master/kinetica-operators/values.onPrem.k3s.gpu.yaml\n\nhelm -n kinetica-system install kinetica-operators charts/kinetica-operators/ --create-namespace --values values.onPrem.k3s.gpu.yaml --set db.gpudbCluster.license=\"your_license_key\" --set dbAdminUser.password=\"your_password\"\n</code></pre> <p>You should be able to access the workbench at http://local.kinetica</p> <p>Username as per the values file mentioned above is kadmin and password is Kinetica1234!</p>"},{"location":"Operators/k3s/#uninstall-k3s","title":"Uninstall k3s","text":"Bash<pre><code>/usr/local/bin/k3s-uninstall.sh\n</code></pre>"},{"location":"Operators/k8s/","title":"Overview","text":"<p>For managed Kubernetes solutions (AKS, EKS, GKE) or other on-prem K8s flavors, follow this generic guide to install the Kinetica Operators, Database and Workbench. A product license key will be required for install. Please contact Kinetica Support to request a trial key.</p>"},{"location":"Operators/k8s/#preparation-and-prerequisites","title":"Preparation and prerequisites","text":"<p>Installation requires Helm3 and access to an on-prem or CSP managed Kubernetes cluster. kubectl is optional but highly recommended. The context for the desired target cluster must be selected from your <code>~/.kube/config</code> file or set via the <code>KUBECONFIG</code> environment variable. Check to see if you have the correct context with, Text Only<pre><code>kubectl config current-context\n</code></pre> and that you can access this cluster correctly with, Text Only<pre><code>kubectl get nodes\n</code></pre> If you do not see a list of nodes for your K8s cluster the helm installation will not work. Please check your Kubernetes installation or access credentials (kubeconfig).</p>"},{"location":"Operators/k8s/#install-the-kinetica-operators-chart","title":"Install the kinetica-operators chart","text":"<p>This chart will install the Kinetica K8s operators together with a default configured database and workbench UI.</p> <p>If you are installing into a managed Kubernetes environment and the NGINX ingress controller that is installed as part of this install creates a LoadBalancer service, you may need to associate the LoadBalancer with the domain you plan to use.</p> <p>Alternatively, if you are installing on a local machine which does not have a domain name, you can add the following entry to your <code>/etc/hosts</code> file or equivalent: Text Only<pre><code>127.0.0.1  local.kinetica\n</code></pre> Note that the default chart configuration points to <code>local.kinetica</code> but this is configurable.</p>"},{"location":"Operators/k8s/#1-add-the-kinetica-chart-repository","title":"1. Add the Kinetica chart repository","text":"<p>Add the repo locally as kinetica-operators: Bash<pre><code>helm repo add kinetica-operators https://kineticadb.github.io/charts\n</code></pre></p>"},{"location":"Operators/k8s/#2-obtain-the-default-helm-values-file","title":"2. Obtain the default Helm values file","text":"<p>For the generic Kubernetes install use the following values file without modification. Advanced users with specific requirements may need to adjust parameters in this file. Bash<pre><code>wget https://raw.githubusercontent.com/kineticadb/charts/master/kinetica-operators/values.onPrem.k8s.yaml\n</code></pre></p>"},{"location":"Operators/k8s/#3-determine-the-following-prior-to-the-chart-install","title":"3. Determine the following prior to the chart install","text":"<p>(a) Obtain a LICENSE-KEY as described in the introduction above. (b) Choose a PASSWORD for the initial administrator user (Note: the default in the chart for this user is <code>kadmin</code> but this is configurable). Non-ASCII characters and typographical symbols in the password must be escaped with a \"\\\". For example, <code>--set dbAdminUser.password=\"MyPassword\\!\"</code> \u00a9 As storage class name varies between K8s flavor and/or there can be multiple, this must be prescribed in the chart installation. Obtain DEFAULT-STORAGE-CLASS name with the command: Bash<pre><code>kubectl get sc -o name \n</code></pre> use the name found after the /, For example, in <code>\"storageclass.storage.k8s.io/TheName\"</code> use \"TheName\" as the parameter.</p>"},{"location":"Operators/k8s/#4-install-the-helm-chart","title":"4. Install the helm chart","text":"<p>Run the following Helm install command after substituting values from section 3 above: Bash<pre><code>helm -n kinetica-system install \\\nkinetica-operators kinetica-operators/kinetica-operators \\\n--create-namespace \\\n--values values.onPrem.k8s.yaml \\\n--set db.gpudbCluster.license=\"LICENSE-KEY\" \\\n--set dbAdminUser.password=\"PASSWORD\" \\\n--set global.defaultStorageClass=\"DEFAULT-STORAGE-CLASS\"\n</code></pre></p>"},{"location":"Operators/k8s/#5-check-installation-progress","title":"5. Check installation progress","text":"<p>After a few moments, follow the progression of the main database pod startup with: Text Only<pre><code>kubectl -n gpudb get po gpudb-0 -w\n</code></pre> until it reaches <code>\"gpudb-0  3/3  Running\"</code> at which point the database should be ready and all other software installed in the cluster. You may have to run this command in a different terminal if the <code>helm</code> command from step 4 has not yet returned to the system prompt. Once running, you can quit this kubectl watch command using ctrl-c.</p>"},{"location":"Operators/k8s/#6-accessing-the-kinetica-installation","title":"6. Accessing the Kinetica installation","text":""},{"location":"Operators/k8s/#optional-install-a-development-chart-version","title":"(Optional) Install a development chart version","text":"<p>Find all alternative chart versions with: Text Only<pre><code>helm search repo kinetica-operators --devel --versions\n</code></pre> Then append <code>--devel --version [CHART-DEVEL-VERSION]</code> to the end of the Helm install command in section 4 above.</p>"},{"location":"Operators/kind/","title":"Overview","text":"<p>This installation in a kind cluster is for trying out the operators and the database in a non production environment. This method currently only supports installing a CPU version of the database.</p> <p>You will need a license key for this to work. Please contact Kinetica Support.</p>"},{"location":"Operators/kind/#kind-kubernetes-in-docker-kindsigsk8sio","title":"Kind (kubernetes in docker kind.sigs.k8s.io)","text":""},{"location":"Operators/kind/#create-kind-cluster-129","title":"Create Kind Cluster 1.29","text":"Bash<pre><code>kind create cluster --config charts/kinetica-operators/kind.yaml\n</code></pre>"},{"location":"Operators/kind/#kind-install-kinetica-operators-including-a-sample-db-to-try-out","title":"Kind - Install kinetica-operators including a sample db to try out","text":"<p>Review the values file charts/kinetica-operators/values.onPrem.kind.yaml. This is trying to install the operators and a simple db with workbench installation for a non production try out.</p> <p>As you can see it is trying to create an ingress pointing towards local.kinetica. If you have a domain pointing to your machine, replace it with the correct domain name.</p>"},{"location":"Operators/kind/#kind-install-the-kinetica-operators-chart","title":"Kind - Install the kinetica-operators chart","text":"Bash<pre><code>wget https://raw.githubusercontent.com/kineticadb/charts/master/kinetica-operators/values.onPrem.kind.yaml\n\nhelm -n kinetica-system install kinetica-operators kinetica-operators/kinetica-operators --create-namespace --values values.onPrem.kind.yaml --set db.gpudbCluster.license=\"your_license_key\" --set dbAdminUser.password=\"your_password\"\n\n# if you want to try out a development version,\nhelm search repo kinetica-operators --devel --versions\nhelm -n kinetica-system install kinetica-operators kinetica-operators/kinetica-operators/ --create-namespace --values values.onPrem.kind.yaml --set db.gpudbCluster.license=\"your_license_key\" --set dbAdminUser.password=\"your_password\" --devel --version 7.2.0-2.rc-2\n</code></pre> <p>You should be able to access the workbench at http://local.kinetica</p> <p>Username as per the values file mentioned above is kadmin and password is Kinetica1234!</p>"},{"location":"Workbench/workbench/","title":"Kinetica Workbench Configuration","text":"<ul> <li>kubectl (yaml)</li> <li>Helm Chart</li> </ul>"},{"location":"Workbench/workbench/#workbench","title":"Workbench","text":"kubectl <p>Using kubetctl a CustomResource of type <code>KineticaCluster</code> is used to define a new Kinetica DB Cluster in a yaml file.</p> <p>The basic Group, Version, Kind or GVK to instantiate a Kinetica Workbench is as follows: -</p> Workbench GVK<pre><code>apiVersion: workbench.com.kinetica/v1\nkind: Workbench\n</code></pre>"},{"location":"Workbench/workbench/#metadata","title":"Metadata","text":"<p>to which we add a <code>metadata:</code> block for the name of the DB CR along with the <code>namespace</code> into which we are targetting the installation of the DB cluster.</p> Workbench metadata<pre><code>apiVersion: workbench.com.kinetica/v1\nkind: Workbench\nmetadata:\n  name: workbench-kinetica-cluster\n  namespace: gpudb\n</code></pre>"},{"location":"Workbench/workbench/#an-example-workbench-cr","title":"An Example Workbench CR","text":"<p>The simplest valid Workbench CR looks as follows: -</p> workbench.yaml<pre><code>apiVersion: workbench.com.kinetica/v1\nkind: Workbench\nmetadata:\n  name: workbench-kinetica-cluster\n  namespace: gpudb\nspec:\n  executeSqlLimit: 10000\n  fqdn: kinetica-cluster.saas.kinetica.com\n  image: kineticastagingcloud/workbench:v7.1.9-8.rc1\n  letsEncrypt:\n    enabled: false\n  userIdleTimeout: 60\n  ingressController: nginx-ingress\n</code></pre>"},{"location":"Workbench/workbench_reference/","title":"Workbench CRD/CR Reference","text":""}]}