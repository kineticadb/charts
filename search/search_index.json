{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kinetica DB Cluster Operator Deployment","text":"<p>Helm Charts and kubectl <code>.yaml</code> files are provided to support the deployment of the Kinetica Database  Kubernetes Operator(s). There are two Kinetica operators that will be deployed: -</p> <ul> <li>Kinetica Database Operator</li> <li>Kinetica Workbench Operator</li> </ul>"},{"location":"#kinetica-database-operator","title":"Kinetica Database Operator","text":"<p>The Kinetica Database Operator manages the full lifecycle of the Database including: - </p> <ul> <li>Deployment</li> <li>Management (Pause/Suspend)</li> <li>Upgrading</li> <li>Backup</li> <li>Restore</li> <li>Deletion</li> <li>User Creation</li> <li>User Deletion</li> <li>User Grants</li> <li>User Schema</li> </ul> <p>both on supported on-premise Kubernetes distributions and  supported cloud platforms.</p>"},{"location":"#kinetica-workbench-operator","title":"Kinetica Workbench Operator","text":"<p>The Kinetica Workbench Operator manages the lifecycle of the Kinetica Workbench deployment.  Each Kinetica DB has a corresponding Workbench associated.</p>"},{"location":"#helm-charts","title":"Helm Charts","text":"<p>Information on the Helm Charts</p>"},{"location":"#supported-on-premise-kubernetes-distributions","title":"Supported On-Premise Kubernetes Distributions","text":"<ul> <li>KinD - Kubernetes Versions &gt;= 1.22.x</li> <li>Kubeadm - Kubernetes Versions &gt;= 1.22.x</li> </ul>"},{"location":"#supported-cloud-platforms","title":"Supported Cloud Platforms","text":"<p>Currently the Kinetica DB Operator supports deployment on: -</p> <ul> <li>Microsoft Azure</li> <li>Amazon AWS</li> </ul>"},{"location":"#microsoft-azure","title":"Microsoft Azure","text":"<p>The operator runs as part of an Azure Marketplace offering.</p>"},{"location":"#amazon-aws","title":"Amazon AWS","text":"<p>The operator runs as part of an AWS Marketplace offering.</p>"},{"location":"Database/database/","title":"Kinetica Database Configuration","text":"<ul> <li>kubectl (yaml)</li> </ul>"},{"location":"Database/database/#kineticacluster","title":"KineticaCluster","text":"<p>To deploy a new Database Instance into a Kubernetes cluster...</p> kubectl <p>Using kubetctl a CustomResource of type <code>KineticaCluster</code> is used to define a new Kinetica DB Cluster in a yaml file.</p> <p>The basic Group, Version, Kind or GVK to instantiate a Kinetica DB Cluster is as follows: -</p> kineticacluster.yaml<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\n</code></pre>"},{"location":"Database/database/#metadata","title":"Metadata","text":"<p>to which we add a <code>metadata:</code> block for the name of the DB CR along with the <code>namespace</code> into which we are targetting the installation of the DB cluster.</p> kineticacluster.yaml<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\nmetadata:\nname: my-kinetica-db-cr\nnamespace: gpudb\nspec:\n</code></pre>"},{"location":"Database/database/#spec","title":"Spec","text":"<p>Under the <code>spec:</code> section of the KineticaCLuster CR we have a number of sections supporting different aspects of the deployed DB cluster:-</p> <ul> <li>gpudbCluster</li> <li>autoSuspend</li> <li>gadmin</li> </ul>"},{"location":"Database/database/#gpudbcluster","title":"gpudbCluster","text":"<p>Configuartion items specific to the DB itself.</p> kineticacluster.yaml - gpudbCluster<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\nmetadata:\nname: my-kinetica-db-cr\nnamespace: gpudb\nspec:\ngpudbCluster:\n</code></pre>"},{"location":"Database/database/#gpudbcluster_1","title":"gpudbCluster","text":"cluster name &amp; size<pre><code>clusterName: kinetica-cluster clusterSize: tshirtSize: M tshirtType: LargeCPU fqdn: kinetica-cluster.saas.kinetica.com\nhaRingName: default\nhasPools: false    </code></pre> <p><code>1. clusterName</code> - the user defined name of the Kinetica DB Cluster</p> <p><code>2. clusterSize</code> - block that defines the number of DB Ranks to run</p> <p><code>3. tshirtSize</code> - sets the cluster size to a defined size based upon the t-shirt size. Valid sizes are: -</p> <ul> <li><code>XS</code> -   1 DB Rank</li> <li><code>S</code> -    2 DB Ranks</li> <li><code>M</code> -    4 DB Ranks</li> <li><code>L</code> -    8 DB Ranks</li> <li><code>XL</code> -   16 DB Ranks</li> <li><code>XXL</code> -  32 DB Ranks</li> <li><code>XXXL</code> - 64 DB Ranks</li> </ul> <p><code>4. tshirtType</code> - block that defines the tyoe DB Ranks to run: -</p> <ul> <li><code>SmallCPU</code> - </li> <li><code>LargeCPU</code> -</li> <li><code>SmallGPU</code> - </li> <li><code>LargeGPU</code> -</li> </ul> <p><code>5. fqdn</code> - The fully qualified URL for the DB cluster. Used on the Ingress records for any exposed services.</p> <p><code>6. haRingName</code> - Default: <code>default</code></p> <p><code>7. hasPools</code> - Whether to enable the separate node 'pools' for \"infra\", \"compute\" pod scheduling.                 Default: false                 +optional</p>"},{"location":"Database/database/#autosuspend","title":"autoSuspend","text":"<p>The DB Cluster autosuspend section allows for the spinning down of the core DB Pods to release the underlying Kubernetes nodes to reduce infrastructure costs when the DB is not in use. </p> kineticacluster.yaml - autoSuspend<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\nmetadata:\nname: my-kinetica-db-cr\nnamespace: gpudb\nspec:\nautoSuspend:\nenabled: false\ninactivityDuration: 1h0m0s\n</code></pre> <p><code>7.</code> the start of the <code>autoSuspend</code> definition</p> <p><code>8.</code> <code>enabled</code> when set to <code>true</code> auto suspend of the DB cluster is enabled otherwise set to <code>false</code> and no      automatic suspending of the DB takes place.  If omitted it defaults to <code>false</code></p> <p><code>9.</code> <code>inactivityDuration</code> the duration after which if no DB activity has taken place the DB will be suspended</p> <p>Horizontal Pod Autoscaler</p> <p>In order for <code>autoSuspend</code> to work correctly the Kubernetes Horizontal Pod Autoscaler needs to be deployed to the cluster.</p>"},{"location":"Database/database/#gadmin","title":"gadmin","text":"<p>GAdmin the Database Administration Console</p> <p></p> kineticacluster.yaml - gadmin<pre><code>apiVersion: app.kinetica.com/v1\nkind: KineticaCluster\nmetadata:\nname: my-kinetica-db-cr\nnamespace: gpudb\nspec:\ngadmin:\ncontainerPort:\ncontainerPort: 8080\nname: gadmin\nprotocol: TCP\nisEnabled: true\n</code></pre> <p><code>7.</code> <code>gadmin</code> configuration block definition</p> <p><code>8.</code> <code>containerPort</code> configuration block i.e. where <code>gadmin</code> is exposed on the DB Pod</p> <p><code>9.</code> <code>containerPort</code> the port number as an integer. Default: <code>8080</code></p> <p><code>10.</code> <code>name</code> the name of the port being exposed. Default:  <code>gadmin</code></p> <p><code>11.</code> <code>protocol</code> network protocal used. Default: <code>TCP</code></p> <p><code>12.</code> <code>isEnabled</code> whether <code>gadmin</code> is exposed from the DB pod. Default: <code>true</code></p>"},{"location":"Database/database/#kineticauser","title":"KineticaUser","text":""},{"location":"Database/database/#kineticagrant","title":"KineticaGrant","text":""},{"location":"Database/database/#kineticaschema","title":"KineticaSchema","text":""},{"location":"Database/database/#kineticaresourcegroup","title":"KineticaResourceGroup","text":""},{"location":"Operators/kinetica-operators/","title":"Kinetica DB Operator Helm Charts","text":""},{"location":"Operators/kinetica-operators/#add-kinetica-helm-repoistory","title":"Add Kinetica Helm Repoistory","text":"<p>To add the Kinetica Helm repoistory to Helm 3:-</p> Bash<pre><code>helm repo add kinetica-operators https://kineticadb.github.io/charts\nhelm repo update\n</code></pre>"},{"location":"Operators/kinetica-operators/#perform-a-helm-installation-of-the-kinetica-operatoes","title":"Perform a Helm installation of the Kinetica Operatoes","text":"Bash<pre><code>helm install -n kinetica-system \\\nkinetica-operators kinetica-operators/kinetica-operators --create-namespace\n</code></pre> <p>This will install all the Kubernetes Operators required into the <code>kinetica-system</code> namespace and create the namespace if it is not currently present.</p> <p>Note</p> <p>Depending on what target platform you are installing to it may be necessary to supply an additional parameter  pointing to a values file to successfully provision the DB.</p> Bash<pre><code>helm install -n kinetica-system -f values.yaml --set provider=aks \\\nkinetica-operators kinetica-operators/kinetica-operators --create-namespace\n</code></pre> <p>The command above uses a custom <code>values.yaml</code> for helm and sets the install platform to Microsoft Azure AKS.</p> <p>Currently supported <code>providers</code> are: -</p> <ul> <li><code>aks</code> - Microsoft Azure AKS</li> <li><code>eks</code> - Amazon AWS EKS</li> <li><code>local</code> - Generic 'On-Prem' Kubernetes Clusters e.g. one deployed using <code>kubeadm</code></li> </ul> <p>Example Helm <code>values.yaml</code> for different Cloud Providers/On-Prem installations: -</p> Azure AKSAmazon EKSOn-Prem values.yaml<pre><code>namespace: kinetica-system\ndb:\nserviceAccount: {}\nimage:\n# Kinetica DB Operator installer image\nrepository: \"registry.harbor.kinetica.com/kinetica/kinetica-k8s-operator\"\n#  Kinetica DB Operator installer image tag\ntag: \"\"\nparameters:\n# &lt;base64 encode of kubeconfig&gt; of the Kubernetes Cluster to deploy to\nkubeconfig: \"\"\n# The storage class to use for PVCs\nstorageClass: \"managed-premium\"\nstorageClass:\npersist:\n# Workbench Operator Persistent Volume Storage Class\nprovisioner: \"disk.csi.azure.com\"\nprocs:\n# Workbench Operator Procs Volume Storage Class\nprovisioner: \"disk.csi.azure.com\"\ncache:\n# Workbench Operator Cache Volume Storage Class\nprovisioner: \"disk.csi.azure.com\"\n</code></pre> <p>15 <code>storageClass: \"managed-premium\"</code> - sets the appropriate <code>storageClass</code> for Microsoft Azure AKS Persistent Volume (PV)</p> <p>20 <code>provisioner: \"disk.csi.azure.com\"</code> - sets the appropriate disk provisioner for the DB (Persist) filesystem for Microsoft Azure</p> <p>23 <code>provisioner: \"disk.csi.azure.com\"</code> - sets the appropriate disk provisioner for the DB Procs filesystem for Microsoft Azure</p> <p>26 <code>provisioner: \"disk.csi.azure.com\"</code> - sets the appropriate disk provisioner for the DB Cache filesystem for Microsoft Azure</p> values.yaml<pre><code>namespace: kinetica-system\ndb:\nserviceAccount: {}\nimage:\n# Kinetica DB Operator installer image\nrepository: \"registry.harbor.kinetica.com/kinetica/kinetica-k8s-operator\"\n#  Kinetica DB Operator installer image tag\ntag: \"\"\nparameters:\n# &lt;base64 encode of kubeconfig&gt; of the Kubernetes Cluster to deploy to\nkubeconfig: \"\"\n# The storage class to use for PVCs\nstorageClass: \"gp2\"\nstorageClass:\npersist:\n# Workbench Operator Persistent Volume Storage Class\nprovisioner: \"kubernetes.io/aws-ebs\"\nprocs:\n# Workbench Operator Procs Volume Storage Class\nprovisioner: \"kubernetes.io/aws-ebs\"\ncache:\n# Workbench Operator Cache Volume Storage Class\nprovisioner: \"kubernetes.io/aws-ebs\"\n</code></pre> <p>15 <code>storageClass: \"gp2\"</code> - sets the appropriate <code>storageClass</code> for Amazon EKS Persistent Volume (PV)</p> <p>20 <code>provisioner: \"kubernetes.io/aws-ebs\"</code> - sets the appropriate disk provisioner for the DB (Persist) filesystem for Microsoft Azure</p> <p>23 <code>provisioner: \"kubernetes.io/aws-ebs\"</code> - sets the appropriate disk provisioner for the DB Procs filesystem for Microsoft Azure</p> <p>26 <code>provisioner: \"kubernetes.io/aws-ebs\"</code> - sets the appropriate disk provisioner for the DB Cache filesystem for Microsoft Azure</p> values.yaml<pre><code>namespace: kinetica-system\ndb:\nserviceAccount: {}\nimage:\n# Kinetica DB Operator installer image\nrepository: \"registry.harbor.kinetica.com/kinetica/kinetica-k8s-operator\"\n#  Kinetica DB Operator installer image tag\ntag: \"\"\nparameters:\n# &lt;base64 encode of kubeconfig&gt; of the Kubernetes Cluster to deploy to\nkubeconfig: \"\"\n# the type of installation e.g. aks, eks, local\nenvironment: \"local\"\n# The storage class to use for PVCs\nstorageClass: \"standard\"\nstorageClass:\nprocs: {}\npersist: {}\ncache: {}\n</code></pre> <p>15 <code>environment: \"local\"</code> - tells the DB Operator to deploy the DB as a 'local' instance to the Kubernetes Cluster</p> <p>17 <code>storageClass: \"standard\"</code> - sets the appropriate <code>storageClass</code> for the On-Prem Persistent Volume Provisioner</p> <p>storageClass</p> <p>The <code>storageClass</code> should be present in the target environment. </p> <p>A list of available <code>storageClass</code> can be obtained using: -</p> Bash<pre><code>kubectl get sc\n</code></pre>"},{"location":"Operators/kinetica-operators/#components","title":"Components","text":"<p>The <code>kinetica-db</code> Helm Chart wraps the deployment of a number of sub-components: -</p> <ul> <li>Porter Operator</li> <li>Kinetica Database Operator</li> <li>Kinetica Workbench Operator</li> </ul> <p>Installation/Upgrading/Deletion of the Kinetica Operators is done via two CRs which leverage porter.sh as the orchestrator. The corresponding Porter Operator, DB Operator &amp; Workbench Operator CRs are submitted by running the appropriate helm command i.e.</p> <ul> <li>install</li> <li>upgrade</li> <li>uninstall</li> </ul>"},{"location":"Operators/kinetica-operators/#porter-operator","title":"Porter Operator","text":""},{"location":"Operators/kinetica-operators/#database-operator","title":"Database Operator","text":"<p>The Kinetica DB Operator installation CR for the porter.sh operator is: -</p> YAML<pre><code>apiVersion: porter.sh/v1\nkind: Installation\nmetadata:\nannotations:\nmeta.helm.sh/release-name: kinetica-operators\nmeta.helm.sh/release-namespace: kinetica-system\nlabels:\napp.kubernetes.io/instance: kinetica-operators\napp.kubernetes.io/managed-by: Helm\napp.kubernetes.io/name: kinetica-operators\napp.kubernetes.io/version: 0.1.0\nhelm.sh/chart: kinetica-operators-0.1.0\ninstallVersion: 0.38.10\nname: kinetica-operators-operator-install\nnamespace: kinetica-system\nspec:\naction: install\nagentConfig:\nvolumeSize: '0'\nparameters:\nenvironment: local\nstorageclass: managed-premium\nreference: docker.io/kinetica/kinetica-k8s-operator:v7.1.9-7.rc3\n</code></pre>"},{"location":"Operators/kinetica-operators/#workbench-operator","title":"Workbench Operator","text":"<p>The Kinetica Workbench installation CR for the porter.sh operator is: -</p> YAML<pre><code>apiVersion: porter.sh/v1\nkind: Installation\nmetadata:\nannotations:\nmeta.helm.sh/release-name: kinetica-operators\nmeta.helm.sh/release-namespace: kinetica-system\nlabels:\napp.kubernetes.io/instance: kinetica-operators\napp.kubernetes.io/managed-by: Helm\napp.kubernetes.io/name: kinetica-operators\napp.kubernetes.io/version: 0.1.0\nhelm.sh/chart: kinetica-operators-0.1.0\ninstallVersion: 0.38.10\nname: kinetica-operators-wb-operator-install\nnamespace: kinetica-system\nspec:\naction: install\nagentConfig:\nvolumeSize: '0'\nparameters:\nenvironment: local\nreference: docker.io/kinetica/workbench-operator:v7.1.9-7.rc3\n</code></pre>"},{"location":"Operators/kinetica-operators/#overriding-images-tags","title":"Overriding Images Tags","text":"Bash<pre><code>helm install -n kinetica-system kinetica-operators kinetica-operators/kinetica-operators \\\n--create-namespace \\\n--set provider=aks  --set dbOperator.image.tag=v7.1.9-7.rc3 \\\n--set dbOperator.image.repository=docker.io/kinetica/kinetica-k8s-operator \\\n--set wbOperator.image.repository=docker.io/kinetica/workbench-operator \\\n--set wbOperator.image.tag=v7.1.9-7.rc3\n</code></pre>"},{"location":"Workbench/workbench/","title":"Kinetica Workbench Configuration","text":"<ul> <li>kubectl (yaml)</li> <li>Helm Chart</li> </ul>"},{"location":"Workbench/workbench/#workbench","title":"Workbench","text":"kubectl <p>Using kubetctl a CustomResource of type <code>KineticaCluster</code> is used to define a new Kinetica DB Cluster in a yaml file.</p> <p>The basic Group, Version, Kind or GVK to instantiate a Kinetica Workbench is as follows: -</p> Workbench GVK<pre><code>apiVersion: workbench.com.kinetica/v1\nkind: Workbench\n</code></pre>"},{"location":"Workbench/workbench/#metadata","title":"Metadata","text":"<p>to which we add a <code>metadata:</code> block for the name of the DB CR along with the <code>namespace</code> into which we are targetting the installation of the DB cluster.</p> Workbench metadata<pre><code>apiVersion: workbench.com.kinetica/v1\nkind: Workbench\nmetadata:\nname: workbench-kinetica-cluster\nnamespace: gpudb\n</code></pre> <p>The simplest valid Workbench CR looks as follows: -</p> workbench.yaml<pre><code>apiVersion: workbench.com.kinetica/v1\nkind: Workbench\nmetadata:\nname: workbench-kinetica-cluster\nnamespace: gpudb\nspec:\nexecuteSqlLimit: 10000\nfqdn: kinetica-cluster.saas.kinetica.com\nimage: kineticastagingcloud/workbench:v7.1.9-8.rc1\nletsEncrypt:\nenabled: false\nuserIdleTimeout: 60\ningressController: nginx-ingress\n</code></pre> <p><code>1. clusterName</code> - the user defined name of the Kinetica DB Cluster</p> <p><code>2. clusterSize</code> - block that defines the number of DB Ranks to run</p> helm"}]}