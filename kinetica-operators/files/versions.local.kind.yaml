charts:
  kyverno:
    default:
      chartVersion: v2.1.3
      values:
        installCRDs: 'true'
        nodeSelector: {}
        image:
          repository: ghcr.io/kyverno/kyverno
          tag: v1.5.1
        initImage:
          repository: ghcr.io/kyverno/kyvernopre
          tag: v1.5.1
  cert-manager:
    default:
      chartVersion: v1.6.1
      values:
        installCRDs: 'true'
        nodeSelector:
          'app.kinetica.com/pool': 'infra'
        image:
          repository: quay.io/jetstack/cert-manager-controller
          tag: v1.6.1
        # https://github.com/jetstack/cert-manager/blob/f61d5349752f83abe4be99c64979840f258e48b6/cmd/controller/app/options/options.go#L146
        extraArgs:
          - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.6.1
        webhook:
          image:
            repository: quay.io/jetstack/cert-manager-webhook
            tag: v1.6.1
          nodeSelector:
            'app.kinetica.com/pool': 'infra'
        cainjector:
          image:
            repository: quay.io/jetstack/cert-manager-cainjector
            tag: v1.6.1
          nodeSelector:
            'app.kinetica.com/pool': 'infra'
        startupapicheck:
          image:
            repository: quay.io/jetstack/cert-manager-ctl
            tag: v1.6.1
          nodeSelector:
            'app.kinetica.com/pool': 'infra'
  ingress-nginx:
    default:
      chartVersion: 4.0.6
      values:
        defaultBackend:
          enabled: true
          image:
            repository: docker.io/kineticadevcloud/kinetica-default-backend
            tag: v0.0.1
            digest: ""
          nodeSelector:
            ingress-ready: "true"
          tolerations:
            - key: "node-role.kubernetes.io/master"
              operator: "Equal"
              effect: "NoSchedule"
            - effect: NoSchedule
              key: node-role.kubernetes.io/control-plane
              operator: Equal
          extraEnvs:
            - name: DEBUG
              value: "false"

        tcp:
          3890: "openldap/openldap:389"
          4317: "kineticaoperator-system/otel-collector:4317"
          5432: "gpudb/kineticacluster-sample-rank0-service:5432"
          9002: "gpudb/kineticacluster-sample-rank0-service:9002"
          9191: "gpudb/kineticacluster-sample-rank0-service:9191"
          9300: "gpudb/kineticacluster-sample-rank0-service:9300"
        controller:
   
          hostPort:
            enabled: true
          terminationGracePeriodSeconds: 0
          service:
            type: "NodePort"            
          extraArgs:
            publish-status-address: "localhost"
          publishService:
            enabled: false
          nodeSelector:
            ingress-ready: "true"
          tolerations:
            - key: "node-role.kubernetes.io/master"
              operator: "Equal"
              effect: "NoSchedule"
            - effect: NoSchedule
              key: node-role.kubernetes.io/control-plane
              operator: Equal
          admissionWebhooks:
            enabled: true
            patch:
              enabled: true  
              image:
                repository: k8s.gcr.io/ingress-nginx/kube-webhook-certgen
                tag: v1.1.1
                digest: ""
  openldap:
    default:
      chartVersion: 1.2.6
      values:
        env:
          LDAP_BACKEND: "mdb"
          LDAP_ORGANISATION: "Kinetica DB Inc."
          LDAP_DOMAIN: "kinetica.com"
          LDAP_LOG_LEVEL: "128"
        persistence:
          enabled: true
          accessMode: ReadWriteOnce
          storageClass: "standard"
          size: 1Mi
        nodeSelector:
          'app.kinetica.com/pool': 'infra'
        image:
          repository: docker.io/kineticadevcloud/openldap
          tag: 1.5.0.a
  stats:
    default:
      chartVersion: 0.0.1
      values:
        image:
          repository: kinetica/kagent
          tag: "7.1.6"
        
      
  kube-rbac-proxy:
    default:
      chartVersion: v0.5.0
      output: config/manager/bases/manager_auth_proxy_patch.yaml
      template:
        - input: config/manager/bases/manager_auth_proxy_patch.yaml
          replace:
            - path: .spec.template.spec.containers[0].image
              value: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
      values:
        nodeSelector: {}
        image:
          repository: gcr.io/kubebuilder/kube-rbac-proxy
          tag: v0.5.0
  open-telemetry-collector:
    default:
      chartVersion: 0.6.0
      output: config/kustomize/base/monitoring/otel/otel-collector-deployment.yaml
      template:
        - input: config/kustomize/base/monitoring/otel/otel-collector-deployment.yaml
          replace:
            - path: .spec.template.spec.containers[0].image
              value: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          
      values:
       image:
          repository: otel/opentelemetry-collector
          tag: 0.57.2
  aws-for-fluent-bit:
    eks:
      chartVersion: 2.10.0
      output: config/kustomize/overlays/eks/monitoring/cloudwatch/fluent-bit/fluent-bit-daemonset.yaml
      template:
        - input: config/kustomize/overlays/eks/monitoring/cloudwatch/fluent-bit/fluent-bit-daemonset.yaml
          replace:
            - path: .spec.template.spec.containers[0].image
              value: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          
      values:
       image:
          repository: amazon/aws-for-fluent-bit
          tag: 2.25.0
  cloudwatch-agent:
    eks:
      chartVersion: 1.247348.0b251302
      output: config/kustomize/overlays/eks/monitoring/cloudwatch/cwagent/daemonset.yaml
      template: []          
      values:
       image:
          repository: amazon/cloudwatch-agent
          tag: 1.247350.0b251814-amd64
policies:
  default:
    policies: []

